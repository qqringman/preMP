#!/usr/bin/env python3
"""
Manifest Pinning Tool - è‡ªå‹•åŒ–å®šç‰ˆå·¥å…· (æ”¹é€²ç‰ˆ + Manifest æ¯”è¼ƒåŠŸèƒ½)
ç”¨æ–¼å¾ SFTP ä¸‹è¼‰ manifest æª”æ¡ˆä¸¦åŸ·è¡Œ repo å®šç‰ˆæ“ä½œ
æ”¹é€²ç‰ˆæœ¬ï¼šç°¡åŒ– SFTPã€æ”¹é€²å ±å‘Šæ ¼å¼ã€æ­£å¸¸æ—¥èªŒè¼¸å‡ºã€å³æ™‚ manifest æ¯”è¼ƒ
"""

from enum import Enum
import os
import sys
import argparse
import pandas as pd
import paramiko
import subprocess
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import logging
from dataclasses import dataclass, field, asdict
import signal
import atexit
from contextlib import contextmanager
from collections import defaultdict
import random
import socket
from threading import Semaphore, Lock
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter
import xml.etree.ElementTree as ET
from xml.dom import minidom
import difflib

# =====================================
# ===== ç‰ˆæœ¬è³‡è¨Š =====
# =====================================
__version__ = '2.2.0'
__author__ = 'Vince Lin'
__date__ = '2024-12-20'

# =====================================
# ===== XML æ¯”è¼ƒå·¥å…· =====
# =====================================

class ManifestComparator:
    """Manifest XML æª”æ¡ˆæ¯”è¼ƒå·¥å…·"""
    
    def __init__(self):
        self.logger = self._setup_logger()
    
    def _setup_logger(self):
        """è¨­ç½®ç¨ç«‹çš„ logger é¿å…é‡è¤‡æ—¥èªŒ"""
        logger = logging.getLogger(f"{__name__}.ManifestComparator")
        # æ¸…é™¤å·²å­˜åœ¨çš„ handlers é¿å…é‡è¤‡
        if logger.handlers:
            logger.handlers.clear()
        return logger
    
    def normalize_xml_element(self, element: ET.Element) -> dict:
        """
        æ¨™æº–åŒ– XML å…ƒç´ ï¼Œå°‡å±¬æ€§è½‰æ›ç‚ºæœ‰åºå­—å…¸
        ç”¨æ–¼æ¯”è¼ƒæ™‚å¿½ç•¥å±¬æ€§é †åº
        """
        result = {
            'tag': element.tag,
            'text': (element.text or '').strip(),
            'tail': (element.tail or '').strip(),
            'attrib': dict(sorted(element.attrib.items())),  # æ’åºå±¬æ€§
            'children': []
        }
        
        # éè¿´è™•ç†å­å…ƒç´ 
        for child in element:
            result['children'].append(self.normalize_xml_element(child))
        
        # æŒ‰ç…§æ¨™ç±¤åå’Œä¸»è¦å±¬æ€§æ’åºå­å…ƒç´ ï¼Œç¢ºä¿æ¯”è¼ƒçš„ä¸€è‡´æ€§
        result['children'].sort(key=lambda x: (
            x['tag'], 
            x['attrib'].get('name', ''),
            x['attrib'].get('path', ''),
            x['attrib'].get('revision', '')
        ))
        
        return result
    
    def compare_manifests(self, original_manifest: str, exported_manifest: str) -> dict:
        """
        æ¯”è¼ƒå…©å€‹ manifest æª”æ¡ˆ
        
        Args:
            original_manifest: åŸå§‹ manifest æª”æ¡ˆè·¯å¾‘
            exported_manifest: å°å‡ºçš„ manifest æª”æ¡ˆè·¯å¾‘
            
        Returns:
            æ¯”è¼ƒçµæœå­—å…¸ï¼ŒåŒ…å« is_identical, differences, summary ç­‰ä¿¡æ¯
        """
        try:
            if not os.path.exists(original_manifest):
                return {
                    'is_identical': False,
                    'error': f'åŸå§‹ manifest æª”æ¡ˆä¸å­˜åœ¨: {original_manifest}',
                    'summary': 'åŸå§‹æª”æ¡ˆä¸å­˜åœ¨'
                }
            
            if not os.path.exists(exported_manifest):
                return {
                    'is_identical': False,
                    'error': f'å°å‡ºçš„ manifest æª”æ¡ˆä¸å­˜åœ¨: {exported_manifest}',
                    'summary': 'å°å‡ºæª”æ¡ˆä¸å­˜åœ¨'
                }
            
            # è§£æ XML æª”æ¡ˆ
            try:
                tree1 = ET.parse(original_manifest)
                root1 = tree1.getroot()
                
                tree2 = ET.parse(exported_manifest)
                root2 = tree2.getroot()
            except ET.ParseError as e:
                return {
                    'is_identical': False,
                    'error': f'XML è§£æå¤±æ•—: {str(e)}',
                    'summary': 'XML æ ¼å¼éŒ¯èª¤'
                }
            
            # æ¨™æº–åŒ–å…©å€‹ XML æ ¹å…ƒç´ 
            norm1 = self.normalize_xml_element(root1)
            norm2 = self.normalize_xml_element(root2)
            
            # æ¯”è¼ƒæ¨™æº–åŒ–å¾Œçš„çµæ§‹
            differences = []
            is_identical = self._deep_compare(norm1, norm2, differences, '')
            
            # ç”Ÿæˆçµ±è¨ˆæ‘˜è¦
            if is_identical:
                summary = "âœ… å®Œå…¨ç›¸åŒ"
            else:
                diff_types = set()
                for diff in differences:
                    if 'project' in diff['path'].lower():
                        diff_types.add('å°ˆæ¡ˆå·®ç•°')
                    elif 'remote' in diff['path'].lower():
                        diff_types.add('é ç«¯è¨­å®šå·®ç•°')
                    elif 'default' in diff['path'].lower():
                        diff_types.add('é è¨­è¨­å®šå·®ç•°')
                    else:
                        diff_types.add('å…¶ä»–å·®ç•°')
                
                summary = f"âŒ æœ‰å·®ç•° ({len(differences)} è™•): {', '.join(diff_types)}"
            
            # è®€å–æª”æ¡ˆå¤§å°ä¿¡æ¯
            original_size = os.path.getsize(original_manifest)
            exported_size = os.path.getsize(exported_manifest)
            
            result = {
                'is_identical': is_identical,
                'differences': differences,
                'summary': summary,
                'original_manifest': original_manifest,
                'exported_manifest': exported_manifest,
                'original_size': original_size,
                'exported_size': exported_size,
                'difference_count': len(differences)
            }
            
            # è¨˜éŒ„æ¯”è¼ƒçµæœ
            orig_name = os.path.basename(original_manifest)
            exp_name = os.path.basename(exported_manifest)
            self.logger.info(f"Manifest æ¯”è¼ƒå®Œæˆ: {orig_name} vs {exp_name}")
            self.logger.info(f"çµæœ: {summary}")
            
            if not is_identical and len(differences) <= 10:  # åªé¡¯ç¤ºå°‘é‡å·®ç•°çš„è©³ç´°ä¿¡æ¯
                for i, diff in enumerate(differences[:5], 1):
                    self.logger.debug(f"å·®ç•° {i}: {diff['path']} - {diff['type']}")
                    if len(differences) > 5:
                        self.logger.debug(f"... é‚„æœ‰ {len(differences)-5} å€‹å·®ç•°")
            
            return result
            
        except Exception as e:
            error_msg = f"Manifest æ¯”è¼ƒéç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            self.logger.error(error_msg)
            return {
                'is_identical': False,
                'error': error_msg,
                'summary': 'æ¯”è¼ƒéç¨‹å‡ºéŒ¯'
            }
    
    def _deep_compare(self, obj1: dict, obj2: dict, differences: list, path: str = '') -> bool:
        """
        æ·±åº¦æ¯”è¼ƒå…©å€‹æ¨™æº–åŒ–çš„ XML ç‰©ä»¶
        
        Args:
            obj1, obj2: è¦æ¯”è¼ƒçš„ç‰©ä»¶
            differences: ç”¨æ–¼æ”¶é›†å·®ç•°çš„åˆ—è¡¨
            path: ç•¶å‰æ¯”è¼ƒçš„è·¯å¾‘ï¼ˆç”¨æ–¼éŒ¯èª¤å ±å‘Šï¼‰
            
        Returns:
            æ˜¯å¦å®Œå…¨ç›¸åŒ
        """
        is_identical = True
        
        # æ¯”è¼ƒæ¨™ç±¤å
        if obj1['tag'] != obj2['tag']:
            differences.append({
                'path': path,
                'type': 'tag_different',
                'expected': obj1['tag'],
                'actual': obj2['tag']
            })
            is_identical = False
        
        # æ¯”è¼ƒæ–‡æœ¬å…§å®¹
        if obj1['text'] != obj2['text']:
            differences.append({
                'path': f"{path}/text",
                'type': 'text_different',
                'expected': obj1['text'],
                'actual': obj2['text']
            })
            is_identical = False
        
        # æ¯”è¼ƒå±¬æ€§
        if obj1['attrib'] != obj2['attrib']:
            # æª¢æŸ¥ç¼ºå¤±çš„å±¬æ€§
            for key in obj1['attrib']:
                if key not in obj2['attrib']:
                    differences.append({
                        'path': f"{path}/@{key}",
                        'type': 'attribute_missing',
                        'expected': obj1['attrib'][key],
                        'actual': None
                    })
                    is_identical = False
                elif obj1['attrib'][key] != obj2['attrib'][key]:
                    differences.append({
                        'path': f"{path}/@{key}",
                        'type': 'attribute_different',
                        'expected': obj1['attrib'][key],
                        'actual': obj2['attrib'][key]
                    })
                    is_identical = False
            
            # æª¢æŸ¥é¡å¤–çš„å±¬æ€§
            for key in obj2['attrib']:
                if key not in obj1['attrib']:
                    differences.append({
                        'path': f"{path}/@{key}",
                        'type': 'attribute_extra',
                        'expected': None,
                        'actual': obj2['attrib'][key]
                    })
                    is_identical = False
        
        # æ¯”è¼ƒå­å…ƒç´ æ•¸é‡
        if len(obj1['children']) != len(obj2['children']):
            differences.append({
                'path': f"{path}/children_count",
                'type': 'children_count_different',
                'expected': len(obj1['children']),
                'actual': len(obj2['children'])
            })
            is_identical = False
        
        # æ¯”è¼ƒå­å…ƒç´ ï¼ˆé…å°æ¯”è¼ƒï¼‰
        min_children = min(len(obj1['children']), len(obj2['children']))
        for i in range(min_children):
            child1 = obj1['children'][i]
            child2 = obj2['children'][i]
            
            # æ§‹å»ºå­è·¯å¾‘
            child_path = f"{path}/{child1['tag']}"
            if 'name' in child1['attrib']:
                child_path += f"[@name='{child1['attrib']['name']}']"
            elif 'path' in child1['attrib']:
                child_path += f"[@path='{child1['attrib']['path']}']"
            else:
                child_path += f"[{i}]"
            
            # éè¿´æ¯”è¼ƒå­å…ƒç´ 
            child_identical = self._deep_compare(child1, child2, differences, child_path)
            if not child_identical:
                is_identical = False
        
        return is_identical
    
    def generate_diff_report(self, comparison_result: dict, output_file: str = None) -> str:
        """
        ç”Ÿæˆè©³ç´°çš„å·®ç•°å ±å‘Š
        
        Args:
            comparison_result: compare_manifests çš„çµæœ
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
            
        Returns:
            å·®ç•°å ±å‘Šçš„æ–‡å­—å…§å®¹
        """
        if comparison_result.get('error'):
            report = f"æ¯”è¼ƒå¤±æ•—: {comparison_result['error']}"
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report)
            return report
        
        lines = []
        lines.append("="*80)
        lines.append("Manifest æ¯”è¼ƒå ±å‘Š")
        lines.append("="*80)
        lines.append(f"åŸå§‹æª”æ¡ˆ: {comparison_result['original_manifest']}")
        lines.append(f"å°å‡ºæª”æ¡ˆ: {comparison_result['exported_manifest']}")
        lines.append(f"åŸå§‹å¤§å°: {comparison_result['original_size']:,} bytes")
        lines.append(f"å°å‡ºå¤§å°: {comparison_result['exported_size']:,} bytes")
        lines.append(f"æ¯”è¼ƒæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        
        if comparison_result['is_identical']:
            lines.append("âœ… çµæœ: å…©å€‹ manifest æª”æ¡ˆå®Œå…¨ç›¸åŒ")
        else:
            lines.append(f"âŒ çµæœ: ç™¼ç¾ {len(comparison_result['differences'])} è™•å·®ç•°")
            lines.append("")
            lines.append("å·®ç•°è©³æƒ…:")
            lines.append("-" * 60)
            
            for i, diff in enumerate(comparison_result['differences'], 1):
                lines.append(f"{i:3d}. è·¯å¾‘: {diff['path']}")
                lines.append(f"     é¡å‹: {diff['type']}")
                lines.append(f"     é æœŸ: {diff.get('expected', 'N/A')}")
                lines.append(f"     å¯¦éš›: {diff.get('actual', 'N/A')}")
                lines.append("")
        
        lines.append("="*80)
        
        report = "\n".join(lines)
        
        if output_file:
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                self.logger.info(f"å·®ç•°å ±å‘Šå·²ä¿å­˜è‡³: {output_file}")
            except Exception as e:
                self.logger.error(f"ä¿å­˜å·®ç•°å ±å‘Šå¤±æ•—: {e}")
        
        return report

# =====================================
# ===== é…ç½®ç®¡ç†å™¨ =====
# =====================================

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰é…ç½®çš„å„ªå…ˆç´šå’Œä¾†æº"""
    
    def __init__(self):
        # åŸºç¤é…ç½®
        self.sftp_config = {
            'host': 'mmsftpx.realtek.com',
            'port': 22,
            'username': 'lgwar_user',
            'password': 'Ab!123456',
            'timeout': 30,
            'retry_count': 3,
            'retry_delay': 2
        }
        
        self.jira_config = {
            'site': 'jira.realtek.com',
            'username': 'vince_lin',
            'password': 'Amon200!Amon200!',
            'api_url': 'https://jira.realtek.com/rest/api/2'
        }
        
        self.path_config = {
            'default_output_dir': './DB-source',
            'default_mapping_table': '../all_chip_mapping_table.xlsx',
            'manifest_pattern': r'manifest_(\d+)\.xml',
            'report_filename': 'pinning_report.xlsx'
        }
        
        self.repo_config = {
            'repo_command': 'repo',
            'sync_jobs': 8,
            'sync_retry': 2,
            'init_timeout': 300,
            'sync_timeout': 7200,
            'async_sync': True,
            'show_sync_output': False,
            # ğŸ”¥ æ–°å¢æ¸…ç†ç›¸é—œé…ç½®
            'check_clean_before_sync': True,  # æ˜¯å¦åœ¨syncå‰æª¢æŸ¥ä¹¾æ·¨ç‹€æ…‹
            'auto_clean_workspace': True,     # æ˜¯å¦è‡ªå‹•æ¸…ç†å·¥ä½œç©ºé–“
            'backup_local_changes': True,     # æ˜¯å¦å‚™ä»½æœ¬åœ°ä¿®æ”¹
            'force_clean_on_dirty': True     # é‡åˆ°é«’repoæ˜¯å¦å¼·åˆ¶æ¸…ç†
        }
        
        self.parallel_config = {
            'max_workers': 4,
            'enable_parallel': True,
        }
        
        self.log_config = {
            'level': logging.INFO,
            'format': '%(asctime)s - [%(levelname)s] - %(name)s - %(message)s',
            'date_format': '%Y-%m-%d %H:%M:%S'
        }
        
        # é è¨­åŸ·è¡Œé…ç½®
        self.default_execution_config = {
            'mapping_table': os.path.join(os.path.dirname(__file__), '..', 'all_chip_mapping_table.xlsx'),
            'db_type': 'all',
            'selected_dbs': ['DB2145', 'DB2858', 'DB2575', 'DB2919'],
            'db_versions': {},
            'output_dir': './DB-source',
            'auto_confirm': False,
            'sftp_override': {}
        }
        
        # é…ç½®å„ªå…ˆç´šè¿½è¹¤
        self.config_sources = {}
        
    def apply_overrides(self, overrides: Dict[str, Any], source: str = 'user'):
        """æ‡‰ç”¨é…ç½®è¦†è“‹ä¸¦è¨˜éŒ„ä¾†æº"""
        for key, value in overrides.items():
            if value is not None:
                self.config_sources[key] = source
                # æ ¹æ“š key æ›´æ–°å°æ‡‰çš„é…ç½®
                if key.startswith('sftp_'):
                    sftp_key = key.replace('sftp_', '')
                    if sftp_key in self.sftp_config:
                        self.sftp_config[sftp_key] = value
                elif key.startswith('repo_'):
                    repo_key = key.replace('repo_', '')
                    if repo_key in self.repo_config:
                        self.repo_config[repo_key] = value
                elif key.startswith('parallel_'):
                    parallel_key = key.replace('parallel_', '')
                    if parallel_key in self.parallel_config:
                        self.parallel_config[parallel_key] = value

# å…¨åŸŸé…ç½®ç®¡ç†å™¨å¯¦ä¾‹
config_manager = ConfigManager()

# =====================================
# ===== JIRA API å®¢æˆ¶ç«¯ =====
# =====================================

from jira import JIRA
import requests
from urllib.parse import quote
import urllib3

# é—œé–‰ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class JiraAPIClient:
    """ç°¡åŒ–ç‰ˆ JIRA API å®¢æˆ¶ç«¯"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.session = None
        self._connected = False
        self.base_url = f"https://{config_manager.jira_config['site']}"
        
    def connect(self) -> bool:
        """é€£æ¥åˆ° JIRA"""
        try:
            username = config_manager.jira_config['username']
            password = config_manager.jira_config['password']
            
            self.session = requests.Session()
            self.session.auth = (username, password)
            self.session.headers.update({
                'Accept': 'application/json',
                'Content-Type': 'application/json'
            })
            self.session.verify = False
            
            # æ¸¬è©¦é€£æ¥
            test_url = f"{self.base_url}/rest/api/2/myself"
            response = self.session.get(test_url, timeout=30)
            
            if response.status_code == 200:
                self._connected = True
                return True
            else:
                self.logger.error(f"JIRA é€£æ¥å¤±æ•—: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"JIRA é€£æ¥å¤±æ•—: {e}")
            return False

    def search_db_ticket(self, db_name: str, module: str = None) -> Optional[str]:
        """æ ¹æ“š DB å‘½åæ…£ä¾‹æœå°‹å°æ‡‰çš„ JIRA ticket"""
        try:
            if not self._connected:
                if not self.connect():
                    return None
            
            # æ ¹æ“šå‘½åæ…£ä¾‹ç›´æ¥æ§‹å»º ticket key
            db_number = db_name.replace('DB', '')
            possible_tickets = [
                f"MMQCDB-{db_number}",
                f"LGSWRD-{db_number}",
                f"RTK-{db_number}",
                f"DB-{db_number}",
            ]
            
            for ticket_key in possible_tickets:
                if self._check_ticket_exists(ticket_key):
                    return ticket_key
            
            return None
            
        except Exception as e:
            self.logger.error(f"æœå°‹ JIRA ticket å¤±æ•—: {e}")
            return None

    def _check_ticket_exists(self, ticket_key: str) -> bool:
        """æª¢æŸ¥æŒ‡å®šçš„ ticket æ˜¯å¦å­˜åœ¨"""
        try:
            url = f"{self.base_url}/rest/api/2/issue/{ticket_key}"
            response = self.session.get(url, timeout=30)
            return response.status_code == 200
        except:
            return False

    def get_source_command_from_ticket(self, ticket_key: str) -> Optional[str]:
        """å¾ JIRA ticket ä¸­æå– source command"""
        try:
            if not self._connected:
                if not self.connect():
                    return None
            
            url = f"{self.base_url}/rest/api/2/issue/{ticket_key}"
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                fields = data.get('fields', {})
                
                # æª¢æŸ¥æè¿°æ¬„ä½
                description = fields.get('description', '')
                if description:
                    cmd = self._extract_repo_command(description)
                    if cmd:
                        return cmd
                
                # æª¢æŸ¥è©•è«–
                comments_data = fields.get('comment', {})
                comments = comments_data.get('comments', [])
                for comment in comments:
                    body = comment.get('body', '')
                    if body:
                        cmd = self._extract_repo_command(body)
                        if cmd:
                            return cmd
                
                return None
            
            return None
                
        except Exception as e:
            self.logger.error(f"å¾ ticket {ticket_key} ç²å– source command å¤±æ•—: {e}")
            return None

    def _extract_repo_command(self, text: str) -> Optional[str]:
        """æå– repo init å‘½ä»¤ - å„ªå…ˆé¸æ“‡æœ‰ -m åƒæ•¸çš„æŒ‡ä»¤"""
        if not text or 'repo init' not in text:
            return None
        
        lines = text.split('\n')
        found_commands = []
        
        for line in lines:
            line = line.strip()
            
            # å°‹æ‰¾åŒ…å« repo init çš„è¡Œ
            if 'repo init' in line:
                repo_start = line.find('repo init')
                if repo_start != -1:
                    cmd = line[repo_start:].strip()
                    cmd = self._clean_command(cmd)
                    if self._is_valid_repo_command(cmd):
                        found_commands.append(cmd)
        
        if not found_commands:
            return None
        
        # ğŸ”¥ å„ªå…ˆé¸æ“‡æœ‰ "-m" åƒæ•¸çš„æŒ‡ä»¤
        commands_with_m = [cmd for cmd in found_commands if '-m ' in cmd]
        if commands_with_m:
            # å¦‚æœæœ‰å¤šå€‹ï¼Œå–ç¬¬ä¸€å€‹
            return commands_with_m[0]
        
        # å¦‚æœæ²’æœ‰ -m åƒæ•¸çš„ï¼Œè¿”å›ç¬¬ä¸€å€‹æœ‰æ•ˆçš„
        return found_commands[0]

    def _clean_command(self, cmd: str) -> str:
        """æ¸…ç†å‘½ä»¤å­—ç¬¦ä¸²"""
        # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´
        prefixes = ['$', '>', '#', '//', '*', '-', '=>']
        for prefix in prefixes:
            if cmd.startswith(prefix):
                cmd = cmd[len(prefix):].strip()
        
        cmd = cmd.rstrip('\n\r\\')
        return cmd.strip()

    def _is_valid_repo_command(self, cmd: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ repo init å‘½ä»¤"""
        if not cmd or not cmd.startswith('repo init'):
            return False
        
        required_parts = ['-u', '-b']
        return all(part in cmd for part in required_parts)

    def disconnect(self):
        """æ–·é–‹ JIRA é€£æ¥"""
        self._connected = False
        if self.session:
            self.session.close()
            self.session = None

# =====================================
# ===== æ—¥èªŒè¨­å®šå‡½å¼ =====
# =====================================

def setup_logger(name: str = __name__) -> logging.Logger:
    """è¨­å®šæ—¥èªŒè¨˜éŒ„å™¨ - ä¿®å¾©é‡è¤‡æ—¥èªŒå•é¡Œ"""
    logger = logging.getLogger(name)
    
    # ğŸ”¥ ä¿®å¾©ï¼šå¦‚æœ logger å·²ç¶“æœ‰ handlersï¼Œå…ˆæ¸…é™¤é¿å…é‡è¤‡
    if logger.handlers:
        logger.handlers.clear()
    
    logger.setLevel(config_manager.log_config['level'])
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(config_manager.log_config['level'])
    
    # Formatter
    formatter = logging.Formatter(
        config_manager.log_config['format'],
        datefmt=config_manager.log_config['date_format']
    )
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    
    # ğŸ”¥ é˜²æ­¢æ—¥èªŒå‘ä¸Šå‚³æ’­é€ æˆé‡è¤‡
    logger.propagate = False
    
    return logger

logger = setup_logger(__name__)

# =====================================
# ===== é€²åº¦ç‹€æ…‹å®šç¾© =====
# =====================================

class DBStatus(Enum):
    """DB è™•ç†ç‹€æ…‹"""
    PENDING = "ç­‰å¾…ä¸­"
    DOWNLOADING_MANIFEST = "ä¸‹è¼‰ manifest"
    CHECKING_REPO = "æª¢æŸ¥ repo ç‹€æ…‹"
    REPO_INIT = "åŸ·è¡Œ repo init"
    REPO_SYNC = "åŸ·è¡Œ repo sync"
    EXPORTING = "å°å‡ºç‰ˆæœ¬"
    COMPARING = "æ¯”è¼ƒ manifest"  # ğŸ”¥ æ–°å¢æ¯”è¼ƒç‹€æ…‹
    SUCCESS = "âœ… å®Œæˆ"
    SUCCESS_WITH_DIFF = "âœ… å®Œæˆ(æœ‰å·®ç•°)"  # ğŸ”¥ æ–°å¢æœ‰å·®ç•°ä½†æˆåŠŸçš„ç‹€æ…‹
    FAILED = "âŒ å¤±æ•—"
    SKIPPED = "â­ï¸ è·³é"

# =====================================
# ===== è³‡æ–™çµæ§‹å®šç¾© =====
# =====================================

@dataclass
class DBInfo:
    """DB è³‡è¨Šè³‡æ–™çµæ§‹ - æ–°å¢ manifest æ¯”è¼ƒç›¸é—œæ¬„ä½"""
    sn: int
    module: str
    db_type: str
    db_info: str
    db_folder: str
    sftp_path: str
    version: Optional[str] = None
    jira_link: Optional[str] = None
    source_command: Optional[str] = None
    manifest_file: Optional[str] = None
    manifest_full_path: Optional[str] = None
    local_path: Optional[str] = None
    has_existing_repo: bool = False
    status: DBStatus = DBStatus.PENDING
    error_message: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    sync_process: Optional[subprocess.Popen] = None
    actual_source_cmd: Optional[str] = None
    sync_log_path: Optional[str] = None
    
    # ğŸ”¥ æ–°å¢ manifest æ¯”è¼ƒç›¸é—œæ¬„ä½
    exported_manifest_path: Optional[str] = None
    manifest_comparison_result: Optional[dict] = None
    manifest_is_identical: Optional[bool] = None
    manifest_differences_count: Optional[int] = None
    manifest_comparison_summary: Optional[str] = None

    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        
        # è™•ç† Enum å’Œ datetime
        if isinstance(result['status'], DBStatus):
            result['status'] = result['status'].value
        if result['start_time']:
            result['start_time'] = result['start_time'].strftime('%Y-%m-%d %H:%M:%S')
        if result['end_time']:
            result['end_time'] = result['end_time'].strftime('%Y-%m-%d %H:%M:%S')
        
        # ç§»é™¤ç„¡æ³•åºåˆ—åŒ–çš„ç‰©ä»¶
        result.pop('sync_process', None)
        result.pop('manifest_comparison_result', None)  # æ¯”è¼ƒçµæœå¤ªè¤‡é›œï¼Œä¸åºåˆ—åŒ–
        
        return result

@dataclass
class PinningReport:
    """å®šç‰ˆå ±å‘Šè³‡æ–™çµæ§‹"""
    total_dbs: int = 0
    successful_dbs: int = 0
    failed_dbs: int = 0
    skipped_dbs: int = 0
    dbs_with_differences: int = 0  # ğŸ”¥ æ–°å¢ï¼šæœ‰å·®ç•°çš„ DB æ•¸é‡
    db_details: List[DBInfo] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    
    def add_db(self, db_info: DBInfo):
        # ğŸ”¥ æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¨˜éŒ„
        for i, existing_db in enumerate(self.db_details):
            if (existing_db.db_info == db_info.db_info and 
                existing_db.version == db_info.version and
                existing_db.db_type == db_info.db_type and
                existing_db.module == db_info.module):
                # æ›´æ–°ç¾æœ‰è¨˜éŒ„è€Œä¸æ˜¯æ·»åŠ æ–°çš„
                self.db_details[i] = db_info
                self._recalculate_stats()
                return
        
        # å¦‚æœä¸å­˜åœ¨ï¼Œæ‰æ·»åŠ æ–°è¨˜éŒ„
        self.db_details.append(db_info)
        self._update_stats(db_info)

    def _recalculate_stats(self):
        """é‡æ–°è¨ˆç®—çµ±è¨ˆæ•¸æ“š"""
        self.successful_dbs = sum(1 for db in self.db_details 
                                if db.status in [DBStatus.SUCCESS, DBStatus.SUCCESS_WITH_DIFF])
        self.failed_dbs = sum(1 for db in self.db_details if db.status == DBStatus.FAILED)
        self.skipped_dbs = sum(1 for db in self.db_details if db.status == DBStatus.SKIPPED)
        self.dbs_with_differences = sum(1 for db in self.db_details if db.status == DBStatus.SUCCESS_WITH_DIFF)

    def _update_stats(self, db_info: DBInfo):
        """æ›´æ–°çµ±è¨ˆæ•¸æ“šï¼ˆæ–°å¢è¨˜éŒ„æ™‚ï¼‰"""
        if db_info.status == DBStatus.SUCCESS:
            self.successful_dbs += 1
        elif db_info.status == DBStatus.SUCCESS_WITH_DIFF:
            self.successful_dbs += 1
            self.dbs_with_differences += 1
        elif db_info.status == DBStatus.FAILED:
            self.failed_dbs += 1
        elif db_info.status == DBStatus.SKIPPED:
            self.skipped_dbs += 1
                
    def finalize(self):
        """å®Œæˆå ±å‘Š"""
        self.end_time = datetime.now()
        self.total_dbs = len(self.db_details)

# =====================================
# ===== è³‡æºç®¡ç†å™¨ =====
# =====================================

class ResourceManager:
    """çµ±ä¸€ç®¡ç†æ‰€æœ‰ç³»çµ±è³‡æº"""
    
    def __init__(self):
        self.active_processes = {}
        self.sftp_connections = []
        self.lock = threading.Lock()
        self.logger = setup_logger(self.__class__.__name__)
        
        atexit.register(self.cleanup_all)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
        print(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ¸…ç†æ‰€æœ‰é€²ç¨‹...")
        
        # åŸæœ‰çš„æ¸…ç†
        self.cleanup_all()
        
        # ğŸ”¥ æ–°å¢ï¼šç³»çµ±ç´šå¼·åˆ¶æ¸…ç†
        print("ğŸš¨ åŸ·è¡Œç³»çµ±ç´šæ¸…ç†...")
        os.system("pkill -TERM -f 'repo sync' 2>/dev/null || true")
        os.system("pkill -TERM -f 'unbuffer.*repo' 2>/dev/null || true")
        time.sleep(2)
        os.system("pkill -KILL -f 'repo sync' 2>/dev/null || true")
        os.system("pkill -KILL -f 'unbuffer.*repo' 2>/dev/null || true")
        
        print("âœ… æ¸…ç†å®Œæˆ")
        sys.exit(0)
    
    def register_process(self, name: str, process: subprocess.Popen):
        """è¨»å†Šæ–°çš„å­é€²ç¨‹"""
        with self.lock:
            self.active_processes[name] = process
    
    def unregister_process(self, name: str):
        """å–æ¶ˆè¨»å†Šå­é€²ç¨‹"""
        with self.lock:
            if name in self.active_processes:
                del self.active_processes[name]
    
    def register_sftp(self, connection):
        """è¨»å†Š SFTP é€£ç·š"""
        with self.lock:
            self.sftp_connections.append(connection)
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰è³‡æº"""
        with self.lock:
            # çµ‚æ­¢æ‰€æœ‰å­é€²ç¨‹
            for name, process in self.active_processes.items():
                try:
                    if process.poll() is None:
                        process.terminate()
                        process.wait(timeout=5)
                except:
                    try:
                        process.kill()
                    except:
                        pass
            
            # é—œé–‰æ‰€æœ‰ SFTP é€£ç·š
            for conn in self.sftp_connections:
                try:
                    if hasattr(conn, 'disconnect'):
                        conn.disconnect()
                    elif hasattr(conn, 'close'):
                        conn.close()
                except:
                    pass
            
            self.active_processes.clear()
            self.sftp_connections.clear()

# å…¨åŸŸè³‡æºç®¡ç†å™¨
resource_manager = ResourceManager()

# =====================================
# ===== æ”¹é€²ç‰ˆ SFTP ç®¡ç†å™¨ï¼ˆä¿®å¾© Garbage packet å•é¡Œï¼‰=====
# =====================================

class SFTPManager:
    """æ”¹é€²ç‰ˆ SFTP ç®¡ç†å™¨ - ä¿®å¾© type 3 unimplemented éŒ¯èª¤"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.config = config_manager.sftp_config
        self.sftp = None
        self.transport = None
        self.connected = False
        self._connection_lock = threading.Lock()
        # å¿«å–ä¼ºæœå™¨èƒ½åŠ›
        self._server_capabilities = {
            'supports_listdir_attr': None,
            'supports_stat': None,
            'checked': False
        }
        
    def connect(self) -> bool:
        """å»ºç«‹ SFTP é€£ç·š"""
        with self._connection_lock:
            try:
                # å¦‚æœå·²é€£ç·šï¼Œå…ˆæ¸…ç†
                if self.connected:
                    self.disconnect()
                
                self.logger.info(f"å»ºç«‹ SFTP é€£ç·š: {self.config['host']}:{self.config['port']}")
                
                # é‡è©¦é€£ç·š
                for attempt in range(self.config['retry_count']):
                    try:
                        # å»ºç«‹ Socket é€£ç·š
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(self.config['timeout'])
                        
                        # è¨­å®š socket é¸é …é¿å… Garbage packet
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                        
                        # é€£æ¥åˆ°ä¼ºæœå™¨
                        sock.connect((self.config['host'], self.config['port']))
                        
                        # å»ºç«‹ Transportï¼ˆä¿®å¾© Garbage packet çš„é—œéµï¼‰
                        self.transport = paramiko.Transport(sock)
                        
                        # è¨­å®šè¼ƒä¿å®ˆçš„åƒæ•¸é¿å…å”è­°å•é¡Œ
                        self.transport.set_keepalive(30)
                        self.transport.use_compression(False)  # é—œé–‰å£“ç¸®
                        
                        # é–‹å§‹ SSH æ¡æ‰‹
                        self.transport.start_client()
                        
                        # èªè­‰
                        self.transport.auth_password(
                            self.config['username'],
                            self.config['password']
                        )
                        
                        # å»ºç«‹ SFTP å®¢æˆ¶ç«¯
                        self.sftp = paramiko.SFTPClient.from_transport(self.transport)
                        
                        # æ¸¬è©¦é€£ç·šä¸¦æª¢æ¸¬ä¼ºæœå™¨èƒ½åŠ›
                        self._detect_server_capabilities()
                        
                        self.connected = True
                        resource_manager.register_sftp(self)
                        self.logger.info(f"SFTP é€£ç·šæˆåŠŸ (å˜—è©¦ {attempt + 1})")
                        return True
                        
                    except Exception as e:
                        self.logger.warning(f"SFTP é€£ç·šå˜—è©¦ {attempt + 1} å¤±æ•—: {e}")
                        self._cleanup_failed_connection()
                        
                        if attempt < self.config['retry_count'] - 1:
                            time.sleep(self.config['retry_delay'])
                
                self.logger.error("SFTP é€£ç·šå¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                return False
                
            except Exception as e:
                self.logger.error(f"SFTP é€£ç·šéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                self._cleanup_failed_connection()
                return False
    
    def _detect_server_capabilities(self):
        """æª¢æ¸¬ SFTP ä¼ºæœå™¨æ”¯æ´çš„åŠŸèƒ½"""
        try:
            self.logger.debug("æª¢æ¸¬ SFTP ä¼ºæœå™¨èƒ½åŠ›...")
            
            # æ¸¬è©¦åŸºæœ¬ listdir
            try:
                self.sftp.listdir('.')
                self.logger.debug("âœ… åŸºæœ¬ listdir æ”¯æ´")
            except Exception as e:
                self.logger.warning(f"âŒ åŸºæœ¬ listdir ä¸æ”¯æ´: {e}")
                raise Exception("ä¼ºæœå™¨ä¸æ”¯æ´åŸºæœ¬ SFTP æ“ä½œ")
            
            # æ¸¬è©¦ listdir_attr
            try:
                self.sftp.listdir_attr('.')
                self._server_capabilities['supports_listdir_attr'] = True
                self.logger.debug("âœ… listdir_attr æ”¯æ´")
            except Exception as e:
                self._server_capabilities['supports_listdir_attr'] = False
                self.logger.debug(f"âŒ listdir_attr ä¸æ”¯æ´: {e}")
            
            # æ¸¬è©¦ stat
            try:
                self.sftp.stat('.')
                self._server_capabilities['supports_stat'] = True
                self.logger.debug("âœ… stat æ”¯æ´")
            except Exception as e:
                self._server_capabilities['supports_stat'] = False
                self.logger.debug(f"âŒ stat ä¸æ”¯æ´: {e}")
            
            self._server_capabilities['checked'] = True
            self.logger.info(f"ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬å®Œæˆ: listdir_attr={self._server_capabilities['supports_listdir_attr']}, stat={self._server_capabilities['supports_stat']}")
            
        except Exception as e:
            self.logger.warning(f"ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬å¤±æ•—: {e}")
            # è¨­å®šç‚ºæœ€ä¿å®ˆçš„æ¨¡å¼
            self._server_capabilities = {
                'supports_listdir_attr': False,
                'supports_stat': False,
                'checked': True
            }
    
    def _cleanup_failed_connection(self):
        """æ¸…ç†å¤±æ•—çš„é€£ç·š"""
        try:
            if self.sftp:
                self.sftp.close()
                self.sftp = None
            if self.transport:
                self.transport.close()
                self.transport = None
            self.connected = False
            self._server_capabilities['checked'] = False
        except:
            pass
    
    def disconnect(self):
        """å®‰å…¨æ–·é–‹ SFTP é€£ç·š"""
        with self._connection_lock:
            try:
                if self.sftp:
                    self.sftp.close()
                    self.sftp = None
                if self.transport:
                    self.transport.close()
                    self.transport = None
                self.connected = False
                self._server_capabilities['checked'] = False
                self.logger.info("SFTP é€£ç·šå·²é—œé–‰")
            except Exception as e:
                self.logger.warning(f"é—œé–‰ SFTP é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _ensure_connected(self) -> bool:
        """ç¢ºä¿é€£ç·šæœ‰æ•ˆ"""
        if not self.connected or not self.sftp or not self.transport:
            return self.connect()
        
        # æ¸¬è©¦é€£ç·šæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        try:
            self.sftp.listdir('.')
            return True
        except:
            self.logger.info("SFTP é€£ç·šå·²æ–·é–‹ï¼Œé‡æ–°é€£ç·š...")
            return self.connect()
    
    def get_latest_version(self, sftp_path: str) -> Tuple[str, str, str]:
        """
        å–å¾—æœ€æ–°ç‰ˆæœ¬è³‡è¨Šï¼ˆç‰ˆè™Ÿæœ€å¤§çš„ï¼‰
        
        Args:
            sftp_path: SFTP è·¯å¾‘
            
        Returns:
            (db_folder, db_version, full_path)
        """
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            # å¾è·¯å¾‘ä¸­æå–åŸºç¤è³‡è¨Š
            path_parts = sftp_path.rstrip('/').split('/')
            db_folder = path_parts[-1] if path_parts else ''
            
            # åˆ—å‡ºç›®éŒ„å…§å®¹
            try:
                items = self._safe_listdir(sftp_path)
            except:
                self.logger.warning(f"ç„¡æ³•åˆ—å‡ºç›®éŒ„: {sftp_path}")
                return db_folder, '', sftp_path
            
            # éæ¿¾ç‰ˆæœ¬è³‡æ–™å¤¾ï¼ˆæ ¼å¼: æ•¸å­—é–‹é ­ï¼Œå¦‚ 536_all_202507312300ï¼‰
            version_folders = []
            for item in items:
                # æª¢æŸ¥æ˜¯å¦ç‚ºç‰ˆæœ¬è³‡æ–™å¤¾æ ¼å¼
                match = re.match(r'^(\d+)', item)
                if match:
                    version_num = int(match.group(1))
                    version_folders.append((version_num, item))
            
            if not version_folders:
                self.logger.warning(f"æ‰¾ä¸åˆ°ç‰ˆæœ¬è³‡æ–™å¤¾: {sftp_path}")
                return db_folder, '', sftp_path
            
            # æŒ‰ç‰ˆæœ¬è™Ÿæ’åºï¼Œå–æœ€å¤§çš„ï¼ˆæœ€æ–°çš„ï¼‰
            version_folders.sort(key=lambda x: x[0], reverse=True)
            latest_version = version_folders[0][1]
            full_path = f"{sftp_path}/{latest_version}"
            
            self.logger.info(f"æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬: {latest_version} (ç‰ˆè™Ÿ: {version_folders[0][0]}) in {sftp_path}")
            return db_folder, latest_version, full_path
            
        except Exception as e:
            self.logger.error(f"å–å¾—ç‰ˆæœ¬è³‡è¨Šå¤±æ•—: {str(e)}")
            return '', '', sftp_path
    
    def get_specific_version(self, sftp_path: str, db_info: str) -> Tuple[str, str, str]:
        """
        å–å¾—ç‰¹å®šç‰ˆæœ¬è³‡è¨Š
        
        Args:
            sftp_path: SFTP åŸºç¤è·¯å¾‘
            db_info: DBè³‡è¨Š (æ ¼å¼: DB2302#196)
            
        Returns:
            (db_folder, db_version, full_path)
        """
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            # è§£æ db_info
            if '#' not in db_info:
                self.logger.warning(f"DBè³‡è¨Šæ ¼å¼éŒ¯èª¤: {db_info}")
                return self.get_latest_version(sftp_path)
            
            db_number, version_prefix = db_info.split('#')
            
            # åˆ—å‡ºç›®éŒ„æ‰¾åˆ°å°æ‡‰çš„ DB è³‡æ–™å¤¾
            parent_path = '/'.join(sftp_path.rstrip('/').split('/')[:-1])
            items = self._safe_listdir(parent_path)
            
            db_folder = None
            for item in items:
                if item.startswith(f"{db_number}_"):
                    db_folder = item
                    break
            
            if not db_folder:
                self.logger.warning(f"æ‰¾ä¸åˆ° DB è³‡æ–™å¤¾: {db_number}")
                return '', '', sftp_path
            
            # å»ºæ§‹å®Œæ•´è·¯å¾‘
            db_path = f"{parent_path}/{db_folder}"
            
            # åˆ—å‡ºç‰ˆæœ¬è³‡æ–™å¤¾
            version_items = self._safe_listdir(db_path)
            
            # æ‰¾åˆ°å°æ‡‰ç‰ˆæœ¬
            for version_item in version_items:
                if version_item.startswith(f"{version_prefix}_"):
                    full_path = f"{db_path}/{version_item}"
                    self.logger.info(f"æ‰¾åˆ°æŒ‡å®šç‰ˆæœ¬: {version_item}")
                    return db_folder, version_item, full_path
            
            self.logger.warning(f"æ‰¾ä¸åˆ°æŒ‡å®šç‰ˆæœ¬: {version_prefix}")
            return db_folder, '', db_path
            
        except Exception as e:
            self.logger.error(f"å–å¾—ç‰¹å®šç‰ˆæœ¬å¤±æ•—: {str(e)}")
            return '', '', sftp_path
    
    def find_latest_manifest(self, path: str, db_name: str = None, target_version: str = None) -> Optional[Tuple[str, str]]:
        """æ”¹é€²ç‰ˆï¼šå¿«é€Ÿæœå°‹ manifest æª”æ¡ˆï¼ˆå¼·åŒ–éŒ¯èª¤è™•ç†ï¼‰"""
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            if target_version:
                self.logger.info(f"å¿«é€Ÿæœå°‹æŒ‡å®šç‰ˆæœ¬ {target_version}: {path}")
                return self._find_specific_version_manifest(path, target_version)
            else:
                self.logger.info(f"å¿«é€Ÿæœå°‹æœ€æ–°ç‰ˆæœ¬: {path}")
                return self._find_latest_version_manifest(path)
                
        except Exception as e:
            self.logger.error(f"æœç´¢ manifest å¤±æ•—: {e}")
            return None

    def _find_latest_version_manifest(self, base_path: str) -> Optional[Tuple[str, str]]:
        """æœå°‹æœ€æ–°ç‰ˆæœ¬çš„ manifest - å¼·åŒ– type 3 éŒ¯èª¤è™•ç†"""
        try:
            # åˆ—å‡ºç›®éŒ„
            version_dirs = []
            try:
                items = self._safe_listdir_with_details(base_path)
                
                for item_info in items:
                    try:
                        if item_info.get('is_dir', False):
                            dir_name = item_info['name']
                            version_num = self._extract_version_number(dir_name)
                            if version_num:
                                version_dirs.append({
                                    'name': dir_name,
                                    'version': int(version_num),
                                    'mtime': item_info.get('mtime', 0)
                                })
                    except Exception as e:
                        self.logger.debug(f"è·³éå•é¡Œç›®éŒ„: {item_info.get('name', 'unknown')}, éŒ¯èª¤: {e}")
                        continue
                        
            except Exception as e:
                self.logger.error(f"åˆ—å‡ºç›®éŒ„å¤±æ•—: {e}")
                return None
            
            if not version_dirs:
                raise Exception(f"åœ¨ {base_path} ä¸­æ²’æœ‰æ‰¾åˆ°ç‰ˆæœ¬ç›®éŒ„")
            
            # æŒ‰ç‰ˆæœ¬è™Ÿé™åºæ’åˆ—ï¼ˆæœ€æ–°ç‰ˆæœ¬åœ¨å‰ï¼‰
            version_dirs.sort(key=lambda x: (x['version'], x['mtime']), reverse=True)
            self.logger.info(f"æ‰¾åˆ° {len(version_dirs)} å€‹ç‰ˆæœ¬ç›®éŒ„ï¼Œæœ€æ–°ç‰ˆæœ¬: {version_dirs[0]['version']}")
            
            # åªæª¢æŸ¥å‰3å€‹æœ€æ–°ç‰ˆæœ¬
            for version_dir in version_dirs[:3]:
                self.logger.info(f"æª¢æŸ¥ç‰ˆæœ¬ {version_dir['version']} ({version_dir['name']})")
                manifest_path = self._try_get_manifest_from_dir(
                    base_path, 
                    version_dir['name'], 
                    str(version_dir['version'])
                )
                if manifest_path:
                    return manifest_path
            
            raise Exception("åœ¨æœ€æ–°ç‰ˆæœ¬ç›®éŒ„ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ manifest")
            
        except Exception as e:
            self.logger.error(f"æœå°‹æœ€æ–°ç‰ˆæœ¬å¤±æ•—: {e}")
            return None
        
    def _find_specific_version_manifest(self, base_path: str, version: str) -> Optional[Tuple[str, str]]:
        """æœå°‹æŒ‡å®šç‰ˆæœ¬çš„ manifest - å¼·åŒ– type 3 éŒ¯èª¤è™•ç†"""
        try:
            self.logger.info(f"æœå°‹æŒ‡å®šç‰ˆæœ¬: {version}")
            
            # å˜—è©¦åˆ—å‡ºç›®éŒ„ï¼Œå¦‚æœå¤±æ•—å°±ç”¨ç›´æ¥è·¯å¾‘æ–¹å¼
            matching_dirs = []
            try:
                items = self._safe_listdir_with_details(base_path)
                
                for item_info in items:
                    try:
                        # æª¢æŸ¥æ˜¯å¦ç‚ºç›®éŒ„
                        if item_info.get('is_dir', False):
                            dir_name = item_info['name']
                            if self._matches_version(dir_name, version):
                                matching_dirs.append({
                                    'name': dir_name,
                                    'mtime': item_info.get('mtime', 0)
                                })
                                self.logger.debug(f"æ‰¾åˆ°åŒ¹é…ç‰ˆæœ¬ç›®éŒ„: {dir_name}")
                    except Exception as e:
                        self.logger.debug(f"è·³éå•é¡Œé …ç›®: {item_info.get('name', 'unknown')}, éŒ¯èª¤: {e}")
                        continue
                        
            except Exception as e:
                self.logger.warning(f"åˆ—å‡ºç›®éŒ„å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è·¯å¾‘: {e}")
                return self._try_direct_version_paths(base_path, version)
            
            if not matching_dirs:
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦ç›´æ¥æ§‹å»ºè·¯å¾‘
                return self._try_direct_version_paths(base_path, version)
            
            # æŒ‰æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
            matching_dirs.sort(key=lambda x: x['mtime'], reverse=True)
            
            # å˜—è©¦æ¯å€‹åŒ¹é…çš„ç‰ˆæœ¬ç›®éŒ„
            for dir_info in matching_dirs:
                manifest_path = self._try_get_manifest_from_dir(base_path, dir_info['name'], version)
                if manifest_path:
                    return manifest_path
            
            # å¦‚æœåŒ¹é…ç›®éŒ„éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è·¯å¾‘
            return self._try_direct_version_paths(base_path, version)
            
        except Exception as e:
            self.logger.error(f"æœå°‹æŒ‡å®šç‰ˆæœ¬å¤±æ•—: {e}")
            return None

    def _safe_listdir(self, path: str) -> list:
        """æœ€å®‰å…¨çš„åˆ—ç›®éŒ„æ–¹æ³• - åªä½¿ç”¨åŸºæœ¬ listdir"""
        try:
            return self.sftp.listdir(path)
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºç›®éŒ„å¤±æ•—: {path}, éŒ¯èª¤: {e}")
            raise
    
    def _safe_listdir_with_details(self, path: str) -> list:
        """å®‰å…¨çš„åˆ—å‡ºç›®éŒ„å…§å®¹ï¼Œæ ¹æ“šä¼ºæœå™¨èƒ½åŠ›é¸æ“‡æ–¹æ³•"""
        try:
            result = []
            
            # å¦‚æœæ”¯æ´ listdir_attrï¼Œå„ªå…ˆä½¿ç”¨
            if self._server_capabilities.get('supports_listdir_attr', False):
                try:
                    items = self.sftp.listdir_attr(path)
                    for item in items:
                        result.append({
                            'name': item.filename,
                            'is_dir': self._is_directory_from_stat(item),
                            'mtime': getattr(item, 'st_mtime', 0),
                            'size': getattr(item, 'st_size', 0)
                        })
                    return result
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self.logger.warning(f"listdir_attr ä¸æ”¯æ´ï¼Œåˆ‡æ›åˆ°åŸºæœ¬æ¨¡å¼: {e}")
                        self._server_capabilities['supports_listdir_attr'] = False
                    else:
                        raise
            
            # å›é€€åˆ°åŸºæœ¬ listdir + å€‹åˆ¥ stat
            self.logger.debug("ä½¿ç”¨åŸºæœ¬ listdir æ¨¡å¼")
            filenames = self.sftp.listdir(path)
            
            for filename in filenames:
                item_info = {
                    'name': filename,
                    'is_dir': False,  # é è¨­ç‚ºæª”æ¡ˆ
                    'mtime': 0,
                    'size': 0
                }
                
                # å˜—è©¦ç²å–è©³ç´°è³‡è¨Š
                if self._server_capabilities.get('supports_stat', True):
                    try:
                        full_path = f"{path}/{filename}"
                        stat_info = self.sftp.stat(full_path)
                        item_info.update({
                            'is_dir': self._is_directory_from_stat(stat_info),
                            'mtime': getattr(stat_info, 'st_mtime', 0),
                            'size': getattr(stat_info, 'st_size', 0)
                        })
                    except Exception as e:
                        error_msg = str(e).lower()
                        if 'unimplemented' in error_msg or 'type 3' in error_msg:
                            self.logger.debug(f"stat ä¸æ”¯æ´: {filename}")
                            self._server_capabilities['supports_stat'] = False
                        else:
                            self.logger.debug(f"stat å¤±æ•—: {filename}, {e}")
                        
                        # ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„
                        item_info['is_dir'] = self._guess_is_directory(filename)
                else:
                    # ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•
                    item_info['is_dir'] = self._guess_is_directory(filename)
                
                result.append(item_info)
            
            return result
            
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºç›®éŒ„è©³ç´°è³‡è¨Šå¤±æ•—: {path}, éŒ¯èª¤: {e}")
            raise
    
    def _is_directory_from_stat(self, stat_obj) -> bool:
        """å¾ stat ç‰©ä»¶åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„"""
        try:
            return bool(stat_obj.st_mode & 0o40000)
        except:
            return False
    
    def _guess_is_directory(self, filename: str) -> bool:
        """å•Ÿç™¼å¼æ–¹æ³•çŒœæ¸¬æ˜¯å¦ç‚ºç›®éŒ„"""
        # å¦‚æœæª”ååŒ…å«æ˜é¡¯çš„å‰¯æª”åï¼Œå¯èƒ½æ˜¯æª”æ¡ˆ
        common_extensions = ['.xml', '.txt', '.log', '.zip', '.tar', '.gz', '.json', '.csv']
        filename_lower = filename.lower()
        
        for ext in common_extensions:
            if filename_lower.endswith(ext):
                return False
        
        # å¦‚æœçœ‹èµ·ä¾†åƒç‰ˆæœ¬ç›®éŒ„æ ¼å¼ï¼Œå‡è¨­æ˜¯ç›®éŒ„
        if re.match(r'^\d+(_all)?(_\d{12})?(_.*)?$', filename):
            return True
        
        # é è¨­å‡è¨­æ˜¯ç›®éŒ„ï¼ˆåœ¨æœå°‹ manifest çš„æƒ…å¢ƒä¸‹æ¯”è¼ƒå®‰å…¨ï¼‰
        return True

    def _try_direct_version_paths(self, base_path: str, version: str) -> Optional[Tuple[str, str]]:
        """ç•¶æ‰¾ä¸åˆ°ç‰ˆæœ¬ç›®éŒ„æ™‚ï¼Œå˜—è©¦ç›´æ¥æ§‹å»ºå¯èƒ½çš„è·¯å¾‘"""
        try:
            import datetime
            today = datetime.datetime.now()
            
            self.logger.info(f"å˜—è©¦ç›´æ¥è·¯å¾‘æœå°‹ç‰ˆæœ¬ {version}")
            
            # ç”Ÿæˆå¯èƒ½çš„æ™‚é–“æˆ³ï¼ˆæœ€è¿‘30å¤©ï¼Œæ¯3å¤©ä¸€å€‹ï¼‰
            for days_back in range(0, 30, 3):
                date = today - datetime.timedelta(days=days_back)
                
                # ç”Ÿæˆå¤šå€‹å¯èƒ½çš„æ™‚é–“æ ¼å¼
                timestamps = [
                    date.strftime("%Y%m%d0000"),  # 202508170000
                    date.strftime("%Y%m%d1200"),  # 202508171200
                    date.strftime("%Y%m%d1800"),  # 202508171800
                    date.strftime("%Y%m%d2300"),  # 202508172300
                ]
                
                for timestamp in timestamps:
                    # å˜—è©¦ä¸åŒçš„ç›®éŒ„æ ¼å¼
                    possible_dirs = [
                        f"{version}_{timestamp}",
                        f"{version}_all_{timestamp}",
                        f"{version}_all_{timestamp}_no-release",
                        f"{version}_all_{timestamp}_backup",
                        f"{version}_{timestamp}_backup",
                    ]
                    
                    for dir_name in possible_dirs:
                        try:
                            manifest_path = self._try_get_manifest_from_dir(base_path, dir_name, version)
                            if manifest_path:
                                self.logger.info(f"âœ… é€éç›´æ¥è·¯å¾‘æ‰¾åˆ°: {dir_name}")
                                return manifest_path
                        except Exception as e:
                            self.logger.debug(f"ç›´æ¥è·¯å¾‘å¤±æ•—: {dir_name}, {e}")
                            continue
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ç›´æ¥è·¯å¾‘æœå°‹å¤±æ•—: {e}")
            return None
            
    def _try_get_manifest_from_dir(self, base_path: str, dir_name: str, version: str) -> Optional[Tuple[str, str]]:
        """å˜—è©¦å¾æŒ‡å®šç›®éŒ„ç²å– manifest - ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹æ³•"""
        try:
            version_path = f"{base_path}/{dir_name}"
            
            # ç›´æ¥æ§‹å»ºå¯èƒ½çš„ manifest æª”æ¡ˆå
            possible_manifests = [
                f"manifest_{version}.xml",
                "manifest.xml",
                f"default_{version}.xml", 
                "default.xml",
                f"{version}.xml"
            ]
            
            for manifest_name in possible_manifests:
                manifest_full_path = f"{version_path}/{manifest_name}"
                
                try:
                    # ä½¿ç”¨æœ€åŸºæœ¬çš„æª¢æŸ¥æ–¹æ³•
                    if self._file_exists_and_valid(manifest_full_path):
                        self.logger.info(f"âœ… æ‰¾åˆ°æœ‰æ•ˆ manifest: {manifest_name} (è·¯å¾‘: {dir_name})")
                        return manifest_full_path, manifest_name
                        
                except Exception as e:
                    self.logger.debug(f"æª¢æŸ¥æª”æ¡ˆå¤±æ•—: {manifest_name}, {e}")
                    continue
            
            return None
            
        except Exception as e:
            self.logger.debug(f"æª¢æŸ¥ç›®éŒ„å¤±æ•—: {dir_name}, {e}")
            return None

    def _file_exists_and_valid(self, file_path: str) -> bool:
        """æœ€åŸºæœ¬çš„æª”æ¡ˆå­˜åœ¨æ€§å’Œæœ‰æ•ˆæ€§æª¢æŸ¥"""
        try:
            # å¦‚æœ stat å¯ç”¨ï¼Œä½¿ç”¨ stat
            if self._server_capabilities.get('supports_stat', True):
                try:
                    stat_info = self.sftp.stat(file_path)
                    file_size = getattr(stat_info, 'st_size', 0)
                    return file_size > 1000  # manifest æª”æ¡ˆæ‡‰è©²å¤§æ–¼ 1KB
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self._server_capabilities['supports_stat'] = False
                    elif 'no such file' in error_msg or 'not found' in error_msg:
                        return False
                    else:
                        raise
            
            # å¦‚æœ stat ä¸å¯ç”¨ï¼Œå˜—è©¦æ‰“é–‹æª”æ¡ˆä¾†æª¢æŸ¥
            try:
                with self.sftp.open(file_path, 'r') as f:
                    # è®€å–å‰å¹¾å€‹å­—ç¯€æª¢æŸ¥æ˜¯å¦åƒ XML
                    header = f.read(100).decode('utf-8', errors='ignore')
                    return '<?xml' in header or '<manifest' in header
            except Exception:
                return False
                
        except Exception:
            return False
                    
    def _matches_version(self, dir_name: str, target_version: str) -> bool:
        """æª¢æŸ¥ç›®éŒ„åæ˜¯å¦åŒ¹é…æŒ‡å®šç‰ˆæœ¬"""
        patterns = [
            rf'^{target_version}_\d{{12}}$',                    # 5_202508170000
            rf'^{target_version}_all_\d{{12}}$',                # 702_all_202508092300  
            rf'^{target_version}_all_\d{{12}}_.*$',             # 669_all_202507142300_no-release
            rf'^{target_version}_\d{{12}}_.*$',                 # 5_202508170000_backup
        ]
        
        for pattern in patterns:
            if re.match(pattern, dir_name):
                return True
        return False
    
    def _extract_version_number(self, name: str) -> Optional[str]:
        """æå–ç‰ˆæœ¬è™Ÿ"""
        patterns = [
            r'^(\d+)_all_\d{12}',      # 702_all_202508092300
            r'^(\d+)_all_\d{12}_',     # 669_all_202507142300_no-release
            r'^(\d+)_\d{12}',          # 5_202508170000
            r'^(\d+)_\d{12}_',         # 5_202508170000_backup
            r'^(\d+)$',                # 5
            r'^v(\d+)',                # v5
        ]
        
        for pattern in patterns:
            match = re.search(pattern, name)
            if match:
                return match.group(1)
        
        return None
    
    def download_file(self, remote_path: str, local_path: str) -> bool:
        """ä¸‹è¼‰æª”æ¡ˆ"""
        try:
            if not self._ensure_connected():
                return False
            
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            filename = os.path.basename(remote_path)
            self.logger.info(f"é–‹å§‹ä¸‹è¼‰: {filename}")
            
            # ç²å–æª”æ¡ˆå¤§å°ï¼ˆå¦‚æœæ”¯æ´çš„è©±ï¼‰
            file_size = 0
            if self._server_capabilities.get('supports_stat', True):
                try:
                    file_stat = self.sftp.stat(remote_path)
                    file_size = file_stat.st_size
                    self.logger.debug(f"æª”æ¡ˆå¤§å°: {file_size} bytes")
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self._server_capabilities['supports_stat'] = False
                        self.logger.debug("stat ä¸æ”¯æ´ï¼Œè·³éæª”æ¡ˆå¤§å°æª¢æŸ¥")
                    else:
                        self.logger.debug(f"ç„¡æ³•ç²å–æª”æ¡ˆå¤§å°: {e}")
            
            # ä¸‹è¼‰æª”æ¡ˆ
            self.sftp.get(remote_path, local_path)
            
            # é©—è­‰ä¸‹è¼‰
            if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                actual_size = os.path.getsize(local_path)
                self.logger.info(f"ä¸‹è¼‰å®Œæˆ: {filename} ({actual_size} bytes)")
                
                # é©—è­‰æª”æ¡ˆå¤§å°æ˜¯å¦ä¸€è‡´ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                if file_size > 0 and actual_size != file_size:
                    self.logger.warning(f"æª”æ¡ˆå¤§å°ä¸ä¸€è‡´: é æœŸ {file_size}, å¯¦éš› {actual_size}")
                
                return True
            else:
                self.logger.error(f"ä¸‹è¼‰çš„æª”æ¡ˆç„¡æ•ˆæˆ–ç‚ºç©º: {local_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {e}")
            return False

# =====================================
# ===== Source Command ç®¡ç†å™¨ =====
# =====================================

class SourceCommandManager:
    """ç®¡ç†ä¸åŒ DB çš„ source command"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.jira_client = JiraAPIClient()
        self.cache = {}
        
    def get_source_command(self, db_info: DBInfo, mapping_df: pd.DataFrame = None) -> Optional[str]:
        """æ ¹æ“š DB è³‡è¨Šç²å–å°æ‡‰çš„ source command"""
        db_name = db_info.db_info
        
        # æª¢æŸ¥å¿«å–
        if db_name in self.cache:
            self.logger.info(f"ä½¿ç”¨å¿«å–çš„ source command for {db_name}")
            return self.cache[db_name]
        
        # å¾ JIRA æœå°‹
        self.logger.info(f"å¾ JIRA æœå°‹ {db_name} çš„ source command")
        ticket_key = self.jira_client.search_db_ticket(db_name, db_info.module)
        if ticket_key:
            # æ›´æ–° jira_link
            db_info.jira_link = f"https://{config_manager.jira_config['site']}/browse/{ticket_key}"
            self.logger.info(f"æ‰¾åˆ° JIRA ticket: {ticket_key}")
            
            cmd = self.jira_client.get_source_command_from_ticket(ticket_key)
            if cmd:
                self.cache[db_name] = cmd
                self.logger.info(f"æˆåŠŸå¾ JIRA ç²å– source command")
                return cmd
        
        self.logger.warning(f"ç„¡æ³•å¾ JIRA ç²å– {db_name} çš„ source command")
        return None
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰å¿«å–"""
        self.cache.clear()

# =====================================
# ===== Mapping Table è®€å–å™¨ =====
# =====================================

class MappingTableReader:
    """è®€å–å’Œè§£æ mapping table çš„é¡åˆ¥"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.df = None
        
    def load_excel(self, file_path: str) -> bool:
        """è¼‰å…¥ Excel æª”æ¡ˆ"""
        try:
            self.df = pd.read_excel(file_path)
            self.logger.info(f"æˆåŠŸè¼‰å…¥ mapping table: {len(self.df)} ç­†è³‡æ–™")
            return True
        except Exception as e:
            self.logger.error(f"è¼‰å…¥ Excel å¤±æ•—: {str(e)}")
            return False
    
    def get_db_info_list(self, db_type: str = 'all') -> List[DBInfo]:
        """å–å¾— DB è³‡è¨Šåˆ—è¡¨"""
        db_list = []
        
        if self.df is None:
            return db_list
        
        type_columns = {
            'master': ('DB_Type', 'DB_Info', 'DB_Folder', 'SftpPath'),
            'premp': ('premp_DB_Type', 'premp_DB_Info', 'premp_DB_Folder', 'premp_SftpPath'),
            'mp': ('mp_DB_Type', 'mp_DB_Info', 'mp_DB_Folder', 'mp_SftpPath'),
            'mpbackup': ('mpbackup_DB_Type', 'mpbackup_DB_Info', 'mpbackup_DB_Folder', 'mpbackup_SftpPath')
        }
        
        types_to_process = type_columns.keys() if db_type == 'all' else [db_type]
        
        for idx, row in self.df.iterrows():
            for dtype in types_to_process:
                if dtype not in type_columns:
                    continue
                    
                cols = type_columns[dtype]
                db_info_col = cols[1]
                
                if db_info_col in row and pd.notna(row[db_info_col]):
                    db_info = DBInfo(
                        sn=row['SN'] if 'SN' in row else idx + 1,
                        module=row['Module'] if 'Module' in row else '',
                        db_type=dtype,
                        db_info=str(row[db_info_col]),
                        db_folder=str(row[cols[2]]) if cols[2] in row and pd.notna(row[cols[2]]) else '',
                        sftp_path=str(row[cols[3]]) if cols[3] in row and pd.notna(row[cols[3]]) else ''
                    )
                    db_list.append(db_info)
        
        return db_list

# =====================================
# ===== Repo ç®¡ç†å™¨ =====
# =====================================

class RepoManager:
    """Repo æŒ‡ä»¤ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.lock = threading.Lock()
    
    def check_repo_exists(self, work_dir: str) -> bool:
        """æª¢æŸ¥ .repo ç›®éŒ„æ˜¯å¦å­˜åœ¨"""
        repo_dir = os.path.join(work_dir, '.repo')
        exists = os.path.exists(repo_dir)
        self.logger.info(f"æª¢æŸ¥ .repo ç›®éŒ„: {work_dir} -> {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
        return exists

    # ğŸ”¥ æ–°å¢ï¼šå¿«é€Ÿæª¢æŸ¥æ–¹æ³•
    def _quick_repo_status_check(self, work_dir: str) -> dict:
        """
        å¿«é€Ÿæª¢æŸ¥repoç‹€æ…‹ï¼ˆé©ç”¨æ–¼å¤§å‹repoï¼‰
        """
        try:
            self.logger.debug(f"å˜—è©¦å¿«é€Ÿrepoç‹€æ…‹æª¢æŸ¥...")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ .repo/project.list ä¾†å¿«é€Ÿåˆ¤æ–·
            repo_dir = os.path.join(work_dir, '.repo')
            project_list_file = os.path.join(repo_dir, 'project.list')
            
            if not os.path.exists(project_list_file):
                return None
            
            # å¿«é€Ÿæª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„ä¿®æ”¹
            quick_cmd = f"{config_manager.repo_config['repo_command']} status --porcelain"
            success, output = self.run_command(quick_cmd, cwd=work_dir, timeout=60)
            
            if success:
                # å¦‚æœè¼¸å‡ºç‚ºç©ºæˆ–å¾ˆå°‘ï¼Œå¯èƒ½æ˜¯ä¹¾æ·¨çš„
                lines = [line.strip() for line in output.split('\n') if line.strip()]
                
                if len(lines) == 0:
                    self.logger.info(f"âœ… å¿«é€Ÿæª¢æŸ¥ï¼šå·¥ä½œç©ºé–“ä¹¾æ·¨")
                    return {
                        'is_clean': True,
                        'modified_files': [],
                        'untracked_files': [],
                        'staged_files': [],
                        'details': 'å¿«é€Ÿæª¢æŸ¥ï¼šå·¥ä½œç©ºé–“ä¹¾æ·¨'
                    }
                elif len(lines) < 50:  # å¦‚æœä¿®æ”¹è¼ƒå°‘ï¼Œç¹¼çºŒå®Œæ•´æª¢æŸ¥
                    self.logger.debug(f"å¿«é€Ÿæª¢æŸ¥ï¼šç™¼ç¾ {len(lines)} è¡Œè®Šæ›´ï¼ŒåŸ·è¡Œå®Œæ•´æª¢æŸ¥")
                    return None
                else:
                    # å¤ªå¤šä¿®æ”¹ï¼Œç›´æ¥è¿”å›é«’ç‹€æ…‹
                    self.logger.warning(f"å¿«é€Ÿæª¢æŸ¥ï¼šç™¼ç¾å¤§é‡è®Šæ›´ ({len(lines)} è¡Œ)")
                    return {
                        'is_clean': False,
                        'modified_files': ['å¤šå€‹æ–‡ä»¶'],
                        'untracked_files': [],
                        'staged_files': [],
                        'details': f'å¿«é€Ÿæª¢æŸ¥ï¼šç™¼ç¾å¤§é‡è®Šæ›´ ({len(lines)} è¡Œ)'
                    }
            
            return None
            
        except Exception as e:
            self.logger.debug(f"å¿«é€Ÿæª¢æŸ¥å¤±æ•—: {e}")
            return None
    
    # ğŸ”¥ æ–°å¢ï¼šåŸºæ–¼gitçš„å‚™ç”¨æª¢æŸ¥
    def _fallback_git_status_check(self, work_dir: str) -> dict:
        """
        ç•¶repo statuså¤±æ•—æ™‚çš„å‚™ç”¨æª¢æŸ¥æ–¹æ³•
        """
        try:
            self.logger.info(f"åŸ·è¡ŒåŸºæ–¼gitçš„å‚™ç”¨ç‹€æ…‹æª¢æŸ¥...")
            
            # æƒæå¯èƒ½çš„gitç›®éŒ„
            modified_files = []
            untracked_files = []
            
            # åœ¨å·¥ä½œç›®éŒ„ä¸‹æŸ¥æ‰¾gitå°ˆæ¡ˆ
            for root, dirs, files in os.walk(work_dir):
                if '.git' in dirs:
                    git_dir = root
                    rel_path = os.path.relpath(git_dir, work_dir)
                    
                    try:
                        # æª¢æŸ¥é€™å€‹gitå°ˆæ¡ˆçš„ç‹€æ…‹
                        git_status_cmd = "git status --porcelain"
                        success, output = self.run_command(git_status_cmd, cwd=git_dir, timeout=30)
                        
                        if success and output.strip():
                            for line in output.split('\n'):
                                if line.strip():
                                    status_code = line[:2]
                                    file_path = line[3:].strip()
                                    full_path = f"{rel_path}/{file_path}" if rel_path != '.' else file_path
                                    
                                    if status_code.strip() in ['M', 'MM', 'AM', 'AD']:
                                        modified_files.append(full_path)
                                    elif status_code.strip() in ['??']:
                                        untracked_files.append(full_path)
                                        
                    except Exception as e:
                        self.logger.debug(f"æª¢æŸ¥gitç›®éŒ„ {git_dir} å¤±æ•—: {e}")
                        continue
                    
                    # é¿å…éæ­¸é€²å…¥.gitç›®éŒ„
                    dirs.remove('.git')
                
                # é™åˆ¶æƒææ·±åº¦é¿å…å¤ªæ…¢
                if root.count(os.sep) - work_dir.count(os.sep) > 3:
                    dirs.clear()
            
            is_clean = len(modified_files) == 0 and len(untracked_files) == 0
            
            details_parts = []
            if modified_files:
                details_parts.append(f"ä¿®æ”¹æ–‡ä»¶: {len(modified_files)}å€‹")
            if untracked_files:
                details_parts.append(f"æœªè¿½è¹¤æ–‡ä»¶: {len(untracked_files)}å€‹")
            
            details = '; '.join(details_parts) if details_parts else 'å‚™ç”¨æª¢æŸ¥ï¼šå·¥ä½œç©ºé–“ä¹¾æ·¨'
            
            result = {
                'is_clean': is_clean,
                'modified_files': modified_files,
                'untracked_files': untracked_files,
                'staged_files': [],
                'details': f"å‚™ç”¨æª¢æŸ¥ï¼š{details}"
            }
            
            self.logger.info(f"å‚™ç”¨æª¢æŸ¥å®Œæˆ: {details}")
            return result
            
        except Exception as e:
            self.logger.error(f"å‚™ç”¨æª¢æŸ¥å¤±æ•—: {e}")
            return {
                'is_clean': False,
                'modified_files': [],
                'untracked_files': [],
                'staged_files': [],
                'details': f'å‚™ç”¨æª¢æŸ¥å¤±æ•—: {str(e)}'
            }

    # ğŸ”¥ æ–°å¢ï¼šæª¢æŸ¥repoå·¥ä½œç©ºé–“æ˜¯å¦ä¹¾æ·¨
    def check_repo_clean_status(self, work_dir: str) -> dict:
        """
        æª¢æŸ¥repoå·¥ä½œç©ºé–“æ˜¯å¦ä¹¾æ·¨ - ğŸ”¥ è¶…å¿«ç‰ˆæœ¬
        """
        try:
            if not self.check_repo_exists(work_dir):
                return {
                    'is_clean': True,
                    'modified_files': [],
                    'untracked_files': [],
                    'staged_files': [],
                    'details': '.repoç›®éŒ„ä¸å­˜åœ¨ï¼Œè¦–ç‚ºä¹¾æ·¨ç‹€æ…‹'
                }
            
            self.logger.info(f"å¿«é€Ÿæª¢æŸ¥repoå·¥ä½œç©ºé–“ç‹€æ…‹: {work_dir}")
            
            # ğŸ”¥ è¶…å¿«æª¢æŸ¥ï¼šåªçœ‹æ˜¯å¦æœ‰æ˜é¡¯çš„ .git/index.lock æ–‡ä»¶
            has_obvious_changes = False
            
            # æª¢æŸ¥å¸¸è¦‹çš„é«’æ–‡ä»¶æŒ‡æ¨™
            common_dirty_indicators = [
                '.repo/project.list.lock',
                '.repo/repo.lock',
            ]
            
            for indicator in common_dirty_indicators:
                if os.path.exists(os.path.join(work_dir, indicator)):
                    has_obvious_changes = True
                    break
            
            if has_obvious_changes:
                return {
                    'is_clean': False,
                    'modified_files': ['detected lock files'],
                    'untracked_files': [],
                    'staged_files': [],
                    'details': 'æª¢æ¸¬åˆ°é–å®šæ–‡ä»¶ï¼Œå¯èƒ½æœ‰æ“ä½œé€²è¡Œä¸­'
                }
            else:
                # å‡è¨­ä¹¾æ·¨
                self.logger.info(f"âœ… å¿«é€Ÿæª¢æŸ¥ï¼šæœªç™¼ç¾æ˜é¡¯é«’ç‹€æ…‹æŒ‡æ¨™")
                return {
                    'is_clean': True,
                    'modified_files': [],
                    'untracked_files': [],
                    'staged_files': [],
                    'details': 'å¿«é€Ÿæª¢æŸ¥ï¼šå‡è¨­å·¥ä½œç©ºé–“ä¹¾æ·¨'
                }
                
        except Exception as e:
            self.logger.warning(f"å¿«é€Ÿæª¢æŸ¥å¤±æ•—ï¼Œå‡è¨­ä¹¾æ·¨: {e}")
            return {
                'is_clean': True,
                'modified_files': [],
                'untracked_files': [],
                'staged_files': [],
                'details': f'æª¢æŸ¥å¤±æ•—ï¼Œå‡è¨­ä¹¾æ·¨: {str(e)}'
            }
    
    # ğŸ”¥ æ–°å¢ï¼šå‚™ä»½æœ¬åœ°ä¿®æ”¹
    def backup_local_changes(self, work_dir: str, db_name: str) -> bool:
        """
        å‚™ä»½æœ¬åœ°ä¿®æ”¹åˆ°å‚™ä»½ç›®éŒ„ - ğŸ”¥ ä¿®å¾©è·¯å¾‘å’ŒæŒ‡ä»¤å•é¡Œ
        
        Args:
            work_dir: å·¥ä½œç›®éŒ„
            db_name: DBåç¨±ï¼ˆç”¨æ–¼å‚™ä»½ç›®éŒ„å‘½åï¼‰
            
        Returns:
            æ˜¯å¦å‚™ä»½æˆåŠŸ
        """
        try:
            if not config_manager.repo_config['backup_local_changes']:
                self.logger.info(f"è·³éå‚™ä»½æœ¬åœ°ä¿®æ”¹ï¼ˆé…ç½®å·²é—œé–‰ï¼‰")
                return True
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = os.path.join(work_dir, 'backups', f'local_changes_{timestamp}')
            os.makedirs(backup_dir, exist_ok=True)
            
            self.logger.info(f"å‚™ä»½æœ¬åœ°ä¿®æ”¹åˆ°: {backup_dir}")
            
            # ğŸ”¥ ä¿®å¾©ï¼šä½¿ç”¨çµ•å°è·¯å¾‘å’Œæ­£ç¢ºçš„é‡å®šå‘æ–¹å¼
            diff_file = os.path.join(backup_dir, 'repo_diff.patch')
            
            # ç¢ºä¿diffæ–‡ä»¶å­˜åœ¨
            with open(diff_file, 'w') as f:
                f.write("")  # å‰µå»ºç©ºæ–‡ä»¶
            
            # ğŸ”¥ ä¿®å¾©æŒ‡ä»¤ï¼šä½¿ç”¨pythonæ–¹å¼è™•ç†é‡å®šå‘
            diff_cmd = f"{config_manager.repo_config['repo_command']} diff"
            success, diff_output = self.run_command(diff_cmd, cwd=work_dir, timeout=120)
            
            if success:
                # ç›´æ¥å¯«å…¥æ–‡ä»¶è€Œä¸æ˜¯ä½¿ç”¨shellé‡å®šå‘
                with open(diff_file, 'w', encoding='utf-8') as f:
                    f.write(diff_output)
                
                self.logger.info(f"âœ… å·®ç•°å·²å‚™ä»½åˆ°: {diff_file}")
                
                # ğŸ”¥ æ–°å¢ï¼šåŒæ™‚å‚™ä»½statusä¿¡æ¯
                status_file = os.path.join(backup_dir, 'repo_status.txt')
                status_cmd = f"{config_manager.repo_config['repo_command']} status"
                status_success, status_output = self.run_command(status_cmd, cwd=work_dir, timeout=60)
                
                if status_success:
                    with open(status_file, 'w', encoding='utf-8') as f:
                        f.write(status_output)
                    self.logger.debug(f"ç‹€æ…‹å·²å‚™ä»½åˆ°: {status_file}")
                
                # å¯«å…¥å‚™ä»½èªªæ˜
                info_file = os.path.join(backup_dir, 'backup_info.txt')
                with open(info_file, 'w', encoding='utf-8') as f:
                    f.write(f"å‚™ä»½æ™‚é–“: {datetime.now()}\n")
                    f.write(f"DBåç¨±: {db_name}\n")
                    f.write(f"å·¥ä½œç›®éŒ„: {work_dir}\n")
                    f.write(f"å‚™ä»½åŸå› : å®šç‰ˆå‰è‡ªå‹•å‚™ä»½\n")
                    f.write(f"diffæª”æ¡ˆ: {diff_file}\n")
                    f.write(f"statusæª”æ¡ˆ: {status_file}\n")
                    f.write(f"å·®ç•°å¤§å°: {len(diff_output)} å­—å…ƒ\n")
                
                self.logger.info(f"âœ… å‚™ä»½å®Œæˆï¼Œå·®ç•°å¤§å°: {len(diff_output)} å­—å…ƒ")
                return True
            else:
                self.logger.warning(f"å‚™ä»½repo diffå¤±æ•—: {diff_output}")
                
                # ğŸ”¥ å˜—è©¦åŸºæ–¼gitçš„å‚™ä»½
                return self._backup_using_git(work_dir, backup_dir, db_name)
                
        except Exception as e:
            self.logger.error(f"å‚™ä»½æœ¬åœ°ä¿®æ”¹å¤±æ•—: {e}")
            return False

    # ğŸ”¥ æ–°å¢ï¼šåŸºæ–¼gitçš„å‚™ä»½æ–¹æ³•
    def _backup_using_git(self, work_dir: str, backup_dir: str, db_name: str) -> bool:
        """
        ä½¿ç”¨gitæŒ‡ä»¤å‚™ä»½ä¿®æ”¹
        """
        try:
            self.logger.info(f"å˜—è©¦ä½¿ç”¨gitæ–¹å¼å‚™ä»½...")
            
            backup_count = 0
            
            # æƒægitç›®éŒ„ä¸¦å‚™ä»½
            for root, dirs, files in os.walk(work_dir):
                if '.git' in dirs:
                    git_dir = root
                    rel_path = os.path.relpath(git_dir, work_dir)
                    
                    try:
                        # ç‚ºæ¯å€‹gité …ç›®å‰µå»ºdiff
                        project_name = rel_path.replace(os.sep, '_') if rel_path != '.' else 'root'
                        diff_file = os.path.join(backup_dir, f'git_diff_{project_name}.patch')
                        
                        git_diff_cmd = "git diff HEAD"
                        success, diff_output = self.run_command(git_diff_cmd, cwd=git_dir, timeout=60)
                        
                        if success and diff_output.strip():
                            with open(diff_file, 'w', encoding='utf-8') as f:
                                f.write(f"# Git diff for: {rel_path}\n")
                                f.write(f"# Generated at: {datetime.now()}\n\n")
                                f.write(diff_output)
                            
                            backup_count += 1
                            self.logger.debug(f"å‚™ä»½gité …ç›®: {rel_path}")
                        
                    except Exception as e:
                        self.logger.debug(f"å‚™ä»½gité …ç›® {git_dir} å¤±æ•—: {e}")
                        continue
                    
                    dirs.remove('.git')
                
                # é™åˆ¶æƒææ·±åº¦
                if root.count(os.sep) - work_dir.count(os.sep) > 3:
                    dirs.clear()
            
            if backup_count > 0:
                self.logger.info(f"âœ… ä½¿ç”¨gitæ–¹å¼å‚™ä»½äº† {backup_count} å€‹é …ç›®")
                return True
            else:
                self.logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°éœ€è¦å‚™ä»½çš„gitä¿®æ”¹")
                return True  # æ²’æœ‰ä¿®æ”¹ä¹Ÿç®—æˆåŠŸ
                
        except Exception as e:
            self.logger.error(f"gitå‚™ä»½å¤±æ•—: {e}")
            return False

    def clean_repo_workspace(self, work_dir: str, force: bool = False) -> bool:
        """
        æ¸…ç†repoå·¥ä½œç©ºé–“ï¼Œé‚„åŸåˆ°ä¹¾æ·¨ç‹€æ…‹ - ğŸ”¥ å„ªåŒ–æ¸…ç†æ­¥é©Ÿå’Œè¶…æ™‚è™•ç†
        
        Args:
            work_dir: å·¥ä½œç›®éŒ„
            force: æ˜¯å¦å¼·åˆ¶æ¸…ç†ï¼ˆæœƒåˆªé™¤æ‰€æœ‰æœ¬åœ°ä¿®æ”¹ï¼‰
            
        Returns:
            æ˜¯å¦æ¸…ç†æˆåŠŸ
        """
        try:
            if not self.check_repo_exists(work_dir):
                self.logger.info(f"repoä¸å­˜åœ¨ï¼Œè·³éæ¸…ç†: {work_dir}")
                return True
            
            self.logger.info(f"é–‹å§‹æ¸…ç†repoå·¥ä½œç©ºé–“: {work_dir}")
            
            # ğŸ”¥ å„ªåŒ–ï¼šåˆ†æ­¥é©ŸåŸ·è¡Œæ¸…ç†ï¼Œæ¯æ­¥éƒ½æœ‰åˆç†çš„è¶…æ™‚æ™‚é–“
            clean_steps = [
                {
                    'name': 'é‡ç½®æ‰€æœ‰ä¿®æ”¹',
                    'cmd': f"{config_manager.repo_config['repo_command']} forall -c 'git reset --hard HEAD'",
                    'timeout': 600  # 10åˆ†é˜
                },
                {
                    'name': 'æ¸…ç†æœªè¿½è¹¤æª”æ¡ˆ',
                    'cmd': f"{config_manager.repo_config['repo_command']} forall -c 'git clean -fd'",
                    'timeout': 300  # 5åˆ†é˜
                },
                {
                    'name': 'ç¢ºä¿æ­£ç¢ºåˆ†æ”¯',
                    'cmd': f"{config_manager.repo_config['repo_command']} forall -c 'git checkout .'",
                    'timeout': 300  # 5åˆ†é˜
                }
            ]
            
            success_count = 0
            
            for i, step in enumerate(clean_steps, 1):
                self.logger.info(f"åŸ·è¡Œæ¸…ç†æ­¥é©Ÿ {i}/{len(clean_steps)}: {step['name']}")
                
                success, output = self.run_command(
                    step['cmd'], 
                    cwd=work_dir, 
                    timeout=step['timeout']
                )
                
                if success:
                    success_count += 1
                    self.logger.info(f"âœ… æ¸…ç†æ­¥é©Ÿ {i} å®Œæˆ")
                else:
                    if force:
                        self.logger.warning(f"âš ï¸ æ¸…ç†æ­¥é©Ÿ {i} å¤±æ•—ä½†ç¹¼çºŒåŸ·è¡Œ: {output[:200]}")
                    else:
                        self.logger.error(f"âŒ æ¸…ç†æ­¥é©Ÿ {i} å¤±æ•—: {output[:200]}")
                        
                        # ğŸ”¥ å¦‚æœæ˜¯è¶…æ™‚ï¼Œå˜—è©¦æ›´æº«å’Œçš„æ–¹å¼
                        if "timeout" in output.lower() or "Command timeout" in output:
                            self.logger.info(f"æ¸…ç†æ­¥é©Ÿè¶…æ™‚ï¼Œå˜—è©¦åŸºæ–¼gitçš„æ¸…ç†...")
                            git_clean_success = self._clean_using_git(work_dir)
                            if git_clean_success:
                                success_count += 1
                                continue
                        
                        return False
            
            # æª¢æŸ¥æ¸…ç†çµæœ
            if success_count >= 2:  # è‡³å°‘2å€‹æ­¥é©ŸæˆåŠŸ
                # å†æ¬¡å¿«é€Ÿæª¢æŸ¥ç‹€æ…‹
                try:
                    clean_status = self._quick_repo_status_check(work_dir)
                    if clean_status and clean_status['is_clean']:
                        self.logger.info(f"âœ… å·¥ä½œç©ºé–“æ¸…ç†å®Œæˆï¼Œç‹€æ…‹ä¹¾æ·¨")
                        return True
                    elif clean_status and not clean_status['is_clean']:
                        if force:
                            self.logger.warning(f"âš ï¸ å¼·åˆ¶æ¨¡å¼ï¼šå·¥ä½œç©ºé–“ä»æœ‰è®Šæ›´ä½†ç¹¼çºŒåŸ·è¡Œ")
                            return True
                        else:
                            self.logger.warning(f"âš ï¸ æ¸…ç†å¾Œä»æœ‰è®Šæ›´: {clean_status['details']}")
                            return False
                    else:
                        # å¿«é€Ÿæª¢æŸ¥å¤±æ•—ï¼Œä½†æ¸…ç†æ­¥é©ŸæˆåŠŸï¼Œå‡è¨­OK
                        self.logger.info(f"âœ… æ¸…ç†æ­¥é©Ÿå®Œæˆï¼Œå‡è¨­å·¥ä½œç©ºé–“å·²æ¸…ç†")
                        return True
                except Exception as e:
                    self.logger.warning(f"æª¢æŸ¥æ¸…ç†çµæœå¤±æ•—: {e}")
                    if force or success_count == len(clean_steps):
                        return True
                    else:
                        return False
            else:
                self.logger.error(f"âŒ æ¸…ç†å¤±æ•—ï¼Œåªæœ‰ {success_count}/{len(clean_steps)} æ­¥é©ŸæˆåŠŸ")
                return False
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†å·¥ä½œç©ºé–“å¤±æ•—: {e}")
            return False
            
    # ğŸ”¥ æ–°å¢ï¼šæ¸…ç†å·¥ä½œç©ºé–“
    def clean_repo_workspace(self, work_dir: str, force: bool = False) -> bool:
        """
        æ¸…ç†repoå·¥ä½œç©ºé–“ï¼Œé‚„åŸåˆ°ä¹¾æ·¨ç‹€æ…‹
        
        Args:
            work_dir: å·¥ä½œç›®éŒ„
            force: æ˜¯å¦å¼·åˆ¶æ¸…ç†ï¼ˆæœƒåˆªé™¤æ‰€æœ‰æœ¬åœ°ä¿®æ”¹ï¼‰
            
        Returns:
            æ˜¯å¦æ¸…ç†æˆåŠŸ
        """
        try:
            if not self.check_repo_exists(work_dir):
                self.logger.info(f"repoä¸å­˜åœ¨ï¼Œè·³éæ¸…ç†: {work_dir}")
                return True
            
            self.logger.info(f"é–‹å§‹æ¸…ç†repoå·¥ä½œç©ºé–“: {work_dir}")
            
            # åŸ·è¡Œæ¸…ç†å‘½ä»¤åºåˆ—
            clean_commands = [
                # 1. é‡ç½®æ‰€æœ‰ä¿®æ”¹
                f"{config_manager.repo_config['repo_command']} forall -c 'git reset --hard HEAD'",
                # 2. æ¸…ç†æœªè¿½è¹¤çš„æª”æ¡ˆ
                f"{config_manager.repo_config['repo_command']} forall -c 'git clean -fd'",
                # 3. ç¢ºä¿åœ¨æ­£ç¢ºçš„åˆ†æ”¯ä¸Š
                f"{config_manager.repo_config['repo_command']} forall -c 'git checkout .'",
            ]
            
            for i, cmd in enumerate(clean_commands, 1):
                self.logger.info(f"åŸ·è¡Œæ¸…ç†æ­¥é©Ÿ {i}/{len(clean_commands)}: {cmd.split(' forall')[0]}...")
                
                success, output = self.run_command(cmd, cwd=work_dir, timeout=300)
                
                if not success:
                    if force:
                        self.logger.warning(f"æ¸…ç†æ­¥é©Ÿ {i} å¤±æ•—ä½†ç¹¼çºŒåŸ·è¡Œ: {output}")
                    else:
                        self.logger.error(f"æ¸…ç†æ­¥é©Ÿ {i} å¤±æ•—: {output}")
                        return False
                else:
                    self.logger.debug(f"æ¸…ç†æ­¥é©Ÿ {i} å®Œæˆ")
            
            # å†æ¬¡æª¢æŸ¥ç‹€æ…‹
            clean_status = self.check_repo_clean_status(work_dir)
            
            if clean_status['is_clean']:
                self.logger.info(f"âœ… å·¥ä½œç©ºé–“æ¸…ç†å®Œæˆï¼Œç‹€æ…‹ä¹¾æ·¨")
                return True
            else:
                if force:
                    self.logger.warning(f"âš ï¸ å¼·åˆ¶æ¨¡å¼ï¼šå·¥ä½œç©ºé–“ä»æœ‰è®Šæ›´ä½†ç¹¼çºŒåŸ·è¡Œ")
                    return True
                else:
                    self.logger.error(f"âŒ æ¸…ç†å¾Œå·¥ä½œç©ºé–“ä»ä¸ä¹¾æ·¨: {clean_status['details']}")
                    return False
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†å·¥ä½œç©ºé–“å¤±æ•—: {e}")
            return False
    
    # ğŸ”¥ æ–°å¢ï¼šè™•ç†é«’repoçš„ç­–ç•¥
    def handle_dirty_repo(self, work_dir: str, db_name: str, clean_status: dict) -> bool:
        """
        è™•ç†æœ‰æœ¬åœ°ä¿®æ”¹çš„repo - ğŸ”¥ æ·»åŠ ç”¨æˆ¶é¸æ“‡å’Œè·³éé¸é …
        
        Args:
            work_dir: å·¥ä½œç›®éŒ„
            db_name: DBåç¨±
            clean_status: repoç‹€æ…‹æª¢æŸ¥çµæœ
            
        Returns:
            æ˜¯å¦è™•ç†æˆåŠŸ
        """
        try:
            self.logger.warning(f"ç™¼ç¾æœ¬åœ°ä¿®æ”¹: {clean_status['details']}")
            
            # ğŸ”¥ å¦‚æœæ˜¯æª¢æŸ¥è¶…æ™‚ï¼Œæä¾›å¿«é€Ÿè™•ç†é¸é …
            if "timeout" in clean_status['details'].lower() or "Command timeout" in clean_status['details']:
                self.logger.warning(f"âš ï¸ repo status æª¢æŸ¥è¶…æ™‚ï¼Œå¯èƒ½å› ç‚ºå·¥ä½œç©ºé–“å¤ªå¤§")
                
                if config_manager.repo_config['skip_clean_on_timeout']:
                    self.logger.info(f"ğŸ“‹ é…ç½®ç‚ºè·³éè¶…æ™‚æ¸…ç†ï¼Œå‡è¨­å·¥ä½œç©ºé–“å¯ç”¨")
                    return True
                
                # è©¢å•ç”¨æˆ¶æ˜¯å¦è¦å¼·åˆ¶æ¸…ç†
                if config_manager.repo_config['ask_user_on_dirty']:
                    print(f"\nâš ï¸ {db_name}: repo status æª¢æŸ¥è¶…æ™‚")
                    print("å¯èƒ½åŸå› ï¼šå·¥ä½œç©ºé–“å¾ˆå¤§æˆ–æœ‰å¤§é‡ä¿®æ”¹")
                    print("é¸é …ï¼š")
                    print("  1. è·³éæ¸…ç†ï¼Œç›´æ¥ä½¿ç”¨ç¾æœ‰å·¥ä½œç©ºé–“ (æ¨è–¦)")
                    print("  2. å¼·åˆ¶æ¸…ç† (å¯èƒ½å¾ˆæ…¢)")
                    print("  3. è·³éé€™å€‹DB")
                    
                    choice = input("è«‹é¸æ“‡ (1/2/3) [1]: ").strip() or "1"
                    
                    if choice == "1":
                        self.logger.info(f"ğŸ‘Œ ç”¨æˆ¶é¸æ“‡è·³éæ¸…ç†ï¼Œç›´æ¥ä½¿ç”¨å·¥ä½œç©ºé–“")
                        return True
                    elif choice == "2":
                        self.logger.info(f"ğŸ§¹ ç”¨æˆ¶é¸æ“‡å¼·åˆ¶æ¸…ç†")
                        return self._force_clean_large_repo(work_dir, db_name)
                    else:
                        self.logger.info(f"â­ï¸ ç”¨æˆ¶é¸æ“‡è·³éæ­¤DB")
                        return False
                else:
                    # éäº’å‹•æ¨¡å¼ï¼Œæ ¹æ“šé…ç½®æ±ºå®š
                    if config_manager.repo_config['force_clean_on_dirty']:
                        return self._force_clean_large_repo(work_dir, db_name)
                    else:
                        self.logger.info(f"â­ï¸ è‡ªå‹•è·³éæ¸…ç†ï¼ˆéäº’å‹•æ¨¡å¼ï¼‰")
                        return True
            
            # é¡¯ç¤ºä¿®æ”¹è©³æƒ…ï¼ˆåªé¡¯ç¤ºå‰å¹¾å€‹ï¼‰
            if clean_status['modified_files']:
                self.logger.info(f"ä¿®æ”¹çš„æ–‡ä»¶:")
                for file_path in clean_status['modified_files'][:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    self.logger.info(f"  - {file_path}")
                if len(clean_status['modified_files']) > 5:
                    self.logger.info(f"  ... é‚„æœ‰ {len(clean_status['modified_files']) - 5} å€‹æ–‡ä»¶")
            
            # ğŸ”¥ æ ¹æ“šé…ç½®æ±ºå®šè™•ç†ç­–ç•¥
            if config_manager.repo_config['ask_user_on_dirty']:
                # äº’å‹•æ¨¡å¼ï¼šè©¢å•ç”¨æˆ¶
                print(f"\nâš ï¸ {db_name}: ç™¼ç¾æœ¬åœ°ä¿®æ”¹")
                print(f"è©³æƒ…: {clean_status['details']}")
                print("é¸é …ï¼š")
                print("  1. è·³éæ¸…ç†ï¼Œç›´æ¥ä½¿ç”¨ (å¿«é€Ÿ)")
                print("  2. å‚™ä»½ä¸¦æ¸…ç† (è¼ƒæ…¢)")
                print("  3. è·³éé€™å€‹DB")
                
                choice = input("è«‹é¸æ“‡ (1/2/3) [1]: ").strip() or "1"
                
                if choice == "1":
                    self.logger.info(f"ğŸ‘Œ ç”¨æˆ¶é¸æ“‡è·³éæ¸…ç†")
                    return True
                elif choice == "2":
                    return self._backup_and_clean(work_dir, db_name)
                else:
                    self.logger.info(f"â­ï¸ ç”¨æˆ¶é¸æ“‡è·³éæ­¤DB")
                    return False
                    
            elif config_manager.repo_config['auto_clean_workspace']:
                # è‡ªå‹•æ¸…ç†æ¨¡å¼
                self.logger.info(f"ğŸ”„ è‡ªå‹•æ¸…ç†æ¨¡å¼ï¼šå‚™ä»½ä¸¦æ¸…ç†æœ¬åœ°ä¿®æ”¹")
                return self._backup_and_clean(work_dir, db_name)
            else:
                # ä¸æ¸…ç†æ¨¡å¼
                self.logger.warning(f"âš ï¸ ç™¼ç¾æœ¬åœ°ä¿®æ”¹ä½†æœªå•Ÿç”¨è‡ªå‹•æ¸…ç†ï¼Œç›´æ¥ä½¿ç”¨ç¾æœ‰å·¥ä½œç©ºé–“")
                return True
                    
        except Exception as e:
            self.logger.error(f"è™•ç†é«’repoå¤±æ•—: {e}")
            return False

    # ğŸ”¥ æ–°å¢ï¼šé‡å°å¤§å‹repoçš„å¼·åˆ¶æ¸…ç†
    def _force_clean_large_repo(self, work_dir: str, db_name: str) -> bool:
        """
        é‡å°å¤§å‹repoçš„å¼·åˆ¶æ¸…ç†ç­–ç•¥
        """
        try:
            self.logger.info(f"ğŸ§¹ é–‹å§‹å¼·åˆ¶æ¸…ç†å¤§å‹repo: {db_name}")
            
            if config_manager.repo_config['quick_clean_only']:
                # åªåšæœ€åŸºæœ¬çš„æ¸…ç†
                self.logger.info(f"åŸ·è¡Œå¿«é€Ÿæ¸…ç†...")
                
                # ç°¡å–®ç²—æš´ï¼šåˆªé™¤ .repo/project-objects å¼·åˆ¶é‡æ–°sync
                project_objects = os.path.join(work_dir, '.repo', 'project-objects')
                if os.path.exists(project_objects):
                    import shutil
                    shutil.rmtree(project_objects)
                    self.logger.info(f"âœ… å·²æ¸…ç†é …ç›®ç·©å­˜")
                
                return True
            else:
                # åŸ·è¡Œå®Œæ•´æ¸…ç†ï¼ˆæœƒå¾ˆæ…¢ï¼‰
                return self.clean_repo_workspace(work_dir, force=True)
                
        except Exception as e:
            self.logger.error(f"å¼·åˆ¶æ¸…ç†å¤±æ•—: {e}")
            return False

    # ğŸ”¥ æ–°å¢ï¼šå‚™ä»½ä¸¦æ¸…ç†çš„çµ„åˆæ–¹æ³•
    def _backup_and_clean(self, work_dir: str, db_name: str) -> bool:
        """
        å‚™ä»½ä¸¦æ¸…ç†çš„çµ„åˆæ–¹æ³•
        """
        try:
            # å…ˆå‚™ä»½
            if config_manager.repo_config['backup_local_changes']:
                backup_success = self.backup_local_changes(work_dir, db_name)
                if not backup_success:
                    self.logger.warning(f"å‚™ä»½å¤±æ•—ï¼Œä½†ç¹¼çºŒæ¸…ç†")
            
            # å†æ¸…ç†
            if config_manager.repo_config['quick_clean_only']:
                return self._force_clean_large_repo(work_dir, db_name)
            else:
                return self.clean_repo_workspace(work_dir, force=True)
                
        except Exception as e:
            self.logger.error(f"å‚™ä»½ä¸¦æ¸…ç†å¤±æ•—: {e}")
            return False
            
    def run_command(self, cmd: str, cwd: str = None, timeout: int = None) -> Tuple[bool, str]:
        """åŒæ­¥åŸ·è¡ŒæŒ‡ä»¤"""
        try:
            self.logger.debug(f"åŸ·è¡ŒæŒ‡ä»¤: {cmd}")
            result = subprocess.run(
                cmd,
                shell=True,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            if result.returncode == 0:
                self.logger.debug(f"æŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
                return True, result.stdout
            else:
                self.logger.warning(f"æŒ‡ä»¤åŸ·è¡Œå¤±æ•— (è¿”å›ç¢¼: {result.returncode})")
                return False, result.stderr
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"æŒ‡ä»¤åŸ·è¡Œè¶…æ™‚")
            return False, "Command timeout"
        except Exception as e:
            self.logger.error(f"æŒ‡ä»¤åŸ·è¡Œç•°å¸¸: {e}")
            return False, str(e)
    
    def repo_init(self, work_dir: str, init_cmd: str) -> bool:
        """åŸ·è¡Œ repo init - å¢å¼·æ¸…ç†å’ŒéŒ¯èª¤æ¢å¾©"""
        # ğŸ”¥ å¾¹åº•æ¸…ç†å¯èƒ½å­˜åœ¨çš„èˆŠ .repo ç›®éŒ„å’Œç›¸é—œæª”æ¡ˆ
        repo_dir = os.path.join(work_dir, '.repo')
        if os.path.exists(repo_dir):
            self.logger.info(f"ç™¼ç¾èˆŠçš„ .repo ç›®éŒ„ï¼ŒåŸ·è¡Œå¾¹åº•æ¸…ç†: {repo_dir}")
            import shutil
            try:
                # å…ˆå˜—è©¦æ­£å¸¸åˆªé™¤
                shutil.rmtree(repo_dir)
                self.logger.info("âœ… èˆŠ .repo ç›®éŒ„æ¸…ç†æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"æ­£å¸¸æ¸…ç†å¤±æ•—ï¼Œå˜—è©¦å¼·åˆ¶æ¸…ç†: {e}")
                # å¼·åˆ¶æ¸…ç†
                try:
                    import subprocess
                    if os.name == 'posix':  # Linux/Unix
                        subprocess.run(['rm', '-rf', repo_dir], check=True)
                        self.logger.info("âœ… å¼·åˆ¶æ¸…ç†æˆåŠŸ")
                    else:
                        raise Exception("ç„¡æ³•å¼·åˆ¶æ¸…ç†")
                except Exception as e2:
                    self.logger.error(f"âŒ å¼·åˆ¶æ¸…ç†ä¹Ÿå¤±æ•—: {e2}")
                    return False
        
        # ğŸ”¥ æ¸…ç†å¯èƒ½å­˜åœ¨çš„å…¶ä»–ç›¸é—œæª”æ¡ˆ
        cleanup_patterns = [
            '.repo_*',
            'repo_*',
            '.repopickle_*'
        ]
        
        for pattern in cleanup_patterns:
            try:
                import glob
                for item in glob.glob(os.path.join(work_dir, pattern)):
                    if os.path.isfile(item):
                        os.remove(item)
                    elif os.path.isdir(item):
                        shutil.rmtree(item)
                    self.logger.debug(f"æ¸…ç†: {item}")
            except Exception as e:
                self.logger.debug(f"æ¸…ç† {pattern} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # ğŸ”¥ å¢åŠ é‡è©¦æ©Ÿåˆ¶å’Œæ›´é•·çš„è¶…æ™‚æ™‚é–“
        max_retries = 3
        timeout_values = [180, 300, 600]  # 3åˆ†é˜ã€5åˆ†é˜ã€10åˆ†é˜
        for attempt in range(max_retries):
            timeout = timeout_values[attempt]
            self.logger.info(f"åŸ·è¡Œ repo init (å˜—è©¦ {attempt + 1}/{max_retries}ï¼Œè¶…æ™‚: {timeout}ç§’): {init_cmd}")
            
            success, output = self.run_command(
                init_cmd,
                cwd=work_dir,
                timeout=timeout
            )
            
            if success:
                self.logger.info("âœ… Repo init åŸ·è¡ŒæˆåŠŸ")
                return True
            else:
                self.logger.warning(f"âŒ Repo init å˜—è©¦ {attempt + 1} å¤±æ•—: {output}")
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæ¸…ç†ä¸¦é‡è©¦
                if attempt < max_retries - 1:
                    self.logger.info(f"æº–å‚™é‡è©¦ï¼Œå…ˆæ¸…ç†ç’°å¢ƒ...")
                    repo_dir = os.path.join(work_dir, '.repo')
                    if os.path.exists(repo_dir):
                        try:
                            import shutil
                            shutil.rmtree(repo_dir)
                        except:
                            pass
                    time.sleep(15)  # ç­‰å¾… 15 ç§’å¾Œé‡è©¦
        
        self.logger.error("âŒ Repo init æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
        return False
    
    def apply_manifest(self, work_dir: str, manifest_file: str) -> bool:
        """æ‡‰ç”¨ manifest æª”æ¡ˆ - æ”¹é€²ç‰ˆæœ¬"""
        try:
            manifest_name = os.path.basename(manifest_file)
            self.logger.info(f"æ‡‰ç”¨ manifest: {manifest_name}")
            
            repo_dir = os.path.join(work_dir, '.repo')
            manifests_dir = os.path.join(repo_dir, 'manifests')
            
            if not os.path.exists(manifests_dir):
                self.logger.error(f"Manifests ç›®éŒ„ä¸å­˜åœ¨: {manifests_dir}")
                return False
            
            # è¤‡è£½ manifest æª”æ¡ˆ
            dest_file = os.path.join(manifests_dir, manifest_name)
            import shutil
            shutil.copy2(manifest_file, dest_file)
            self.logger.debug(f"è¤‡è£½ manifest: {manifest_file} -> {dest_file}")
            
            # ğŸ”¥ ä¿®æ”¹ï¼šå¢åŠ è¶…æ™‚æ™‚é–“ä¸¦æ·»åŠ é‡è©¦æ©Ÿåˆ¶
            switch_cmd = f"{config_manager.repo_config['repo_command']} init -m {manifest_name}"
            self.logger.info(f"åˆ‡æ› manifest: {switch_cmd}")
            
            # ğŸ”¥ å˜—è©¦å¤šæ¬¡ï¼Œå¢åŠ è¶…æ™‚æ™‚é–“
            max_attempts = 3
            timeout_values = [120, 180, 300]  # 2åˆ†é˜ã€3åˆ†é˜ã€5åˆ†é˜
            
            for attempt in range(max_attempts):
                timeout = timeout_values[attempt]
                self.logger.info(f"å˜—è©¦ {attempt + 1}/{max_attempts}ï¼Œè¶…æ™‚è¨­å®š: {timeout}ç§’")
                
                success, output = self.run_command(
                    switch_cmd,
                    cwd=work_dir,
                    timeout=timeout
                )
                
                if success:
                    self.logger.info(f"âœ… æˆåŠŸåˆ‡æ›åˆ° manifest: {manifest_name}")
                    return True
                else:
                    self.logger.warning(f"âŒ å˜—è©¦ {attempt + 1} å¤±æ•—: {output}")
                    
                    # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡ï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è©¦
                    if attempt < max_attempts - 1:
                        import time
                        wait_time = 10 * (attempt + 1)  # 10ç§’ã€20ç§’
                        self.logger.info(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
            
            self.logger.error(f"âŒ æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œç„¡æ³•åˆ‡æ› manifest")
            return False
            
        except Exception as e:
            self.logger.error(f"æ‡‰ç”¨ manifest å¤±æ•—: {str(e)}")
            return False
    
    def start_repo_sync_async(self, work_dir: str, db_name: str) -> subprocess.Popen:
        """ğŸ¯ ä¿®å¾©ç‰ˆæœ¬ - ä½¿ç”¨ unbuffer ç¢ºä¿å¯¦æ™‚è¼¸å‡º"""
        try:
            self.logger.info(f"{db_name}: å•Ÿå‹• unbuffer ç‰ˆæœ¬ repo sync")
            
            # æª¢æŸ¥å·¥ä½œç›®éŒ„
            if not os.path.exists(os.path.join(work_dir, '.repo')):
                raise Exception(f"å·¥ä½œç›®éŒ„æ²’æœ‰ .repo: {work_dir}")
            
            # æª¢æŸ¥ unbuffer æ˜¯å¦å¯ç”¨
            try:
                subprocess.run(['which', 'unbuffer'], check=True, capture_output=True, timeout=5)
                use_unbuffer = True
            except:
                use_unbuffer = False
                self.logger.warning(f"{db_name}: unbuffer ä¸å¯ç”¨ï¼Œä½¿ç”¨ script æ–¹æ³•")
            
            # å»ºç«‹æ—¥èªŒ
            log_dir = os.path.join(work_dir, 'logs')
            os.makedirs(log_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            method_name = "unbuffer" if use_unbuffer else "script"
            log_file = os.path.join(log_dir, f'repo_sync_{method_name}_{timestamp}.log')
            
            # å¯«å…¥åˆå§‹ä¿¡æ¯
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"=== Fixed Repo Sync Log for {db_name} ===\n")
                f.write(f"é–‹å§‹æ™‚é–“: {datetime.now()}\n")
                f.write(f"å·¥ä½œç›®éŒ„: {work_dir}\n")
                f.write(f"ä½¿ç”¨æ–¹æ³•: {method_name}\n\n")
            
            # æº–å‚™å‘½ä»¤
            repo_cmd = config_manager.repo_config['repo_command']
            jobs = min(config_manager.repo_config['sync_jobs'], 4)
            
            if use_unbuffer:
                # ğŸ¯ ä½¿ç”¨ unbuffer æ–¹æ³•ï¼ˆå·²é©—è­‰æœ‰æ•ˆï¼‰
                cmd_parts = [
                    'unbuffer',
                    repo_cmd, 'sync', 
                    f'-j{jobs}', 
                    '--verbose', 
                    '--force-sync'
                ]
                
                process = subprocess.Popen(
                    cmd_parts,
                    cwd=work_dir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=0,
                    preexec_fn=os.setsid
                )
            else:
                # å‚™ç”¨ï¼šä½¿ç”¨ script æ–¹æ³•
                shell_cmd = f"""
    cd "{work_dir}"
    echo "[SCRIPT] é€²ç¨‹å•Ÿå‹•ï¼ŒPID: $$" >> "{log_file}"
    script -fq /dev/null -c "{repo_cmd} sync -j{jobs} --verbose --force-sync" | tee -a "{log_file}"
    echo "[SCRIPT] é€²ç¨‹çµæŸï¼Œæ™‚é–“: $(date)" >> "{log_file}"
    """
                
                process = subprocess.Popen(
                    ['bash', '-c', shell_cmd],
                    cwd=work_dir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
            
            # ğŸ¯ å‰µå»ºå¯¦æ™‚æ—¥èªŒå¯«å…¥ç·šç¨‹ï¼ˆé—œéµæ”¹é€²ï¼‰
            def log_writer():
                try:
                    with open(log_file, 'a', encoding='utf-8', buffering=1) as f:
                        f.write(f"[UNBUFFER] é€²ç¨‹å•Ÿå‹•ï¼ŒPID: {process.pid}\n")
                        f.write(f"[UNBUFFER] é–‹å§‹æ™‚é–“: {datetime.now()}\n\n")
                        f.flush()
                        
                        # ğŸ”¥ é€²åº¦è¿½è¹¤è®Šæ•¸
                        last_reported_progress = -1
                        last_report_time = datetime.now()
                        message_count = 0
                        
                        if use_unbuffer:
                            while True:
                                line = process.stdout.readline()
                                if line:
                                    # ğŸ” æ‰€æœ‰å…§å®¹éƒ½å¯«å…¥æ–‡ä»¶ï¼ˆä¸è®Šï¼‰
                                    f.write(line)
                                    f.flush()
                                    
                                    # ğŸ”¥ æ™ºèƒ½éæ¿¾ console è¼¸å‡º
                                    line_clean = line.strip()
                                    message_count += 1
                                    
                                    # âœ… åªå ±å‘Šé‡è¦çš„é€²åº¦è®ŠåŒ–
                                    if "Syncing:" in line_clean and "%" in line_clean:
                                        import re
                                        progress_match = re.search(r'Syncing:\s*(\d+)%\s*\((\d+)/(\d+)\)', line_clean)
                                        if progress_match:
                                            current_progress = int(progress_match.group(1))
                                            current_count = int(progress_match.group(2))
                                            total_count = int(progress_match.group(3))
                                            
                                            # ğŸ¯ åªåœ¨ä»¥ä¸‹æƒ…æ³å ±å‘Šé€²åº¦ï¼š
                                            should_report = (
                                                last_reported_progress == -1 or  # ç¬¬ä¸€æ¬¡
                                                current_progress - last_reported_progress >= 5 or  # é€²åº¦å¢åŠ  5%ä»¥ä¸Š
                                                current_progress % 10 == 0 or  # æ¯10%é‡Œç¨‹ç¢‘
                                                current_progress >= 95  # æ¥è¿‘å®Œæˆæ™‚
                                            )
                                            
                                            if should_report:
                                                # è¨ˆç®—é€Ÿåº¦
                                                current_time = datetime.now()
                                                elapsed = (current_time - last_report_time).total_seconds()
                                                
                                                speed_info = ""
                                                if last_reported_progress > 0 and elapsed > 0:
                                                    progress_diff = current_progress - last_reported_progress
                                                    speed = progress_diff / (elapsed / 60)  # %/åˆ†é˜
                                                    if speed > 0:
                                                        remaining_time = (100 - current_progress) / speed
                                                        speed_info = f" (é è¨ˆå‰©é¤˜: {remaining_time:.0f}åˆ†é˜)"
                                                
                                                # ğŸ“Š ç°¡æ½”çš„é€²åº¦å ±å‘Š
                                                # self.logger.info(
                                                #    f"{db_name}: {current_progress}% "
                                                #    f"({current_count}/{total_count}){speed_info}"
                                                #)
                                                
                                                last_reported_progress = current_progress
                                                last_report_time = current_time
                                    
                                    # âš ï¸ å ±å‘ŠéŒ¯èª¤å’Œè­¦å‘Š
                                    elif any(keyword in line_clean.lower() for keyword in 
                                        ['error:', 'fatal:', 'failed', 'timeout', 'exception', 'abort']):
                                        self.logger.warning(f"{db_name}: {line_clean}")
                                    
                                    # ğŸ‰ å ±å‘Šé‡è¦é‡Œç¨‹ç¢‘
                                    elif any(phrase in line_clean.lower() for phrase in
                                        ['sync has finished', 'completed successfully', 'repo sync complete']):
                                        self.logger.info(f"{db_name}: åŒæ­¥å®Œæˆï¼")
                                    
                                    # ğŸš« ä¸å ±å‘Šçš„å…§å®¹ï¼š
                                    # - "Skipped fetching project"
                                    # - "fetching project" 
                                    # - "..working.."
                                    # - é‡è¤‡çš„é€²åº¦ä¿¡æ¯
                                    
                                elif process.poll() is not None:
                                    break
                        
                        return_code = process.poll()
                        f.write(f"\n[UNBUFFER] é€²ç¨‹çµæŸï¼Œè¿”å›ç¢¼: {return_code}\n")
                        f.write(f"[UNBUFFER] çµæŸæ™‚é–“: {datetime.now()}\n")
                        f.write(f"[UNBUFFER] ç¸½è™•ç†æ¶ˆæ¯æ•¸: {message_count}\n")
                        f.flush()
                        
                        # ğŸ“ˆ æœ€çµ‚å ±å‘Š
                        if return_code == 0:
                            self.logger.info(f"{db_name}: âœ… åŒæ­¥æˆåŠŸå®Œæˆ")
                        else:
                            self.logger.error(f"{db_name}: âŒ åŒæ­¥å¤±æ•— (è¿”å›ç¢¼: {return_code})")
                        
                except Exception as e:
                    self.logger.error(f"{db_name}: æ—¥èªŒå¯«å…¥éŒ¯èª¤: {e}")
            
            # å•Ÿå‹•æ—¥èªŒç·šç¨‹
            if use_unbuffer:  # åªæœ‰ unbuffer éœ€è¦æ—¥èªŒç·šç¨‹
                log_thread = threading.Thread(target=log_writer, daemon=True)
                log_thread.start()
                process._log_thread = log_thread
            
            # é©—è­‰å•Ÿå‹•
            time.sleep(2)
            if process.poll() is not None:
                raise Exception(f"é€²ç¨‹ç«‹å³å¤±æ•—ï¼Œè¿”å›ç¢¼: {process.poll()}")
            
            # ä¿å­˜é€²ç¨‹ä¿¡æ¯
            process._log_file_path = log_file
            process._db_name = db_name
            process._start_time = datetime.now()
            
            resource_manager.register_process(db_name, process)
            self.logger.info(f"{db_name}: repo sync å•Ÿå‹•æˆåŠŸ (PID: {process.pid})")
            
            return process
            
        except Exception as e:
            self.logger.error(f"{db_name}: å•Ÿå‹•å¤±æ•—: {e}")
            return None
    
    def check_process_status(self, db_name: str, process: subprocess.Popen) -> Optional[int]:
        """æ”¹é€²çš„é€²ç¨‹ç‹€æ…‹æª¢æŸ¥"""
        with self.lock:
            if process:
                try:
                    poll = process.poll()
                    
                    # ğŸ”¥ åŠ å¼·æ—¥èªŒ
                    self.logger.debug(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹ PID={process.pid}, poll={poll}")
                    
                    if poll is not None:
                        # ğŸ”¥ é€²ç¨‹å·²çµæŸï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯
                        self.logger.info(f"{db_name}: é€²ç¨‹å·²çµæŸï¼Œè¿”å›ç¢¼={poll}")
                        
                        # æ­£ç¢ºé—œé–‰æ–‡ä»¶å¥æŸ„
                        if hasattr(process, '_log_file_handle') and process._log_file_handle:
                            try:
                                process._log_file_handle.flush()
                                process._log_file_handle.close()
                                self.logger.debug(f"{db_name}: æ—¥èªŒæ–‡ä»¶å¥æŸ„å·²é—œé–‰")
                            except Exception as e:
                                self.logger.warning(f"{db_name}: é—œé–‰æ—¥èªŒå¥æŸ„å¤±æ•—: {e}")
                        
                        resource_manager.unregister_process(db_name)
                        
                        # ğŸ”¥ å¯«å…¥å®Œæˆæ¨™è¨˜åˆ°æ—¥èªŒ
                        if hasattr(process, '_log_file_path'):
                            try:
                                with open(process._log_file_path, 'a') as f:
                                    f.write(f"\n=== é€²ç¨‹çµæŸ ===\n")
                                    f.write(f"è¿”å›ç¢¼: {poll}\n")
                                    f.write(f"çµæŸæ™‚é–“: {datetime.now()}\n")
                            except:
                                pass
                        
                        return poll
                    else:
                        # ğŸ”¥ é€²ç¨‹ä»åœ¨é‹è¡Œï¼Œä½†é©—è­‰ PID æ˜¯å¦çœŸçš„å­˜åœ¨
                        try:
                            os.kill(process.pid, 0)  # æª¢æŸ¥é€²ç¨‹æ˜¯å¦å­˜åœ¨
                            self.logger.debug(f"{db_name}: é€²ç¨‹ {process.pid} ç¢ºå¯¦åœ¨é‹è¡Œ")
                        except OSError:
                            self.logger.warning(f"{db_name}: é€²ç¨‹ {process.pid} ä¸å­˜åœ¨ä½† poll() è¿”å› None")
                            return -1  # é€²ç¨‹ç•°å¸¸æ¶ˆå¤±
                    
                    return None
                    
                except Exception as e:
                    self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
                    return -1
            
            return None
    
    def export_manifest(self, work_dir: str, output_file: str = "vp_manifest.xml") -> bool:
        """å°å‡º manifest - ğŸ”¥ ä¿®æ”¹ï¼šå¢åŠ é‡è©¦æ©Ÿåˆ¶å’Œæ›´è©³ç´°çš„é©—è­‰"""
        cmd = f"{config_manager.repo_config['repo_command']} manifest -r -o {output_file}"
        self.logger.info(f"å°å‡º manifest: {cmd}")
        
        # ğŸ”¥ å¢åŠ è¶…æ™‚æ™‚é–“åˆ°æ›´é•·ï¼Œä¸¦æ·»åŠ é‡è©¦æ©Ÿåˆ¶
        max_attempts = 5  # å¢åŠ åˆ°5æ¬¡
        timeout_values = [120, 180, 240, 300, 600]  # é€æ­¥å¢åŠ è¶…æ™‚æ™‚é–“
        
        for attempt in range(max_attempts):
            timeout = timeout_values[attempt]
            self.logger.info(f"å°å‡º manifest å˜—è©¦ {attempt + 1}/{max_attempts}ï¼Œè¶…æ™‚è¨­å®š: {timeout}ç§’")
            
            success, output = self.run_command(cmd, cwd=work_dir, timeout=timeout)
            
            if success:
                output_path = os.path.join(work_dir, output_file)
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    
                    # ğŸ”¥ æ›´åš´æ ¼çš„æª”æ¡ˆæœ‰æ•ˆæ€§æª¢æŸ¥
                    if file_size > 1000:  # è‡³å°‘è¦æœ‰ 1KB
                        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºæœ‰æ•ˆçš„ XML
                        try:
                            with open(output_path, 'r', encoding='utf-8') as f:
                                first_line = f.readline().strip()
                                if first_line.startswith('<?xml') or '<manifest' in first_line:
                                    self.logger.info(f"âœ… æˆåŠŸå°å‡ºæœ‰æ•ˆçš„  manifest: {output_path} ({file_size} bytes)")
                                    return True
                                else:
                                    self.logger.warning(f"âŒ å°å‡ºæª”æ¡ˆæ ¼å¼ç„¡æ•ˆ: {first_line[:50]}")
                                    continue
                        except Exception as e:
                            self.logger.warning(f"âŒ ç„¡æ³•é©—è­‰å°å‡ºæª”æ¡ˆ: {e}")
                            continue
                    else:
                        self.logger.warning(f"âŒ å°å‡ºæª”æ¡ˆå¤ªå°: {file_size} bytes")
                        continue
                else:
                    self.logger.warning(f"âŒ å°å‡ºæª”æ¡ˆä¸å­˜åœ¨: {output_path}")
            else:
                self.logger.warning(f"âŒ å°å‡ºå˜—è©¦ {attempt + 1} å¤±æ•—: {output}")
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡ï¼Œç­‰å¾…å¾Œé‡è©¦
                if attempt < max_attempts - 1:
                    wait_time = 10 * (attempt + 1)
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
        
        self.logger.error(f"âŒ æ‰€æœ‰å°å‡ºå˜—è©¦éƒ½å¤±æ•—")
        return False

# =====================================
# ===== ä¸»è¦è™•ç†é¡åˆ¥ =====
# =====================================

class ManifestPinningTool:
    """Manifest å®šç‰ˆå·¥å…·ï¼ˆæ”¹é€²ç‰ˆ + é›¶å¤±æ•—æ©Ÿåˆ¶ + Manifest æ¯”è¼ƒåŠŸèƒ½ï¼‰"""

    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.repo_manager = RepoManager()
        self.mapping_reader = MappingTableReader()
        self.source_cmd_manager = SourceCommandManager()
        self.manifest_comparator = ManifestComparator()  
        self.report = PinningReport()
        self.output_dir = config_manager.path_config['default_output_dir']
        self.dry_run = False
        self.zero_fail_mode = False
        
        # å³æ™‚å ±å‘Šæ›´æ–°çš„é–
        self._report_update_lock = threading.Lock()
        
        # ç·šç¨‹å®‰å…¨é–
        self._sftp_lock = threading.Lock()
        
        # ğŸ”¥ æ–°å¢ï¼šé˜²æ­¢é‡è¤‡è™•ç†çš„å±¬æ€§
        self._processed_dbs = set()
        self._db_processing_lock = threading.Lock()

    def load_mapping_table(self, file_path: str) -> bool:
        """è¼‰å…¥ mapping table"""
        return self.mapping_reader.load_excel(file_path)

    def get_all_dbs(self, db_type: str = 'all') -> List[DBInfo]:
        """å–å¾—æ‰€æœ‰ DB è³‡è¨Š"""
        return self.mapping_reader.get_db_info_list(db_type)

    def process_db_phase1(self, db_info: DBInfo) -> DBInfo:
        """
        æ”¹é€²ç‰ˆ Phase 1 è™•ç† - ç·šç¨‹å®‰å…¨ + ğŸ”¥ é˜²æ­¢é‡è¤‡è™•ç† + è·³éæ¸…ç†æª¢æŸ¥
        """
        db_info.start_time = datetime.now()
        local_sftp_manager = None
        
        # ğŸ”¥ æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨è™•ç†ä¸­ï¼Œé˜²æ­¢é‡è¤‡
        db_key = f"{db_info.db_info}_{db_info.db_type}_{db_info.module}"
        
        with self._db_processing_lock:
            if db_key in self._processed_dbs:
                self.logger.warning(f"{db_info.db_info}: å·²åœ¨è™•ç†ä¸­ï¼Œè·³éé‡è¤‡è™•ç†")
                return db_info
            
            self._processed_dbs.add(db_key)
        
        try:
            self.logger.info(f"é–‹å§‹è™•ç† {db_info.db_info} (Phase 1)")
            
            # å»ºç«‹æœ¬åœ°ç›®éŒ„
            local_path = os.path.join(self.output_dir, db_info.module, db_info.db_info)
            os.makedirs(local_path, exist_ok=True)
            db_info.local_path = local_path
            
            # ğŸ”¥ æª¢æŸ¥ç£ç¢Ÿç©ºé–“ï¼ˆè‡³å°‘éœ€è¦ 15GBï¼‰
            import shutil
            free_space = shutil.disk_usage(local_path).free
            required_space = 15 * 1024 * 1024 * 1024  # 15GB
            
            if free_space < required_space:
                raise Exception(f"ç£ç¢Ÿç©ºé–“ä¸è¶³: å¯ç”¨ {free_space/1024/1024/1024:.1f}GBï¼Œå»ºè­°è‡³å°‘ 15GB")
            
            self.logger.debug(f"{db_info.db_info}: ç£ç¢Ÿç©ºé–“æª¢æŸ¥é€šé ({free_space/1024/1024/1024:.1f}GB å¯ç”¨)")
            
            # ğŸ”¥ æª¢æŸ¥æ–‡ä»¶ç³»çµ±é¡å‹å’Œç¡¬éˆæ¥æ”¯æŒ
            try:
                # æª¢æŸ¥æ˜¯å¦æ”¯æŒç¡¬éˆæ¥
                test_file1 = os.path.join(local_path, f'.test_hardlink_src_{os.getpid()}')
                test_file2 = os.path.join(local_path, f'.test_hardlink_dst_{os.getpid()}')
                
                try:
                    with open(test_file1, 'w') as f:
                        f.write('test')
                    os.link(test_file1, test_file2)
                    os.unlink(test_file1)
                    os.unlink(test_file2)
                    self.logger.debug(f"{db_info.db_info}: æ–‡ä»¶ç³»çµ±æ”¯æŒç¡¬éˆæ¥")
                except Exception as e:
                    self.logger.warning(f"{db_info.db_info}: æ–‡ä»¶ç³»çµ±å¯èƒ½ä¸æ”¯æŒç¡¬éˆæ¥: {e}")
                    # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
                    for test_file in [test_file1, test_file2]:
                        try:
                            if os.path.exists(test_file):
                                os.unlink(test_file)
                        except:
                            pass
                            
            except Exception as e:
                self.logger.debug(f"æ–‡ä»¶ç³»çµ±æª¢æŸ¥å¤±æ•—: {e}")
            
            # Step 1: SFTP æ“ä½œï¼ˆç·šç¨‹å®‰å…¨ï¼‰
            with self._sftp_lock:
                self.logger.info(f"{db_info.db_info}: å¿«é€Ÿæœå°‹ manifest (ç·šç¨‹å®‰å…¨)")
                
                local_sftp_manager = SFTPManager()
                
                if not local_sftp_manager.connect():
                    raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
                
                target_version = db_info.version
                result = local_sftp_manager.find_latest_manifest(
                    db_info.sftp_path, 
                    db_info.db_info,
                    target_version
                )
                
                if not result:
                    raise Exception("æ‰¾ä¸åˆ° manifest æª”æ¡ˆ")
                
                manifest_full_path, manifest_name = result
                db_info.manifest_full_path = manifest_full_path
                db_info.manifest_file = manifest_name
                
                if not db_info.version:
                    match = re.match(r'manifest_(\d+)\.xml', manifest_name)
                    if match:
                        db_info.version = match.group(1)
                        self.logger.info(f"{db_info.db_info}: æª¢æ¸¬åˆ°ç‰ˆæœ¬ {db_info.version}")
                
                local_manifest = os.path.join(local_path, manifest_name)
                if not local_sftp_manager.download_file(manifest_full_path, local_manifest):
                    raise Exception("ä¸‹è¼‰ manifest å¤±æ•—")
                
                self.logger.info(f"{db_info.db_info}: manifest ä¸‹è¼‰å®Œæˆ: {manifest_name}")
                
                local_sftp_manager.disconnect()
                self.logger.info(f"{db_info.db_info}: âœ… SFTP é€£ç·šå·²ç«‹å³æ–·é–‹")
                local_sftp_manager = None
            
            # Step 2: æª¢æŸ¥ repo ç‹€æ…‹
            db_info.has_existing_repo = self.repo_manager.check_repo_exists(local_path)
            
            # ğŸ”¥ Step 2.5: æ¸…ç†æª¢æŸ¥ï¼ˆæ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦åŸ·è¡Œï¼‰
            if (db_info.has_existing_repo and 
                config_manager.repo_config.get('check_clean_before_sync', False)):
                
                self.logger.info(f"{db_info.db_info}: æª¢æŸ¥å·¥ä½œç©ºé–“æ˜¯å¦ä¹¾æ·¨")
                
                clean_status = self.repo_manager.check_repo_clean_status(local_path)
                
                if not clean_status['is_clean']:
                    self.logger.warning(f"{db_info.db_info}: ç™¼ç¾æœ¬åœ°ä¿®æ”¹: {clean_status['details']}")
                    
                    # è™•ç†é«’repo
                    handle_success = self.repo_manager.handle_dirty_repo(
                        local_path, 
                        db_info.db_info, 
                        clean_status
                    )
                    
                    if not handle_success:
                        raise Exception(f"å·¥ä½œç©ºé–“æœ‰æœªæäº¤çš„ä¿®æ”¹ä¸”ç„¡æ³•è‡ªå‹•æ¸…ç†: {clean_status['details']}")
                    
                    self.logger.info(f"{db_info.db_info}: âœ… å·¥ä½œç©ºé–“æ¸…ç†å®Œæˆ")
                else:
                    self.logger.info(f"{db_info.db_info}: âœ… å·¥ä½œç©ºé–“ç‹€æ…‹ä¹¾æ·¨")
            else:
                if db_info.has_existing_repo:
                    self.logger.info(f"{db_info.db_info}: â­ï¸ è·³éå·¥ä½œç©ºé–“æ¸…ç†æª¢æŸ¥ï¼ˆæ ¹æ“šé…ç½®ï¼‰")
                else:
                    self.logger.info(f"{db_info.db_info}: æ–°çš„å·¥ä½œç©ºé–“ï¼Œç„¡éœ€æ¸…ç†")
            
            # Step 3: ç²å– source command
            self.logger.info(f"{db_info.db_info}: ç²å– source command")
            source_cmd = self.source_cmd_manager.get_source_command(db_info, self.mapping_reader.df)
            if not source_cmd:
                raise Exception("ç„¡æ³•å–å¾— source command")
            
            db_info.actual_source_cmd = source_cmd
            self.logger.info(f"{db_info.db_info}: source command ç²å–æˆåŠŸ")
            
            # Step 4: åŸ·è¡Œ repo initï¼ˆå¦‚æœéœ€è¦ï¼‰
            self.logger.info(f"{db_info.db_info}: åŸ·è¡Œ repo åˆå§‹åŒ–")
            if not db_info.has_existing_repo:
                self.logger.info(f"{db_info.db_info}: .repo ç›®éŒ„ä¸å­˜åœ¨ï¼ŒåŸ·è¡Œå®Œæ•´ repo init")
                if not self.repo_manager.repo_init(local_path, source_cmd):
                    raise Exception("Repo init å¤±æ•—")
            else:
                self.logger.info(f"{db_info.db_info}: .repo ç›®éŒ„å­˜åœ¨ï¼Œè·³é repo init")
            
            # Step 5: æ‡‰ç”¨ manifestï¼ˆé€™æ˜¯é—œéµæ­¥é©Ÿï¼‰
            self.logger.info(f"{db_info.db_info}: é–‹å§‹æ‡‰ç”¨ manifestï¼ˆå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰")
            if not self.repo_manager.apply_manifest(local_path, local_manifest):
                raise Exception("å¥—ç”¨ manifest å¤±æ•—")
            
            self.logger.info(f"{db_info.db_info}: âœ… repo åˆå§‹åŒ–å®Œæˆ")
            
            # Step 6: å•Ÿå‹• repo sync
            if not self.dry_run:
                self.logger.info(f"{db_info.db_info}: å•Ÿå‹• repo sync")
                process = self.repo_manager.start_repo_sync_async(local_path, db_info.db_info)
                if not process:
                    raise Exception("å•Ÿå‹• repo sync å¤±æ•—")
                
                db_info.sync_process = process
                self.logger.info(f"{db_info.db_info}: repo sync å·²å•Ÿå‹• (PID: {process.pid})")
            else:
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                self.logger.info(f"{db_info.db_info}: æ¸¬è©¦æ¨¡å¼ - è·³é repo sync")
            
            self.logger.info(f"{db_info.db_info}: âœ… Phase 1 å®Œæˆ")
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            db_info.end_time = datetime.now()
            self.logger.error(f"{db_info.db_info}: âŒ Phase 1 å¤±æ•— - {str(e)}")
            
            # ğŸ”¥ è™•ç†å¤±æ•—æ™‚ç§»é™¤è™•ç†æ¨™è¨˜ï¼Œå…è¨±é‡è©¦
            with self._db_processing_lock:
                self._processed_dbs.discard(db_key)
                
        finally:
            # ç¢ºä¿ SFTP é€£ç·šä¸€å®šæœƒè¢«æ–·é–‹
            if local_sftp_manager:
                try:
                    local_sftp_manager.disconnect()
                    self.logger.debug(f"{db_info.db_info}: ç¢ºä¿ SFTP é€£ç·šå·²æ–·é–‹")
                except:
                    pass
        
        return db_info

    def process_db_phase2(self, db_info: DBInfo) -> DBInfo:
        """ğŸ”¥ æ–°ç‰ˆ Phase 2ï¼šSync å®Œæˆå¾Œç«‹å³æ¯”è¼ƒ manifest ä¸¦æ›´æ–°å ±å‘Š"""
        try:
            # ğŸ”¥ é˜²æ­¢é‡è¤‡è™•ç†ï¼šå¦‚æœå·²ç¶“æœ‰æœ€çµ‚ç‹€æ…‹ï¼Œç›´æ¥è¿”å›
            if db_info.status in [DBStatus.SUCCESS, DBStatus.SUCCESS_WITH_DIFF, DBStatus.FAILED, DBStatus.SKIPPED]:
                self.logger.debug(f"{db_info.db_info}: å·²è™•ç†å®Œæˆï¼Œç‹€æ…‹: {db_info.status.value}")
                return db_info
            
            self.logger.info(f"{db_info.db_info}: é–‹å§‹ Phase 2")
            
            if self.dry_run:
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                self.logger.info(f"{db_info.db_info}: æ¸¬è©¦æ¨¡å¼å®Œæˆ")
                return db_info
            
            # æª¢æŸ¥ sync é€²ç¨‹ç‹€æ…‹
            sync_result = None
            if db_info.sync_process:
                poll = self.repo_manager.check_process_status(
                    db_info.db_info, 
                    db_info.sync_process
                )
                
                if poll is None:
                    self.logger.debug(f"{db_info.db_info}: repo sync ä»åœ¨åŸ·è¡Œä¸­")
                    return db_info  # é‚„åœ¨åŸ·è¡Œä¸­
                
                sync_result = poll
                
                # è¨˜éŒ„ sync log è·¯å¾‘
                if hasattr(db_info.sync_process, '_log_file_path'):
                    db_info.sync_log_path = db_info.sync_process._log_file_path
            
            # ğŸ”¥ å…ˆæª¢æŸ¥ sync çµæœï¼Œåªæœ‰æˆåŠŸçš„æ‰å˜—è©¦å°å‡º manifest
            if sync_result is None:
                # æ²’æœ‰ sync é€²ç¨‹ï¼Œå¯èƒ½æ˜¯ dry run æˆ–å…¶ä»–æƒ…æ³
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                self.logger.info(f"{db_info.db_info}: æ²’æœ‰ sync é€²ç¨‹ï¼Œè·³é")
                return db_info
            
            elif sync_result == 0:
                # ğŸ‰ å®Œå…¨æˆåŠŸçš„æƒ…æ³
                self.logger.info(f"{db_info.db_info}: âœ… Sync å®Œæˆï¼Œç«‹å³å°å‡ºç‰ˆæœ¬è³‡è¨Š ...")
                
                # Step 1: ç«‹å³å°å‡º vp_manifest.xml
                exported_manifest_path = os.path.join(db_info.local_path, 'vp_manifest.xml')
                export_success = self.repo_manager.export_manifest(db_info.local_path, 'vp_manifest.xml')
                
                if export_success and os.path.exists(exported_manifest_path):
                    db_info.exported_manifest_path = exported_manifest_path
                    self.logger.info(f"{db_info.db_info}: âœ… manifest å°å‡ºæˆåŠŸ")
                    
                    # Step 2: ç«‹å³æ¯”è¼ƒ manifest
                    self.logger.info(f"{db_info.db_info}: ğŸ” é–‹å§‹æ¯”è¼ƒ manifest...")
                    db_info.status = DBStatus.COMPARING
                    
                    original_manifest_path = os.path.join(db_info.local_path, db_info.manifest_file)
                    comparison_result = self.manifest_comparator.compare_manifests(
                        original_manifest_path, 
                        exported_manifest_path
                    )
                    
                    # ä¿å­˜æ¯”è¼ƒçµæœ
                    db_info.manifest_comparison_result = comparison_result
                    db_info.manifest_is_identical = comparison_result.get('is_identical', False)
                    db_info.manifest_differences_count = comparison_result.get('difference_count', 0)
                    db_info.manifest_comparison_summary = comparison_result.get('summary', 'æ¯”è¼ƒå¤±æ•—')
                    
                    # ç”Ÿæˆå·®ç•°å ±å‘Šï¼ˆå¦‚æœæœ‰å·®ç•°ï¼‰
                    if not db_info.manifest_is_identical:
                        diff_report_path = os.path.join(db_info.local_path, f'manifest_diff_report.txt')
                        self.manifest_comparator.generate_diff_report(comparison_result, diff_report_path)
                        self.logger.info(f"{db_info.db_info}: ğŸ“„ å·®ç•°å ±å‘Šå·²ä¿å­˜: {diff_report_path}")
                    
                    self.logger.info(f"{db_info.db_info}: ğŸ” Manifest æ¯”è¼ƒå®Œæˆ: {db_info.manifest_comparison_summary}")
                    
                    # è¨­å®šæœ€çµ‚ç‹€æ…‹
                    if db_info.manifest_is_identical:
                        db_info.status = DBStatus.SUCCESS
                    else:
                        db_info.status = DBStatus.SUCCESS_WITH_DIFF
                        
                else:
                    self.logger.warning(f"{db_info.db_info}: âš ï¸ manifest å°å‡ºå¤±æ•—")
                    db_info.manifest_comparison_summary = "å°å‡ºå¤±æ•—ï¼Œç„¡æ³•æ¯”è¼ƒ"
                    db_info.status = DBStatus.SUCCESS  # sync æˆåŠŸä½†å°å‡ºå¤±æ•—ï¼Œé‚„æ˜¯ç®—æˆåŠŸ
                
                db_info.end_time = datetime.now()
                elapsed = db_info.end_time - db_info.start_time
                self.logger.info(f"{db_info.db_info}: âœ… å®Œå…¨æˆåŠŸ (è€—æ™‚: {elapsed})")
            
            elif sync_result == 1:
                # ğŸ”„ å¯èƒ½çš„éƒ¨åˆ†æˆåŠŸæƒ…æ³
                self.logger.info(f"{db_info.db_info}: ğŸ” åˆ†æéƒ¨åˆ†æˆåŠŸçµæœ...")
                success_rate, failed_projects = self._analyze_sync_result(db_info)
                
                if success_rate >= 95.0:  # ğŸ”¥ 95% ä»¥ä¸ŠæˆåŠŸç‡ç®—æˆåŠŸ
                    self.logger.info(f"{db_info.db_info}: âœ… éƒ¨åˆ†æˆåŠŸ ({success_rate:.1f}%)ï¼Œå˜—è©¦å°å‡ºç‰ˆæœ¬è³‡è¨Š...")
                    
                    # å˜—è©¦å°å‡º manifest
                    exported_manifest_path = os.path.join(db_info.local_path, 'vp_manifest.xml')
                    export_success = self.repo_manager.export_manifest(db_info.local_path, 'vp_manifest.xml')
                    
                    if export_success and os.path.exists(exported_manifest_path):
                        # æœ‰æˆåŠŸå°å‡ºï¼Œé€²è¡Œæ¯”è¼ƒ
                        db_info.exported_manifest_path = exported_manifest_path
                        self.logger.info(f"{db_info.db_info}: ğŸ” é–‹å§‹æ¯”è¼ƒ manifest...")
                        
                        original_manifest_path = os.path.join(db_info.local_path, db_info.manifest_file)
                        comparison_result = self.manifest_comparator.compare_manifests(
                            original_manifest_path, 
                            exported_manifest_path
                        )
                        
                        db_info.manifest_comparison_result = comparison_result
                        db_info.manifest_is_identical = comparison_result.get('is_identical', False)
                        db_info.manifest_differences_count = comparison_result.get('difference_count', 0)
                        db_info.manifest_comparison_summary = comparison_result.get('summary', 'æ¯”è¼ƒå¤±æ•—')
                        
                        if db_info.manifest_is_identical:
                            db_info.status = DBStatus.SUCCESS
                        else:
                            db_info.status = DBStatus.SUCCESS_WITH_DIFF
                    else:
                        # å°å‡ºå¤±æ•—ä½† sync éƒ¨åˆ†æˆåŠŸ
                        db_info.status = DBStatus.SUCCESS
                        db_info.manifest_comparison_summary = "éƒ¨åˆ†æˆåŠŸä½†å°å‡ºå¤±æ•—"
                    
                    warning_msg = f"éƒ¨åˆ†æˆåŠŸ ({success_rate:.1f}%)ï¼Œå¤±æ•—é …ç›®: {len(failed_projects)} å€‹"
                    if failed_projects:
                        warning_msg += f" - {', '.join(failed_projects[:3])}"
                        if len(failed_projects) > 3:
                            warning_msg += f" ç­‰ {len(failed_projects)} å€‹"
                    
                    db_info.error_message = warning_msg
                else:
                    # æˆåŠŸç‡å¤ªä½ï¼Œç®—å¤±æ•—
                    raise Exception(f"åŒæ­¥æˆåŠŸç‡å¤ªä½ ({success_rate:.1f}%)ï¼Œå¤±æ•—é …ç›®: {len(failed_projects)} å€‹")
                
                db_info.end_time = datetime.now()
                elapsed = db_info.end_time - db_info.start_time
                self.logger.info(f"{db_info.db_info}: âœ… éƒ¨åˆ†æˆåŠŸå®Œæˆ (è€—æ™‚: {elapsed})")
            
            else:
                # ğŸš« å…¶ä»–éŒ¯èª¤ç¢¼ï¼Œç›´æ¥ç®—å¤±æ•—ï¼Œä¸å˜—è©¦å°å‡º
                if sync_result == -15:
                    error_msg = "Sync è¢«çµ‚æ­¢ (SIGTERM)"
                elif sync_result == -9:
                    error_msg = "Sync è¢«å¼·åˆ¶çµ‚æ­¢ (SIGKILL)"
                elif sync_result < 0:
                    error_msg = f"Sync è¢«ä¿¡è™Ÿçµ‚æ­¢ (ä¿¡è™Ÿ: {-sync_result})"
                else:
                    error_msg = f"Sync å¤±æ•— (è¿”å›ç¢¼: {sync_result})"
                
                raise Exception(error_msg)
            
            # Step 3: ğŸ”¥ ç«‹å³æ›´æ–°å ±å‘Šåˆ° Excelï¼ˆä¸ç­‰æœ€å¾Œï¼‰
            self._update_report_immediately(db_info)
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            db_info.end_time = datetime.now()
            self.logger.error(f"{db_info.db_info}: Phase 2 å¤±æ•— - {str(e)}")
            
            # å¤±æ•—çš„ä¹Ÿè¦ç«‹å³æ›´æ–°å ±å‘Š
            self._update_report_immediately(db_info)
        
        return db_info

    def _update_report_immediately(self, db_info: DBInfo):
        """ğŸ”¥ ä¿®å¾©ç‰ˆæœ¬ï¼šå³æ™‚æ›´æ–°å–®å€‹ DB çš„å ±å‘Šåˆ° Excel - é˜²æ­¢é‡è¤‡"""
        try:
            with self._report_update_lock:
                self.logger.info(f"{db_info.db_info}: ğŸ“Š ç«‹å³æ›´æ–°å ±å‘Š...")
                
                # ğŸ”¥ æ›´ç²¾ç¢ºçš„æŸ¥æ‰¾é‚è¼¯ - ä½¿ç”¨å¤šå€‹æ¢ä»¶
                found_index = -1
                for i, existing_db in enumerate(self.report.db_details):
                    # ğŸ”¥ ä½¿ç”¨æ›´åš´æ ¼çš„æ¯”è¼ƒæ¢ä»¶
                    if (existing_db.db_info == db_info.db_info and 
                        existing_db.db_type == db_info.db_type and
                        existing_db.module == db_info.module):
                        found_index = i
                        break
                
                if found_index >= 0:
                    # ğŸ”¥ æ›´æ–°ç¾æœ‰è¨˜éŒ„ - ä¿ç•™è¼ƒæ–°çš„è³‡æ–™
                    existing_db = self.report.db_details[found_index]
                    
                    # åªæ›´æ–°æœ‰å€¼çš„æ¬„ä½ï¼Œé¿å…è¦†è“‹æœ‰ç”¨è³‡æ–™
                    if db_info.status != DBStatus.PENDING:
                        existing_db.status = db_info.status
                    if db_info.end_time:
                        existing_db.end_time = db_info.end_time
                    if db_info.error_message:
                        existing_db.error_message = db_info.error_message
                    if db_info.sync_log_path:
                        existing_db.sync_log_path = db_info.sync_log_path
                    if db_info.exported_manifest_path:
                        existing_db.exported_manifest_path = db_info.exported_manifest_path
                    if db_info.manifest_comparison_result:
                        existing_db.manifest_comparison_result = db_info.manifest_comparison_result
                        existing_db.manifest_is_identical = db_info.manifest_is_identical
                        existing_db.manifest_differences_count = db_info.manifest_differences_count
                        existing_db.manifest_comparison_summary = db_info.manifest_comparison_summary
                    
                    self.logger.debug(f"{db_info.db_info}: æ›´æ–°ç¾æœ‰è¨˜éŒ„ (ç´¢å¼•: {found_index})")
                else:
                    # ğŸ”¥ æ·»åŠ æ–°è¨˜éŒ„å‰å†æ¬¡ç¢ºèªä¸é‡è¤‡
                    duplicate_check = any(
                        existing.db_info == db_info.db_info and 
                        existing.db_type == db_info.db_type and
                        existing.module == db_info.module
                        for existing in self.report.db_details
                    )
                    
                    if not duplicate_check:
                        self.report.db_details.append(db_info)
                        self.logger.debug(f"{db_info.db_info}: æ·»åŠ æ–°è¨˜éŒ„")
                    else:
                        self.logger.warning(f"{db_info.db_info}: ç™¼ç¾é‡è¤‡ï¼Œè·³éæ·»åŠ ")
                
                # ğŸ”¥ æ¸…ç†é‡è¤‡è³‡æ–™
                self._remove_duplicates()
                
                # é‡æ–°è¨ˆç®—çµ±è¨ˆ
                self.report.total_dbs = len(self.report.db_details)
                self.report.successful_dbs = sum(1 for db in self.report.db_details 
                                            if db.status in [DBStatus.SUCCESS, DBStatus.SUCCESS_WITH_DIFF])
                self.report.failed_dbs = sum(1 for db in self.report.db_details if db.status == DBStatus.FAILED)
                self.report.skipped_dbs = sum(1 for db in self.report.db_details if db.status == DBStatus.SKIPPED)
                self.report.dbs_with_differences = sum(1 for db in self.report.db_details if db.status == DBStatus.SUCCESS_WITH_DIFF)
                
                # ç«‹å³ç”Ÿæˆä¸¦æ›´æ–° Excel å ±å‘Š
                report_path = os.path.join(self.output_dir, config_manager.path_config['report_filename'])
                self._generate_partial_report(report_path)
                
                self.logger.info(f"{db_info.db_info}: âœ… å ±å‘Šå·²ç«‹å³æ›´æ–°")
                
        except Exception as e:
            self.logger.warning(f"{db_info.db_info}: ç«‹å³æ›´æ–°å ±å‘Šå¤±æ•—: {e}")
            
    # ğŸ”¥ æ–°å¢ï¼šæ¸…ç†é‡è¤‡è³‡æ–™çš„æ–¹æ³•
    def _remove_duplicates(self):
        """æ¸…ç†å ±å‘Šä¸­çš„é‡è¤‡è³‡æ–™"""
        try:
            seen = set()
            unique_dbs = []
            
            for db in self.report.db_details:
                # å»ºç«‹å”¯ä¸€è­˜åˆ¥ç¬¦
                key = (db.db_info, db.db_type, db.module)
                
                if key not in seen:
                    seen.add(key)
                    unique_dbs.append(db)
                else:
                    self.logger.debug(f"ç§»é™¤é‡è¤‡è¨˜éŒ„: {db.db_info} ({db.db_type}, {db.module})")
            
            # å¦‚æœç™¼ç¾é‡è¤‡ï¼Œæ›´æ–°åˆ—è¡¨
            if len(unique_dbs) != len(self.report.db_details):
                removed_count = len(self.report.db_details) - len(unique_dbs)
                self.report.db_details = unique_dbs
                self.logger.info(f"æ¸…ç†äº† {removed_count} ç­†é‡è¤‡è¨˜éŒ„")
                
        except Exception as e:
            self.logger.warning(f"æ¸…ç†é‡è¤‡è³‡æ–™å¤±æ•—: {e}")
                        
    def _update_report_immediately(self, db_info: DBInfo):
        """ğŸ”¥ æ–°å¢ï¼šç«‹å³æ›´æ–°å–®å€‹ DB çš„å ±å‘Šåˆ° Excel"""
        try:
            with self._report_update_lock:
                self.logger.info(f"{db_info.db_info}: ğŸ“Š ç«‹å³æ›´æ–°å ±å‘Š...")
                
                # æ›´æ–°æˆ–æ·»åŠ åˆ°å ±å‘Šä¸­
                found = False
                for i, existing_db in enumerate(self.report.db_details):
                    if existing_db.db_info == db_info.db_info and existing_db.db_type == db_info.db_type:
                        self.report.db_details[i] = db_info
                        found = True
                        break
                
                if not found:
                    self.report.db_details.append(db_info)
                
                # é‡æ–°è¨ˆç®—çµ±è¨ˆ
                self.report.total_dbs = len(self.report.db_details)
                self.report.successful_dbs = sum(1 for db in self.report.db_details 
                                               if db.status in [DBStatus.SUCCESS, DBStatus.SUCCESS_WITH_DIFF])
                self.report.failed_dbs = sum(1 for db in self.report.db_details if db.status == DBStatus.FAILED)
                self.report.skipped_dbs = sum(1 for db in self.report.db_details if db.status == DBStatus.SKIPPED)
                self.report.dbs_with_differences = sum(1 for db in self.report.db_details if db.status == DBStatus.SUCCESS_WITH_DIFF)
                
                # ç«‹å³ç”Ÿæˆä¸¦æ›´æ–° Excel å ±å‘Š
                report_path = os.path.join(self.output_dir, config_manager.path_config['report_filename'])
                self._generate_partial_report(report_path)
                
                self.logger.info(f"{db_info.db_info}: âœ… å ±å‘Šå·²ç«‹å³æ›´æ–°")
                
        except Exception as e:
            self.logger.warning(f"{db_info.db_info}: ç«‹å³æ›´æ–°å ±å‘Šå¤±æ•—: {e}")

    def _wait_and_process_syncs_enhanced(self, db_results: List[DBInfo]):
        """ğŸ”¥ æ–°ç‰ˆï¼šç­‰å¾…åŒæ­¥å®Œæˆä¸¦å³æ™‚è™•ç†ï¼Œé¿å…é‡è¤‡è™•ç†"""
        max_wait_time = config_manager.repo_config['sync_timeout']
        start_wait = time.time()
        
        active_syncs = [db for db in db_results if db.sync_process and db.status != DBStatus.FAILED]
        self.logger.info(f"ç›£æ§ {len(active_syncs)} å€‹æ´»èºçš„ repo sync é€²ç¨‹")
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
        progress_tracker = {}
        processed_dbs = set()  # ğŸ”¥ è¿½è¹¤å·²è™•ç†çš„ DBï¼Œé¿å…é‡è¤‡è™•ç†
        
        for db_info in active_syncs:
            progress_tracker[db_info.db_info] = {
                'start_time': db_info.start_time or datetime.now(),
                'last_log_size': 0,
                'estimated_progress': 0,
                'current_activity': 'åˆå§‹åŒ–ä¸­...',
                'log_file': self._get_sync_log_file(db_info),
                'last_check_time': datetime.now(),
                'error_count': 0,
                'critical_errors': []
            }
        
        check_interval = 30  # 30ç§’æª¢æŸ¥ä¸€æ¬¡
        
        while True:
            all_complete = True
            elapsed = int(time.time() - start_wait)
            current_time = time.time()
            
            print("\n" + "="*100)
            print(f"ğŸ“Š Repo Sync é€²åº¦ç›£æ§ - å·²ç­‰å¾… {elapsed}s")
            print("="*100)
            
            current_failed_count = 0
            current_running_count = 0
            current_completed_count = 0
            
            for db_info in active_syncs:
                db_name = db_info.db_info
                
                # ğŸ”¥ å¦‚æœå·²ç¶“è™•ç†éï¼Œè·³é
                if db_name in processed_dbs:
                    current_completed_count += 1
                    continue
                
                if db_info.status == DBStatus.FAILED:
                    current_failed_count += 1
                    processed_dbs.add(db_name)
                    continue
                
                tracker = progress_tracker[db_name]
                
                # æ§‹å»ºåŒ…å«ç‰ˆæœ¬ä¿¡æ¯çš„é¡¯ç¤ºåç¨±
                manifest_info = ""
                if db_info.manifest_file:
                    manifest_info = f" ({db_info.manifest_file})"
                elif db_info.version:
                    manifest_info = f" (v{db_info.version})"
                
                display_name = f"{db_name}{manifest_info}"
                
                if db_info.sync_process:
                    try:
                        poll = db_info.sync_process.poll()
                        
                        if poll is None:  # ä»åœ¨é‹è¡Œ
                            all_complete = False
                            current_running_count += 1
                            
                            # æ›´æ–°é€²åº¦ä¿¡æ¯
                            self._update_progress_info(db_info, tracker)
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            progress = tracker['estimated_progress']
                            bar_length = 20
                            filled = int(bar_length * progress / 100)
                            bar = "â–ˆ" * filled + "â–’" * (bar_length - filled)
                            
                            activity = tracker.get('current_project', '').split('/')[-1] or tracker.get('current_activity', 'åŒæ­¥ä¸­')
                            if len(activity) > 15:
                                activity = activity[:12] + "..."
                            
                            # é¡¯ç¤ºéŒ¯èª¤ç‹€æ…‹
                            status_info = ""
                            if tracker['critical_errors']:
                                status_info = f" âš ï¸{len(tracker['critical_errors'])}"
                            
                            print(f"ğŸ”„ {display_name:30s} â”‚{bar}â”‚ {progress:3d}% â”‚ {runtime_str} â”‚ {activity}{status_info}")
                            
                            # æª¢æŸ¥è¶…æ™‚
                            if time.time() - start_wait > max_wait_time:
                                self.logger.warning(f"{db_name}: repo sync è¶…æ™‚ï¼Œå¼·åˆ¶çµ‚æ­¢")
                                try:
                                    db_info.sync_process.terminate()
                                    db_info.sync_process.wait(timeout=5)
                                except:
                                    try:
                                        db_info.sync_process.kill()
                                    except:
                                        pass
                                db_info.status = DBStatus.FAILED
                                db_info.error_message = "Sync è¶…æ™‚"
                                current_failed_count += 1
                                processed_dbs.add(db_name)
                                print(f"â° {display_name:30s} â”‚ è¶…æ™‚çµ‚æ­¢")
                                
                        else:  # é€²ç¨‹å·²çµæŸ
                            # ğŸ”¥ ç«‹å³è™•ç†é€™å€‹ DBï¼Œåªè™•ç†ä¸€æ¬¡
                            if db_name not in processed_dbs:
                                processed_dbs.add(db_name)
                                
                                # åœ¨å¾Œå°ç·šç¨‹ä¸­è™•ç† Phase 2ï¼Œé¿å…é˜»å¡ç›£æ§
                                def process_phase2_async(db):
                                    try:
                                        processed_db = self.process_db_phase2(db)
                                    except Exception as e:
                                        self.logger.error(f"{db.db_info}: å¾Œå°è™•ç† Phase 2 å¤±æ•—: {e}")
                                
                                # å•Ÿå‹•å¾Œå°è™•ç†
                                import threading
                                phase2_thread = threading.Thread(target=process_phase2_async, args=(db_info,), daemon=True)
                                phase2_thread.start()
                                
                                runtime = datetime.now() - tracker['start_time']
                                runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                                
                                if poll == 0:  # æˆåŠŸå®Œæˆ
                                    current_completed_count += 1
                                    bar = "â–ˆ" * 20
                                    print(f"âœ… {display_name:30s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ å®Œæˆ+è™•ç†ä¸­")
                                else:  # å¤±æ•—
                                    current_failed_count += 1
                                    error_msg = f"Sync å¤±æ•— (è¿”å›ç¢¼: {poll})"
                                    print(f"âŒ {display_name:30s} â”‚{'':20s}â”‚   0% â”‚ {runtime_str} â”‚ {error_msg[:30]}")
                            else:
                                # å·²ç¶“è™•ç†éçš„
                                current_completed_count += 1
                            
                    except Exception as e:
                        self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹å¤±æ•—: {e}")
                        if db_name not in processed_dbs:
                            processed_dbs.add(db_name)
                            db_info.status = DBStatus.FAILED
                            db_info.error_message = f"é€²ç¨‹ç›£æ§å¤±æ•—: {e}"
                            self.report.add_db(db_info)
                            current_failed_count += 1
                            print(f"âš ï¸  {display_name:30s} â”‚ ç›£æ§éŒ¯èª¤ â”‚   0% â”‚ {str(e)[:30]}")
            
            # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
            print("-"*100)
            print(f"ğŸ“ˆ ç¸½è¨ˆ: é‹è¡Œä¸­ {current_running_count} | å®Œæˆ {current_completed_count} | å¤±æ•— {current_failed_count}")
            
            if all_complete or (time.time() - start_wait) > max_wait_time:
                break
            
            # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
            time.sleep(check_interval)
        
        # ç­‰å¾…æ‰€æœ‰å¾Œå° Phase 2 è™•ç†å®Œæˆ
        self.logger.info("ç­‰å¾…æ‰€æœ‰å¾Œå°è™•ç†å®Œæˆ...")
        time.sleep(5)  # çµ¦å¾Œå°ç·šç¨‹ä¸€äº›æ™‚é–“å®Œæˆ
        
        # æœ€çµ‚çµ±è¨ˆ
        completed = len([db for db in active_syncs if db.db_info in processed_dbs and 
                        db.status in [DBStatus.SUCCESS, DBStatus.SUCCESS_WITH_DIFF]])
        failed = len([db for db in active_syncs if db.status == DBStatus.FAILED])
        
        print(f"\nğŸ“‹ Repo sync æœ€çµ‚çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸ: {completed}")
        print(f"   âŒ å¤±æ•—: {failed}")
        
        self.logger.info(f"ğŸ“‹ Repo sync å®Œæˆçµ±è¨ˆ: æˆåŠŸ {completed}, å¤±æ•— {failed}")
        
    def _generate_partial_report(self, output_file: str):
        """ğŸ”¥ æ–°å¢ï¼šç”Ÿæˆéƒ¨åˆ†å ±å‘Šï¼ˆç”¨æ–¼å³æ™‚æ›´æ–°ï¼‰"""
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            report_data = []
            for i, db in enumerate(self.report.db_details, 1):
                # æ‰‹å‹•æ§‹å»ºå­—å…¸ï¼Œç¢ºä¿æ‰€æœ‰æ¬„ä½æ­£ç¢º
                db_dict = {
                    'sn': i,
                    'module': db.module,
                    'db_type': db.db_type,
                    'db_info': db.db_info,
                    'status': db.status.value if hasattr(db.status, 'value') else str(db.status),
                    'version': db.version or 'æœªæŒ‡å®š',
                    'manifest_file': db.manifest_file or '',
                    'start_time': db.start_time.strftime('%Y-%m-%d %H:%M:%S') if db.start_time else '',
                    'end_time': db.end_time.strftime('%Y-%m-%d %H:%M:%S') if db.end_time else '',
                    'error_message': db.error_message or '',
                    'sync_log_path': db.sync_log_path or '',
                    'sftp_path': db.sftp_path,
                    'local_path': db.local_path or '',
                    'has_existing_repo': 'æ˜¯' if db.has_existing_repo else 'å¦',
                    # 'jira_link': db.jira_link or 'æœªæ‰¾åˆ°',
                    # ğŸ”¥ æ–°å¢ manifest æ¯”è¼ƒç›¸é—œæ¬„ä½
                    'manifest_æ¯”è¼ƒçµæœ': db.manifest_comparison_summary or '',
                    'manifest_æ˜¯å¦ç›¸åŒ': 'æ˜¯' if db.manifest_is_identical else 'å¦' if db.manifest_is_identical is False else '',
                    'manifest_å·®ç•°æ•¸é‡': db.manifest_differences_count or 0,
                    'exported_manifest_path': db.exported_manifest_path or '',
                }
                
                # é‡æ–°å‘½åæ¬„ä½
                db_dict['å®Œæ•´_JIRA_é€£çµ'] = db.jira_link or 'æœªæ‰¾åˆ°'
                db_dict['å®Œæ•´_repo_init_æŒ‡ä»¤'] = db.actual_source_cmd or 'æœªè¨˜éŒ„'
                
                report_data.append(db_dict)
            
            df = pd.DataFrame(report_data)
            
            # é‡æ–°æ’åˆ—æ¬„ä½é †åº
            important_columns = [
                'sn', 'module', 'db_type', 'db_info', 'status', 'version', 'manifest_file',
                'manifest_æ¯”è¼ƒçµæœ', 'manifest_æ˜¯å¦ç›¸åŒ', 'manifest_å·®ç•°æ•¸é‡',
                'å®Œæ•´_JIRA_é€£çµ', 'å®Œæ•´_repo_init_æŒ‡ä»¤',
                'start_time', 'end_time', 'sync_log_path', 'error_message'
            ]
            
            existing_columns = [col for col in important_columns if col in df.columns]
            other_columns = [col for col in df.columns if col not in important_columns]
            df = df[existing_columns + other_columns]
            
            # ğŸ”¥ ä¿®æ­£ï¼šé‡æ–°è¨ˆç®—çµ±è¨ˆï¼ŒåŸºæ–¼å¯¦éš›çš„ status å€¼
            status_counts = df['status'].value_counts()
            successful_count = status_counts.get('âœ… å®Œæˆ', 0) + status_counts.get('âœ… å®Œæˆ(æœ‰å·®ç•°)', 0)
            failed_count = status_counts.get('âŒ å¤±æ•—', 0)
            skipped_count = status_counts.get('â­ï¸ è·³é', 0)
            dbs_with_diff = status_counts.get('âœ… å®Œæˆ(æœ‰å·®ç•°)', 0)
            
            # å»ºç«‹æ‘˜è¦
            current_time = datetime.now()
            summary = {
                'é …ç›®': ['ç¸½ DB æ•¸', 'æˆåŠŸ', 'å¤±æ•—', 'è·³é', 'æœ‰å·®ç•°', 'æœ€å¾Œæ›´æ–°æ™‚é–“'],
                'æ•¸å€¼': [
                    len(self.report.db_details),
                    successful_count,
                    failed_count,
                    skipped_count,
                    dbs_with_diff,
                    current_time.strftime('%Y-%m-%d %H:%M:%S')
                ]
            }
            summary_df = pd.DataFrame(summary)
            
            # å¯«å…¥æ”¹é€²ç‰ˆ Excel
            self._write_enhanced_excel(df, summary_df, output_file)
            
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆéƒ¨åˆ†å ±å‘Šå¤±æ•—: {str(e)}")

    def _analyze_sync_result(self, db_info: DBInfo) -> tuple:
        """åˆ†æ sync çµæœï¼Œç²¾ç¢ºæå–å¤±æ•—é …ç›® - æ¸…ç†ç‰ˆæœ¬"""
        try:
            log_file = db_info.sync_log_path
            if not log_file:
                log_file = self._get_sync_log_file(db_info)
            
            if not log_file or not os.path.exists(log_file):
                return 0.0, []
            
            total_projects = 0
            failed_projects = set()
            
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # ğŸ”¥ å…ˆæ¸…ç† ANSI è½‰ç¾©å­—ç¬¦
                import re
                clean_content = re.sub(r'\x1b\[[0-9;]*[mK]', '', content)
                
                # æå–åŒæ­¥çµ±è¨ˆ
                sync_pattern = r'Syncing:\s*\d+%\s*\((\d+)/(\d+)\)'
                matches = re.findall(sync_pattern, clean_content)
                
                if matches:
                    last_match = matches[-1]
                    completed = int(last_match[0])
                    total_projects = int(last_match[1])
                    self.logger.debug(f"{db_info.db_info}: åŒæ­¥çµ±è¨ˆ - å®Œæˆ {completed}/{total_projects}")
                
                # ğŸ”¥ ç²¾ç¢ºæå–å¤±æ•—é …ç›® - åªå¾ç‰¹å®šéŒ¯èª¤éƒ¨åˆ†æå–
                
                # æ–¹å¼1ï¼šå¾ "The following projects failed" åˆ°ä¸‹ä¸€å€‹éŒ¯èª¤éƒ¨åˆ†
                failed_section_match = re.search(
                    r'The following projects failed[^:]*:\s*(.*?)(?=\n\s*error:|$)', 
                    clean_content, 
                    re.DOTALL | re.IGNORECASE
                )
                
                if failed_section_match:
                    failed_text = failed_section_match.group(1)
                    # æå–æœ‰æ•ˆçš„é …ç›®è·¯å¾‘
                    project_lines = [line.strip() for line in failed_text.split('\n') if line.strip()]
                    for line in project_lines:
                        # åªæ¥å—ä»¥ kernel/ é–‹é ­çš„æœ‰æ•ˆè·¯å¾‘
                        if line.startswith('kernel/') and '/' in line and len(line) > 10:
                            # æ¸…ç†å¯èƒ½çš„å°¾éƒ¨å­—ç¬¦
                            clean_project = re.sub(r'[^\w/\-_.].*$', '', line)
                            if clean_project and clean_project.count('/') >= 2:  # è‡³å°‘åŒ…å« kernel/xxx/yyy
                                failed_projects.add(clean_project)
                
                # æ–¹å¼2ï¼šå¾å…·é«”çš„ GitCommandError æ¶ˆæ¯ä¸­æå–
                git_error_pattern = r"GitCommandError:.*?on\s+(kernel/[^\s'\"]+)\s+failed"
                git_errors = re.findall(git_error_pattern, clean_content, re.IGNORECASE)
                for project in git_errors:
                    # æ¸…ç†é …ç›®è·¯å¾‘
                    clean_project = re.sub(r'[^\w/\-_.].*$', '', project)
                    if clean_project and clean_project.count('/') >= 2:
                        failed_projects.add(clean_project)
                
                # æ–¹å¼3ï¼šå¾ "fatal: not a git repository" éŒ¯èª¤æ¨æ–·é …ç›®
                repo_error_pattern = r"fatal: not a git repository.*?/([^/\s'\"]+)\.git"
                repo_errors = re.findall(repo_error_pattern, clean_content)
                
                # æ ¹æ“šé …ç›®åç¨±æ¨æ–·å®Œæ•´è·¯å¾‘
                known_project_mappings = {
                    'vts': 'kernel/android/U/test/vts',
                    'hal': 'kernel/android/U/test/vts-testcase/hal',
                    'hal-trace': 'kernel/android/U/test/vts-testcase/hal-trace'
                }
                
                for project_name in repo_errors:
                    if project_name in known_project_mappings:
                        failed_projects.add(known_project_mappings[project_name])
            
            # ğŸ”¥ æ¸…ç†å’Œé©—è­‰å¤±æ•—é …ç›®åˆ—è¡¨
            valid_failed_projects = []
            for project in failed_projects:
                # é©—è­‰é …ç›®è·¯å¾‘æ ¼å¼
                if (project.startswith('kernel/') and 
                    project.count('/') >= 2 and 
                    len(project) > 10 and
                    not any(char in project for char in ['\x1b', '"', "'", ':', ' ']) and
                    re.match(r'^kernel/[a-zA-Z0-9_\-/]+$', project)):
                    valid_failed_projects.append(project)
            
            # å»é‡ä¸¦æ’åº
            valid_failed_projects = sorted(list(set(valid_failed_projects)))
            
            # è¨ˆç®—æˆåŠŸç‡
            if total_projects > 0:
                success_count = total_projects - len(valid_failed_projects)
                success_rate = (success_count / total_projects) * 100
                
                self.logger.info(f"{db_info.db_info}: æ¸…ç†å¾Œåˆ†æ - ç¸½è¨ˆ:{total_projects}, æˆåŠŸ:{success_count}, å¤±æ•—:{len(valid_failed_projects)}, æˆåŠŸç‡:{success_rate:.1f}%")
                self.logger.info(f"{db_info.db_info}: æœ‰æ•ˆå¤±æ•—é …ç›®: {valid_failed_projects}")
                
                return success_rate, valid_failed_projects
            
            return 0.0, []
            
        except Exception as e:
            self.logger.debug(f"åˆ†æåŒæ­¥çµæœå¤±æ•—: {e}")
            return 0.0, []

    def _get_sync_log_file(self, db_info: DBInfo) -> str:
        """ç²å– sync æ—¥èªŒæ–‡ä»¶è·¯å¾‘ - å„ªå…ˆä½¿ç”¨ unbuffer ç‰ˆæœ¬"""
        try:
            log_dir = os.path.join(db_info.local_path, 'logs')
            if os.path.exists(log_dir):
                # å°‹æ‰¾æœ€æ–°çš„æ—¥èªŒæ–‡ä»¶ï¼Œå„ªå…ˆé¸æ“‡ unbuffer ç‰ˆæœ¬
                log_files = []
                for f in os.listdir(log_dir):
                    if f.startswith('repo_sync_') and f.endswith('.log'):
                        file_path = os.path.join(log_dir, f)
                        mtime = os.path.getmtime(file_path)
                        
                        # ğŸ”¥ çµ¦ä¸åŒé¡å‹çš„æ—¥èªŒä¸åŒå„ªå…ˆç´š
                        priority = 0
                        if 'unbuffer' in f:
                            priority = 100  # æœ€é«˜å„ªå…ˆç´š
                        elif 'script' in f:
                            priority = 50
                        elif 'hotfix' in f or 'fixed' in f:
                            priority = 25
                        # èˆŠç‰ˆæœ¬çš„æ—¥èªŒå„ªå…ˆç´šç‚º 0
                        
                        log_files.append((priority, mtime, file_path))
                
                if log_files:
                    # æŒ‰å„ªå…ˆç´šå’Œä¿®æ”¹æ™‚é–“æ’åº
                    log_files.sort(key=lambda x: (x[0], x[1]), reverse=True)
                    latest_log = log_files[0][2]
                    self.logger.debug(f"{db_info.db_info}: ä½¿ç”¨æ—¥èªŒæ–‡ä»¶: {os.path.basename(latest_log)}")
                    return latest_log
            
            self.logger.debug(f"{db_info.db_info}: æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨: {log_dir}")
        except Exception as e:
            self.logger.debug(f"{db_info.db_info}: ç²å–æ—¥èªŒæ–‡ä»¶å¤±æ•—: {e}")
        
        return ""

    def _wait_for_all_syncs_enhanced(self, db_results: List[DBInfo]):
        """å®Œæ•´ç‰ˆé€²åº¦ç›£æ§ - ğŸ”¥ ä¿®å¾©é‡è¤‡æ—¥èªŒå•é¡Œ"""
        max_wait_time = config_manager.repo_config['sync_timeout']
        start_wait = time.time()
        
        active_syncs = [db for db in db_results if db.sync_process and db.status != DBStatus.FAILED]
        self.logger.info(f"ç›£æ§ {len(active_syncs)} å€‹æ´»èºçš„ repo sync é€²ç¨‹")
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
        progress_tracker = {}
        for db_info in active_syncs:
            progress_tracker[db_info.db_info] = {
                'start_time': db_info.start_time or datetime.now(),
                'last_log_size': 0,
                'estimated_progress': 0,
                'current_activity': 'åˆå§‹åŒ–ä¸­...',
                'log_file': self._get_sync_log_file(db_info),
                'last_check_time': datetime.now(),
                'error_count': 0,
                'critical_errors': []  # ğŸ”¥ è¨˜éŒ„åš´é‡éŒ¯èª¤
            }
        
        check_interval = 30  # 30ç§’æª¢æŸ¥ä¸€æ¬¡
        
        while True:
            all_complete = True
            elapsed = int(time.time() - start_wait)
            current_time = time.time()
            
            print("\n" + "="*100)
            print(f"ğŸ“Š Repo Sync é€²åº¦ç›£æ§ - å·²ç­‰å¾… {elapsed}s")
            print("="*100)
            
            current_failed_count = 0
            current_running_count = 0
            current_completed_count = 0
            
            for db_info in active_syncs:
                if db_info.status == DBStatus.FAILED:
                    current_failed_count += 1
                    continue
                
                db_name = db_info.db_info
                tracker = progress_tracker[db_name]
                
                # æ§‹å»ºåŒ…å«ç‰ˆæœ¬ä¿¡æ¯çš„é¡¯ç¤ºåç¨±
                manifest_info = ""
                if db_info.manifest_file:
                    manifest_info = f" ({db_info.manifest_file})"
                elif db_info.version:
                    manifest_info = f" (v{db_info.version})"
                
                display_name = f"{db_name}{manifest_info}"
                
                if db_info.sync_process:
                    try:
                        poll = db_info.sync_process.poll()
                        
                        if poll is None:  # ä»åœ¨é‹è¡Œ
                            all_complete = False
                            current_running_count += 1
                            
                            # æ›´æ–°é€²åº¦ä¿¡æ¯
                            self._update_progress_info(db_info, tracker)
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            progress = tracker['estimated_progress']
                            bar_length = 20
                            filled = int(bar_length * progress / 100)
                            bar = "â–ˆ" * filled + "â–’" * (bar_length - filled)
                            
                            activity = tracker.get('current_project', '').split('/')[-1] or tracker.get('current_activity', 'åŒæ­¥ä¸­')
                            if len(activity) > 15:
                                activity = activity[:12] + "..."
                            
                            # ğŸ”¥ é¡¯ç¤ºéŒ¯èª¤ç‹€æ…‹
                            status_info = ""
                            if tracker['critical_errors']:
                                status_info = f" âš ï¸{len(tracker['critical_errors'])}"
                            
                            print(f"ğŸ”„ {display_name:30s} â”‚{bar}â”‚ {progress:3d}% â”‚ {runtime_str} â”‚ {activity}{status_info}")
                            
                            # æª¢æŸ¥è¶…æ™‚
                            if time.time() - start_wait > max_wait_time:
                                self.logger.warning(f"{db_name}: repo sync è¶…æ™‚ï¼Œå¼·åˆ¶çµ‚æ­¢")
                                try:
                                    db_info.sync_process.terminate()
                                    db_info.sync_process.wait(timeout=5)
                                except:
                                    try:
                                        db_info.sync_process.kill()
                                    except:
                                        pass
                                db_info.status = DBStatus.FAILED
                                db_info.error_message = "Sync è¶…æ™‚"
                                current_failed_count += 1
                                print(f"â° {display_name:30s} â”‚ è¶…æ™‚çµ‚æ­¢")
                                
                        elif poll == 0:  # æˆåŠŸå®Œæˆ
                            current_completed_count += 1
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            bar = "â–ˆ" * 20
                            print(f"âœ… {display_name:30s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ å®Œæˆ+å·²å°å‡º")
                            
                        else:  # å¤±æ•— (poll != 0)
                            current_failed_count += 1
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            # ğŸ”¥ åˆ†æå¤±æ•—åŸå› 
                            error_msg = f"Sync å¤±æ•— (è¿”å›ç¢¼: {poll})"
                            
                            # æª¢æŸ¥æ˜¯å¦ç‚ºéƒ¨åˆ†å¤±æ•—
                            if poll == 1:  # éƒ¨åˆ†å¤±æ•—ï¼Œå¯èƒ½å¯ä»¥æ•‘æ´»
                                success_rate, failed_projects = self._analyze_sync_result(db_info)
                                if failed_projects:
                                    error_msg += f" - {len(failed_projects)} å€‹é …ç›®å¤±æ•—"
                            
                            db_info.status = DBStatus.FAILED
                            db_info.error_message = error_msg
                            print(f"âŒ {display_name:30s} â”‚{'':20s}â”‚   0% â”‚ {runtime_str} â”‚ {error_msg[:30]}")
                            
                    except Exception as e:
                        self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹å¤±æ•—: {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = f"é€²ç¨‹ç›£æ§å¤±æ•—: {e}"
                        current_failed_count += 1
                        print(f"âš ï¸  {display_name:30s} â”‚ ç›£æ§éŒ¯èª¤ â”‚   0% â”‚ {str(e)[:30]}")
            
            # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
            print("-"*100)
            print(f"ğŸ“ˆ ç¸½è¨ˆ: é‹è¡Œä¸­ {current_running_count} | å®Œæˆ {current_completed_count} | å¤±æ•— {current_failed_count}")
            
            if all_complete or (time.time() - start_wait) > max_wait_time:
                break
            
            # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
            time.sleep(check_interval)
        
        # æœ€çµ‚çµ±è¨ˆ
        completed = sum(1 for db in active_syncs if db.sync_process and db.sync_process.poll() == 0)
        failed = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        
        print(f"\nğŸ“‹ Repo sync æœ€çµ‚çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸ: {completed}")
        print(f"   âŒ å¤±æ•—: {failed}")
        
        self.logger.info(f"ğŸ“‹ Repo sync å®Œæˆçµ±è¨ˆ: æˆåŠŸ {completed}, å¤±æ•— {failed}")

    def _update_progress_info(self, db_info: DBInfo, tracker: dict):
        """æ›´æ–°é€²åº¦ä¿¡æ¯ - å°ˆé–€å„ªåŒ– unbuffer è¼¸å‡ºè§£æ"""
        try:
            log_file = tracker.get('log_file')
            
            # ğŸ”¥ æ¯æ¬¡éƒ½é‡æ–°ç²å–æ—¥èªŒæ–‡ä»¶ï¼Œç¢ºä¿ä½¿ç”¨æœ€æ–°çš„ unbuffer æ—¥èªŒ
            current_log_file = self._get_sync_log_file(db_info)
            if current_log_file and current_log_file != log_file:
                tracker['log_file'] = current_log_file
                log_file = current_log_file
                self.logger.debug(f"{db_info.db_info}: åˆ‡æ›åˆ°æ–°æ—¥èªŒæ–‡ä»¶: {os.path.basename(log_file)}")
            
            if not log_file or not os.path.exists(log_file):
                tracker['current_activity'] = 'ç­‰å¾…æ—¥èªŒ...'
                tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                return
            
            # ğŸ”¥ å„ªåŒ–çš„æ—¥èªŒè§£æ - å°ˆé–€è™•ç† unbuffer æ ¼å¼
            try:
                file_size = os.path.getsize(log_file)
                
                # åªè®€å–æœ€å¾Œ 2KB é¿å…è™•ç†å¤§æ–‡ä»¶
                read_size = min(file_size, 2048)
                
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(max(0, file_size - read_size))
                    content = f.read()
                    lines = content.split('\n')[-15:]  # æœ€å¾Œ15è¡Œ
                
                # è§£ææœ€æ–°çš„åŒæ­¥ç‹€æ…‹
                latest_progress = 0
                latest_activity = "åŒæ­¥ä¸­..."
                current_project = ""
                total_projects = 0
                current_count = 0
                
                # ğŸ”¥ é‡é»ï¼šè§£æ unbuffer è¼¸å‡ºçš„ç‰¹å®šæ ¼å¼
                for line in reversed(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£æ "Syncing: X% (current/total) time | jobs | project" æ ¼å¼
                    if "Syncing:" in line and "%" in line:
                        import re
                        
                        # åŒ¹é…é€²åº¦ç™¾åˆ†æ¯”å’Œè¨ˆæ•¸
                        progress_match = re.search(r'Syncing:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                        if progress_match:
                            latest_progress = int(progress_match.group(1))
                            current_count = int(progress_match.group(2))
                            total_projects = int(progress_match.group(3))
                            
                            # æå–ç•¶å‰è™•ç†çš„é …ç›®
                            # æ ¼å¼é€šå¸¸æ˜¯: "... | X jobs | time project_path @ ..."
                            if '|' in line:
                                parts = line.split('|')
                                for part in parts:
                                    part = part.strip()
                                    # å°‹æ‰¾åŒ…å«é …ç›®è·¯å¾‘çš„éƒ¨åˆ†
                                    if '@' in part:
                                        project_info = part.split('@')[-1].strip()
                                    elif '/' in part and 'job' not in part and ':' not in part:
                                        project_info = part
                                    else:
                                        continue
                                    
                                    # æå–é …ç›®åç¨±
                                    if '/' in project_info:
                                        current_project = project_info.split('/')[-1]
                                    else:
                                        current_project = project_info
                                    break
                            
                            # æ§‹å»ºæ´»å‹•æè¿°
                            latest_activity = f"åŒæ­¥: {current_count}/{total_projects}"
                            if current_project:
                                # é™åˆ¶é …ç›®åç¨±é•·åº¦
                                project_name = current_project[:20] + "..." if len(current_project) > 20 else current_project
                                latest_activity += f" - {project_name}"
                            
                            break
                    
                    # ğŸ”¥ è§£æå…¶ä»–ç‹€æ…‹ä¿¡æ¯
                    elif "Fetching project" in line:
                        project_match = re.search(r'Fetching project\s+([^\s]+)', line)
                        if project_match:
                            project_path = project_match.group(1)
                            current_project = project_path.split('/')[-1]
                            latest_activity = f"ç²å–: {current_project}"
                    
                    elif "Skipped fetching project" in line:
                        project_match = re.search(r'Skipped fetching project\s+([^\s]+)', line)
                        if project_match:
                            project_path = project_match.group(1)
                            current_project = project_path.split('/')[-1]
                            latest_activity = f"è·³é: {current_project}"
                
                # ğŸ”¥ æ›´æ–°è¿½è¹¤ä¿¡æ¯
                if latest_progress > 0:
                    tracker['estimated_progress'] = latest_progress
                else:
                    # å¦‚æœæ²’æœ‰è§£æåˆ°é€²åº¦ï¼Œä½¿ç”¨æ™‚é–“ä¼°ç®—
                    tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                
                tracker['current_activity'] = latest_activity
                tracker['current_project'] = current_project
                tracker['total_projects'] = total_projects
                tracker['current_count'] = current_count
                tracker['last_update'] = datetime.now()
                
                # ğŸ”¥ èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
                if latest_progress > 0:
                    self.logger.debug(f"{db_info.db_info}: è§£ææˆåŠŸ - {latest_progress}% ({current_count}/{total_projects}) {current_project}")
                    
            except Exception as e:
                self.logger.debug(f"{db_info.db_info}: è§£ææ—¥èªŒå¤±æ•—: {e}")
                tracker['current_activity'] = 'è§£æå¤±æ•—'
                tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                
        except Exception as e:
            self.logger.debug(f"{db_info.db_info}: é€²åº¦æ›´æ–°å¤±æ•—: {e}")
            tracker['current_activity'] = 'æ›´æ–°å¤±æ•—'
            tracker['estimated_progress'] = self._get_time_based_progress(tracker)

    def _get_time_based_progress(self, tracker: dict) -> int:
        """åŸºæ–¼æ™‚é–“çš„é€²åº¦ä¼°ç®—"""
        runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
        # æ¯åˆ†é˜ç´„1.5%çš„é€²åº¦ï¼Œæœ€å¤š95%
        return min(int(runtime_minutes * 1.5), 95)

    def process_dbs_async(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """ç•°æ­¥è™•ç†å¤šå€‹ DB"""
        db_versions = db_versions or {}
        db_infos = []
        
        # æº–å‚™ DB è³‡è¨Š
        all_db_infos = self.get_all_dbs('all')
        
        for db_name in db_list:
            if '#' in db_name:
                db_name, version = db_name.split('#', 1)
            else:
                version = db_versions.get(db_name)
            
            for db_info in all_db_infos:
                if db_info.db_info == db_name:
                    db_info.version = version
                    db_infos.append(db_info)
                    break
        
        if not db_infos:
            self.logger.error("æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
            return

        self.logger.info(f"é–‹å§‹è™•ç† {len(db_infos)} å€‹ DB")
        
        try:
            # Phase 1: æº–å‚™å’Œå•Ÿå‹• sync
            self.logger.info("åŸ·è¡Œ Phase 1: æº–å‚™å·¥ä½œå’Œå•Ÿå‹•åŒæ­¥")
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase1, db_info): db_info for db_info in db_infos}
                
                phase1_results = []
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=300)
                        phase1_results.append(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 1 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        db_info.end_time = datetime.now()
                        phase1_results.append(db_info)
            
            # ğŸ”¥ ä¿®å¾©ï¼šä½¿ç”¨è‡ªå®šç¾©çš„ç­‰å¾…å’Œè™•ç†é‚è¼¯ï¼Œé¿å…é‡è¤‡èª¿ç”¨ Phase 2
            if not self.dry_run:
                self.logger.info("ç­‰å¾…æ‰€æœ‰ repo sync å®Œæˆä¸¦å³æ™‚è™•ç†...")
                self._wait_and_process_syncs_enhanced(phase1_results)
            else:
                # æ¸¬è©¦æ¨¡å¼ç›´æ¥è™•ç†
                for db_info in phase1_results:
                    self.report.add_db(db_info)
            
            self.logger.info("æ‰€æœ‰ DB è™•ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

    def process_selected_dbs(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """è™•ç†é¸å®šçš„ DB"""
        if config_manager.repo_config['async_sync'] and not self.dry_run:
            self.process_dbs_async(db_list, db_versions)
        else:
            # åŒæ­¥è™•ç†é‚è¼¯
            for db_name in db_list:
                if '#' in db_name:
                    db_name, version = db_name.split('#', 1)
                else:
                    version = db_versions.get(db_name) if db_versions else None
                
                for db_info in self.get_all_dbs('all'):
                    if db_info.db_info == db_name:
                        db_info.version = version
                        
                        db_info = self.process_db_phase1(db_info)
                        
                        if not self.dry_run and db_info.sync_process:
                            self.logger.info(f"ç­‰å¾… {db_name} sync å®Œæˆ...")
                            db_info.sync_process.wait()
                        
                        db_info = self.process_db_phase2(db_info)
                        self.report.add_db(db_info)
                        break

    def generate_report(self, output_file: str = None):
        """ç”¢ç”Ÿæ”¹é€²ç‰ˆå ±å‘Š - ğŸ”¥ ä¿®å¾©é‡è¤‡å•é¡Œ"""
        self.report.finalize()
        
        if not output_file:
            output_file = os.path.join(
                self.output_dir, 
                config_manager.path_config['report_filename']
            )
        
        try:
            self.logger.info("é–‹å§‹ç”¢ç”Ÿ Excel å ±å‘Š")
            
            # ğŸ”¥ åœ¨ç”Ÿæˆå ±å‘Šå‰å…ˆæ¸…ç†é‡è¤‡
            self._remove_duplicates()
            
            report_data = []
            seen_combinations = set()
            
            for i, db in enumerate(self.report.db_details, 1):
                # ğŸ”¥ å†æ¬¡ç¢ºä¿ä¸é‡è¤‡
                key = (db.db_info, db.db_type, db.module)
                if key in seen_combinations:
                    self.logger.warning(f"è·³éé‡è¤‡è¨˜éŒ„: {db.db_info} ({db.db_type}, {db.module})")
                    continue
                
                seen_combinations.add(key)
                
                # ç¢ºä¿ status æœ‰å€¼ä¸¦è½‰æ›ç‚ºå­—ç¬¦ä¸²
                status_value = db.status
                if hasattr(status_value, 'value'):
                    status_str = status_value.value
                elif isinstance(status_value, str):
                    status_str = status_value
                else:
                    status_str = str(status_value)
                
                # æ‰‹å‹•æ§‹å»ºå­—å…¸ï¼Œç¢ºä¿æ‰€æœ‰æ¬„ä½æ­£ç¢º
                db_dict = {
                    'sn': len(report_data) + 1,
                    'module': db.module or '',
                    'db_type': db.db_type or '',
                    'db_info': db.db_info or '',
                    'status': status_str,
                    'version': db.version or 'æœªæŒ‡å®š',
                    'manifest_file': db.manifest_file or '',
                    'start_time': db.start_time.strftime('%Y-%m-%d %H:%M:%S') if db.start_time else '',
                    'end_time': db.end_time.strftime('%Y-%m-%d %H:%M:%S') if db.end_time else '',
                    'error_message': db.error_message or '',
                    'sync_log_path': db.sync_log_path or '',
                    'sftp_path': db.sftp_path or '',
                    'local_path': db.local_path or '',
                    'has_existing_repo': 'æ˜¯' if db.has_existing_repo else 'å¦',
                    # ğŸš« ç§»é™¤é€™è¡Œï¼š'jira_link': db.jira_link or 'æœªæ‰¾åˆ°',
                    # manifest æ¯”è¼ƒç›¸é—œæ¬„ä½
                    'manifest_æ¯”è¼ƒçµæœ': getattr(db, 'manifest_comparison_summary', '') or '',
                    'manifest_æ˜¯å¦ç›¸åŒ': 'æ˜¯' if getattr(db, 'manifest_is_identical', None) else 'å¦' if getattr(db, 'manifest_is_identical', None) is False else '',
                    'manifest_å·®ç•°æ•¸é‡': getattr(db, 'manifest_differences_count', 0) or 0,
                    'exported_manifest_path': getattr(db, 'exported_manifest_path', '') or '',
                }
                
                # é‡æ–°å‘½åæ¬„ä½
                db_dict['å®Œæ•´_JIRA_é€£çµ'] = db.jira_link or 'æœªæ‰¾åˆ°'  # âœ… ä¿ç•™é€™å€‹
                db_dict['å®Œæ•´_repo_init_æŒ‡ä»¤'] = getattr(db, 'actual_source_cmd', '') or 'æœªè¨˜éŒ„'
                
                report_data.append(db_dict)
            
            self.logger.info(f"å ±å‘Šè³‡æ–™ï¼šå»é‡è¤‡å¾Œå…± {len(report_data)} ç­†è¨˜éŒ„")
            
            if not report_data:
                self.logger.warning("æ²’æœ‰è³‡æ–™å¯ä»¥ç”¢ç”Ÿå ±å‘Š")
                return
            
            # ... å…¶é¤˜ä»£ç¢¼ä¿æŒä¸è®Š
            
        except Exception as e:
            self.logger.error(f"ç”¢ç”Ÿå ±å‘Šå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

    def _write_enhanced_excel(self, main_df: pd.DataFrame, summary_df: pd.DataFrame, output_file: str):
        """å¯«å…¥æ”¹é€²ç‰ˆ Excelï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰"""
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # å¯«å…¥ä¸»è¦è³‡æ–™
                main_df.to_excel(writer, sheet_name='è©³ç´°è³‡è¨Š', index=False)
                summary_df.to_excel(writer, sheet_name='æ‘˜è¦', index=False)
                
                # æ ¼å¼åŒ–å·¥ä½œè¡¨
                for sheet_name in ['è©³ç´°è³‡è¨Š', 'æ‘˜è¦']:
                    worksheet = writer.sheets[sheet_name]
                    self._format_worksheet_enhanced(worksheet)
            
            self.logger.info(f"æˆåŠŸå¯«å…¥å¢å¼·ç‰ˆ Excel æª”æ¡ˆ: {output_file}")
            
        except Exception as e:
            self.logger.error(f"å¯«å…¥ Excel æª”æ¡ˆå¤±æ•—: {str(e)}")
            raise

    def _format_worksheet_enhanced(self, worksheet):
        """æ ¼å¼åŒ–å·¥ä½œè¡¨ï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰"""
        try:
            from openpyxl.styles import Font, PatternFill, Alignment
            from openpyxl.utils import get_column_letter
            
            # è¨­å®šè¡¨é ­æ ¼å¼ï¼ˆè—åº•ç™½å­—ï¼‰
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_font = Font(color="FFFFFF", bold=True)
            header_alignment = Alignment(horizontal="center", vertical="center")
            
            # æ‡‰ç”¨åˆ°ç¬¬ä¸€è¡Œï¼ˆè¡¨é ­ï¼‰
            for cell in worksheet[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = header_alignment
            
            # è‡ªå‹•èª¿æ•´æ¬„å¯¬
            for column in worksheet.columns:
                max_length = 0
                column_letter = get_column_letter(column[0].column)
                
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                
                # è¨­å®šæœ€å°å¯¬åº¦å’Œæœ€å¤§å¯¬åº¦
                adjusted_width = min(max(max_length + 2, 10), 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
            
            # è¨­å®šè¡Œé«˜
            for row in worksheet.iter_rows():
                worksheet.row_dimensions[row[0].row].height = 20
            
            # å‡çµç¬¬ä¸€è¡Œ
            worksheet.freeze_panes = 'A2'
            
        except Exception as e:
            self.logger.warning(f"æ ¼å¼åŒ–å·¥ä½œè¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# =====================================
# ===== äº’å‹•å¼ä»‹é¢ =====
# =====================================

class InteractiveUI:
    """äº’å‹•å¼ä½¿ç”¨è€…ä»‹é¢"""
    
    def __init__(self):
        self.tool = ManifestPinningTool()
        self.logger = setup_logger(self.__class__.__name__)
        self.selected_dbs = []
        self.db_versions = {}
        self.selected_db_type = 'all'
        
        self._load_default_config()
    
    def _load_default_config(self):
        """è¼‰å…¥é è¨­é…ç½®"""
        if config_manager.default_execution_config.get('sftp_override'):
            config_manager.apply_overrides(
                config_manager.default_execution_config['sftp_override'],
                source='default_config'
            )
        
        if config_manager.default_execution_config.get('output_dir'):
            self.tool.output_dir = config_manager.default_execution_config['output_dir']
    
    def setup_sftp(self):
        """è¨­å®š SFTP é€£ç·šè³‡è¨Š"""
        print("\nç›®å‰ SFTP è¨­å®š:")
        print(f"  Host: {config_manager.sftp_config['host']}")
        print(f"  Port: {config_manager.sftp_config['port']}")
        print(f"  Username: {config_manager.sftp_config['username']}")
        
        if input("\næ˜¯å¦è¦ä¿®æ”¹è¨­å®š? (y/N): ").strip().lower() == 'y':
            host = input(f"Host [{config_manager.sftp_config['host']}]: ").strip()
            if host:
                config_manager.sftp_config['host'] = host
                
            port = input(f"Port [{config_manager.sftp_config['port']}]: ").strip()
            if port:
                config_manager.sftp_config['port'] = int(port)
                
            username = input(f"Username [{config_manager.sftp_config['username']}]: ").strip()
            if username:
                config_manager.sftp_config['username'] = username
                
            password = input("Password (ç•™ç©ºä¿æŒåŸå€¼): ").strip()
            if password:
                config_manager.sftp_config['password'] = password
            
            print("âœ… SFTP è¨­å®šå·²æ›´æ–°")
    
    def display_current_settings(self):
        """é¡¯ç¤ºç›®å‰è¨­å®š"""
        print("\nç›®å‰è¨­å®š:")
        print(f"  Mapping table: {'å·²è¼‰å…¥' if self.tool.mapping_reader.df is not None else 'æœªè¼‰å…¥'}")
        print(f"  DB é¡å‹: {self.selected_db_type}")
        print(f"  é¸æ“‡çš„ DB: {len(self.selected_dbs)} å€‹")
        print(f"  è¨­å®šç‰ˆæœ¬çš„ DB: {len(self.db_versions)} å€‹")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        print(f"  å¹³è¡Œè™•ç†: {'å•Ÿç”¨' if config_manager.parallel_config['enable_parallel'] else 'é—œé–‰'}")
        print(f"  Worker æ•¸é‡: {config_manager.parallel_config['max_workers']}")
    
    def load_mapping_table(self):
        """è¼‰å…¥ mapping table"""
        default_mapping = config_manager.default_execution_config.get('mapping_table')
        
        if default_mapping and os.path.exists(default_mapping):
            print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ mapping table: {default_mapping}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­æª”æ¡ˆ? (Y/n): ").strip().lower() != 'n':
                file_path = default_mapping
            else:
                file_path = input(
                    f"è«‹è¼¸å…¥ mapping table è·¯å¾‘ [{config_manager.path_config['default_mapping_table']}]: "
                ).strip()
                if not file_path:
                    file_path = config_manager.path_config['default_mapping_table']
        else:
            default_path = config_manager.path_config['default_mapping_table']
            file_path = input(f"è«‹è¼¸å…¥ mapping table è·¯å¾‘ [{default_path}]: ").strip()
            if not file_path:
                file_path = default_path
        
        if not os.path.exists(file_path):
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            return
        
        if self.tool.load_mapping_table(file_path):
            print(f"âœ… æˆåŠŸè¼‰å…¥ mapping tableï¼Œå…± {len(self.tool.mapping_reader.df)} ç­†è³‡æ–™")
            all_dbs = self.tool.get_all_dbs()
            unique_db_names = list(set([db.db_info for db in all_dbs]))
            print(f"   æ‰¾åˆ° {len(unique_db_names)} å€‹ä¸é‡è¤‡çš„ DB")
            
            if config_manager.default_execution_config.get('db_type'):
                self.selected_db_type = config_manager.default_execution_config['db_type']
                print(f"   ğŸ“Œ ä½¿ç”¨é è¨­ DB é¡å‹: {self.selected_db_type}")
        else:
            print("âŒ è¼‰å…¥å¤±æ•—")
    
    def select_db_type(self):
        """é¸æ“‡ DB é¡å‹"""
        default_type = config_manager.default_execution_config.get('db_type')
        
        if default_type and default_type in ['all', 'master', 'premp', 'mp', 'mpbackup']:
            print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ DB é¡å‹: {default_type}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­å€¼? (Y/n): ").strip().lower() != 'n':
                self.selected_db_type = default_type
                print(f"âœ… å·²é¸æ“‡ DB é¡å‹: {self.selected_db_type}")
                return self.selected_db_type
        
        print("\né¸æ“‡ DB é¡å‹:")
        print("1. All (æ‰€æœ‰é¡å‹)")
        print("2. Master")
        print("3. PreMP")
        print("4. MP")
        print("5. MP Backup")
        
        choice = input("è«‹é¸æ“‡: ").strip()
        
        type_map = {
            '1': 'all',
            '2': 'master',
            '3': 'premp',
            '4': 'mp',
            '5': 'mpbackup'
        }
        
        self.selected_db_type = type_map.get(choice, 'all')
        print(f"âœ… å·²é¸æ“‡ DB é¡å‹: {self.selected_db_type}")
        return self.selected_db_type
    
    def select_dbs(self):
        """é¸æ“‡è¦å®šç‰ˆçš„ DB"""
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return []
        
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        if not all_db_infos:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {self.selected_db_type} é¡å‹çš„ DB")
            return []
        
        # å–å¾—ä¸é‡è¤‡çš„ DB åˆ—è¡¨
        unique_dbs = {}
        for db_info in all_db_infos:
            if db_info.db_info not in unique_dbs:
                unique_dbs[db_info.db_info] = db_info
        
        db_list = list(unique_dbs.keys())
        db_list.sort()
        
        # æª¢æŸ¥é è¨­ DB åˆ—è¡¨
        default_dbs = config_manager.default_execution_config.get('selected_dbs')
        
        if default_dbs:
            if default_dbs in ['all', '*']:
                print(f"\nğŸ“Œ é è¨­é…ç½®ç‚ºé¸æ“‡æ‰€æœ‰ {self.selected_db_type} é¡å‹çš„ DB")
                if input(f"æ˜¯å¦é¸æ“‡å…¨éƒ¨ {len(db_list)} å€‹ DB? (Y/n): ").strip().lower() != 'n':
                    self.selected_dbs = db_list
                    print(f"âœ… å·²é¸æ“‡æ‰€æœ‰ {len(db_list)} å€‹ DB")
                    return db_list
            
            elif isinstance(default_dbs, list) and len(default_dbs) > 0:
                parsed_dbs = []
                for db_spec in default_dbs:
                    if '#' in db_spec:
                        db_name, version = db_spec.split('#', 1)
                        if db_name in db_list:
                            parsed_dbs.append(db_name)
                            self.db_versions[db_name] = version
                    else:
                        if db_spec in db_list:
                            parsed_dbs.append(db_spec)
                
                if parsed_dbs:
                    print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ DB åˆ—è¡¨: {', '.join(default_dbs)}")
                    print(f"   å…¶ä¸­ {len(parsed_dbs)} å€‹ DB å­˜åœ¨æ–¼ç•¶å‰ mapping table")
                    if input("æ˜¯å¦ä½¿ç”¨é è¨­ DB åˆ—è¡¨? (Y/n): ").strip().lower() != 'n':
                        self.selected_dbs = parsed_dbs
                        print(f"âœ… å·²é¸æ“‡ {len(parsed_dbs)} å€‹ DB")
                        return parsed_dbs
        
        # é¡¯ç¤º DB åˆ—è¡¨å’Œé¸æ“‡é‚è¼¯
        print(f"\næ‰¾åˆ° {len(db_list)} å€‹ä¸é‡è¤‡çš„ DB (é¡å‹: {self.selected_db_type})")
        
        print("\nDB åˆ—è¡¨:")
        for i, db in enumerate(db_list, 1):
            db_info = unique_dbs[db]
            print(f"{i:3d}. {db:10s} - {db_info.module:10s} ({db_info.db_type})")
        
        print("\né¸æ“‡æ–¹å¼:")
        print("1. å…¨é¸")
        print("2. è¼¸å…¥ç·¨è™Ÿç¯„åœ (å¦‚: 1-5,7,9-12)")
        print("3. è¼¸å…¥ DB åç¨±åˆ—è¡¨ (é€—è™Ÿåˆ†éš”)")
        
        choice = input("è«‹é¸æ“‡: ").strip()
        
        selected = []
        
        if choice == '1':
            selected = db_list
        elif choice == '2':
            indices_input = input("è«‹è¼¸å…¥ç·¨è™Ÿç¯„åœ: ").strip()
            try:
                indices = self._parse_number_range(indices_input, len(db_list))
                selected = [db_list[i-1] for i in indices if 1 <= i <= len(db_list)]
            except Exception as e:
                print(f"âŒ ç„¡æ•ˆçš„ç·¨è™Ÿç¯„åœ: {e}")
                return []
        elif choice == '3':
            db_names = input("è«‹è¼¸å…¥ DB åç¨± (å¦‚: DB2302,DB2575): ").strip()
            input_dbs = [db.strip() for db in db_names.split(',')]
            for db in input_dbs:
                if db in db_list:
                    selected.append(db)
                else:
                    print(f"âš ï¸ DB {db} ä¸å­˜åœ¨ï¼Œå·²è·³é")
        
        self.selected_dbs = selected
        print(f"âœ… å·²é¸æ“‡ {len(selected)} å€‹ DB")
        return selected
    
    def _parse_number_range(self, range_str: str, max_num: int) -> List[int]:
        """è§£ææ•¸å­—ç¯„åœå­—ä¸²"""
        result = []
        parts = range_str.split(',')
        
        for part in parts:
            part = part.strip()
            if '-' in part:
                start, end = part.split('-', 1)
                start = int(start.strip())
                end = int(end.strip())
                
                if start > end:
                    start, end = end, start
                
                for i in range(start, min(end + 1, max_num + 1)):
                    if i > 0:
                        result.append(i)
            else:
                num = int(part)
                if 1 <= num <= max_num:
                    result.append(num)
        
        return sorted(list(set(result)))
    
    def setup_db_versions(self):
        """è¨­å®š DB ç‰ˆæœ¬"""
        if not self.selected_dbs:
            print("âŒ è«‹å…ˆé¸æ“‡ DB")
            return
        
        default_versions = config_manager.default_execution_config.get('db_versions', {})
        
        if default_versions:
            applicable_versions = {
                db: ver for db, ver in default_versions.items() 
                if db in self.selected_dbs
            }
            
            if applicable_versions:
                print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ç‰ˆæœ¬è¨­å®š:")
                for db, ver in applicable_versions.items():
                    print(f"   {db}: ç‰ˆæœ¬ {ver}")
                
                if input("æ˜¯å¦ä½¿ç”¨é è¨­ç‰ˆæœ¬è¨­å®š? (Y/n): ").strip().lower() != 'n':
                    self.db_versions.update(applicable_versions)
                    print(f"âœ… å·²å¥—ç”¨ {len(applicable_versions)} å€‹ DB çš„é è¨­ç‰ˆæœ¬")
                    
                    unset_dbs = [db for db in self.selected_dbs if db not in self.db_versions]
                    if unset_dbs:
                        print(f"\né‚„æœ‰ {len(unset_dbs)} å€‹ DB æœªè¨­å®šç‰ˆæœ¬:")
                        for db in unset_dbs:
                            version = input(f"{db} çš„ç‰ˆæœ¬ [æœ€æ–°]: ").strip()
                            if version:
                                self.db_versions[db] = version
                    return
        
        print("\nè¨­å®š DB ç‰ˆæœ¬ (ç•™ç©ºä½¿ç”¨æœ€æ–°ç‰ˆæœ¬)")
        for db in self.selected_dbs:
            if db in self.db_versions:
                print(f"{db}: å·²è¨­å®šç‰ˆæœ¬ {self.db_versions[db]}")
                continue
            
            version = input(f"{db} çš„ç‰ˆæœ¬ [æœ€æ–°]: ").strip()
            if version:
                self.db_versions[db] = version
        
        if self.db_versions:
            print(f"âœ… å·²è¨­å®š {len(self.db_versions)} å€‹ DB çš„ç‰ˆæœ¬")
    
    def execute_pinning(self):
        """åŸ·è¡Œå®šç‰ˆ"""
        if not self.selected_dbs:
            print("âŒ è«‹å…ˆé¸æ“‡è¦å®šç‰ˆçš„ DB")
            return
        
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return
        
        print("\n" + "="*60)
        print("æº–å‚™åŸ·è¡Œå®šç‰ˆ")
        print("="*60)
        print(f"ğŸ“Œ DB æ•¸é‡: {len(self.selected_dbs)}")
        print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # è©¢å•è¼¸å‡ºç›®éŒ„
        default_output = config_manager.default_execution_config.get('output_dir')
        if default_output:
            print(f"ğŸ“Œ æ‰¾åˆ°é è¨­è¼¸å‡ºç›®éŒ„: {default_output}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­è¼¸å‡ºç›®éŒ„? (Y/n): ").strip().lower() != 'n':
                self.tool.output_dir = default_output
            else:
                output_dir = input(f"è¼¸å‡ºç›®éŒ„ [{self.tool.output_dir}]: ").strip()
                if output_dir:
                    self.tool.output_dir = output_dir
        else:
            output_dir = input(f"è¼¸å‡ºç›®éŒ„ [{self.tool.output_dir}]: ").strip()
            if output_dir:
                self.tool.output_dir = output_dir
        
        # ç¢ºèªåŸ·è¡Œ
        if config_manager.default_execution_config.get('auto_confirm'):
            print("ğŸ“Œ è‡ªå‹•ç¢ºèªåŸ·è¡Œï¼ˆæ ¹æ“šé è¨­é…ç½®ï¼‰")
        else:
            if input("\nç¢ºèªé–‹å§‹åŸ·è¡Œ? (Y/n): ").strip().lower() == 'n':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                return
        
        print("\nğŸš€ é–‹å§‹åŸ·è¡Œå®šç‰ˆ...")
        
        print("ğŸŒ æº–å‚™ SFTP é€£ç·šï¼ˆæ¯å€‹ DB ä½¿ç”¨ç¨ç«‹é€£ç·šï¼‰...")
        
        try:
            # åŸ·è¡Œå®šç‰ˆ
            self.tool.process_selected_dbs(self.selected_dbs, self.db_versions)
            
            # ç”¢ç”Ÿå ±å‘Š
            print("\nğŸ“Š ç”¢ç”Ÿå ±å‘Š...")
            report_path = os.path.join(
                self.tool.output_dir, 
                config_manager.path_config['report_filename']
            )
            self.tool.generate_report(report_path)
            
            print("\nâœ¨ å®šç‰ˆå®Œæˆï¼")
            
        finally:
            resource_manager.cleanup_all()
    
    def quick_execute_with_defaults(self):
        """ä½¿ç”¨é è¨­é…ç½®å¿«é€ŸåŸ·è¡Œ"""
        print("\n" + "="*60)
        print("å¿«é€ŸåŸ·è¡Œæ¨¡å¼ - ä½¿ç”¨é è¨­é…ç½®")
        print("="*60)
        
        # è‡ªå‹•è¼‰å…¥ mapping table
        if config_manager.default_execution_config.get('mapping_table'):
            file_path = config_manager.default_execution_config['mapping_table']
            if os.path.exists(file_path):
                print(f"ğŸ“Œ è¼‰å…¥é è¨­ mapping table: {file_path}")
                if self.tool.load_mapping_table(file_path):
                    print(f"âœ… æˆåŠŸè¼‰å…¥")
                else:
                    print("âŒ è¼‰å…¥å¤±æ•—")
                    return
            else:
                print(f"âŒ é è¨­ mapping table ä¸å­˜åœ¨: {file_path}")
                return
        else:
            print("âŒ æœªè¨­å®šé è¨­ mapping table")
            return
        
        # è¨­å®š DB é¡å‹
        if config_manager.default_execution_config.get('db_type'):
            self.selected_db_type = config_manager.default_execution_config['db_type']
            print(f"ğŸ“Œ ä½¿ç”¨é è¨­ DB é¡å‹: {self.selected_db_type}")
        
        # é¸æ“‡ DB
        default_dbs = config_manager.default_execution_config.get('selected_dbs')
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        
        if default_dbs:
            if default_dbs in ['all', '*']:
                unique_dbs = list(set([db.db_info for db in all_db_infos]))
                self.selected_dbs = unique_dbs
                print(f"ğŸ“Œ é¸æ“‡æ‰€æœ‰ {self.selected_db_type} é¡å‹çš„ DB: {len(unique_dbs)} å€‹")
            
            elif isinstance(default_dbs, list) and len(default_dbs) > 0:
                parsed_dbs = []
                for db_spec in default_dbs:
                    if '#' in db_spec:
                        db_name, version = db_spec.split('#', 1)
                        parsed_dbs.append(db_name)
                        self.db_versions[db_name] = version
                    else:
                        parsed_dbs.append(db_spec)
                self.selected_dbs = parsed_dbs
                print(f"ğŸ“Œ ä½¿ç”¨é è¨­ DB åˆ—è¡¨: {len(parsed_dbs)} å€‹")
        else:
            print("âš ï¸ é è¨­é…ç½®æœªæŒ‡å®š DB åˆ—è¡¨ï¼Œç„¡æ³•è‡ªå‹•åŸ·è¡Œ")
            return
        
        # è¨­å®šç‰ˆæœ¬
        if config_manager.default_execution_config.get('db_versions'):
            self.db_versions.update(config_manager.default_execution_config['db_versions'])
            print(f"ğŸ“Œ å¥—ç”¨é è¨­ç‰ˆæœ¬è¨­å®š: {len(self.db_versions)} å€‹")
        
        # è¨­å®šè¼¸å‡ºç›®éŒ„
        if config_manager.default_execution_config.get('output_dir'):
            self.tool.output_dir = config_manager.default_execution_config['output_dir']
            print(f"ğŸ“Œ ä½¿ç”¨é è¨­è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # é¡¯ç¤ºæ‘˜è¦
        print("\nåŸ·è¡Œæ‘˜è¦:")
        print(f"  DB é¡å‹: {self.selected_db_type}")
        print(f"  DB æ•¸é‡: {len(self.selected_dbs)}")
        print(f"  è¨­å®šç‰ˆæœ¬: {len(self.db_versions)} å€‹")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # ç¢ºèªåŸ·è¡Œ
        if not config_manager.default_execution_config.get('auto_confirm'):
            if input("\nç¢ºèªåŸ·è¡Œ? (Y/n): ").strip().lower() == 'n':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆ")
                return
        
        # åŸ·è¡Œå®šç‰ˆ
        self.execute_pinning()
    
    def test_jira_connection(self):
        """æ¸¬è©¦ JIRA é€£ç·š"""
        print("\næ¸¬è©¦ JIRA é€£ç·š...")
        
        if not hasattr(self.tool, 'source_cmd_manager'):
            self.tool.source_cmd_manager = SourceCommandManager()
        
        if self.tool.source_cmd_manager.jira_client.connect():
            print("âœ… JIRA é€£ç·šæˆåŠŸï¼")
            
            if input("\næ˜¯å¦è¦æ¸¬è©¦æŸ¥è©¢ DB çš„ source command? (y/N): ").strip().lower() == 'y':
                db_name = input("è«‹è¼¸å…¥ DB åç¨± (ä¾‹å¦‚: DB2302): ").strip()
                if db_name:
                    print(f"\næŸ¥è©¢ {db_name} çš„ source command...")
                    
                    test_db_info = DBInfo(
                        sn=0,
                        module="Test",
                        db_type="master",
                        db_info=db_name,
                        db_folder="",
                        sftp_path=""
                    )
                    
                    cmd = self.tool.source_cmd_manager.get_source_command(
                        test_db_info,
                        self.tool.mapping_reader.df if self.tool.mapping_reader else None
                    )
                    
                    if cmd:
                        print(f"\næ‰¾åˆ°çš„ source command:")
                        print(f"  {cmd}")
                    else:
                        print(f"\næœªæ‰¾åˆ° {db_name} çš„ source command")
        else:
            print("âŒ JIRA é€£ç·šå¤±æ•—ï¼")
    
    def test_sftp_connection(self):
        """æ¸¬è©¦ SFTP é€£ç·š"""
        print("\næ¸¬è©¦ SFTP é€£ç·š...")
        print(f"ä¼ºæœå™¨: {config_manager.sftp_config['host']}:{config_manager.sftp_config['port']}")
        print(f"ç”¨æˆ¶: {config_manager.sftp_config['username']}")
        
        # å‰µå»ºè‡¨æ™‚ SFTP ç®¡ç†å™¨é€²è¡Œæ¸¬è©¦
        test_sftp_manager = SFTPManager()
        
        if test_sftp_manager.connect():
            print("âœ… SFTP é€£ç·šæˆåŠŸï¼")
            
            if input("\næ˜¯å¦è¦æ¸¬è©¦è·¯å¾‘å­˜å–? (y/N): ").strip().lower() == 'y':
                path = input("è«‹è¼¸å…¥è¦æ¸¬è©¦çš„è·¯å¾‘: ").strip()
                if path:
                    try:
                        result = test_sftp_manager.find_latest_manifest(path)
                        if result:
                            print(f"âœ… æ‰¾åˆ° manifest: {result[1]}")
                        else:
                            print("âŒ æ²’æœ‰æ‰¾åˆ° manifest")
                    except Exception as e:
                        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
            
            test_sftp_manager.disconnect()
        else:
            print("âŒ SFTP é€£ç·šå¤±æ•—ï¼")

    # ğŸ”¥ æ–°å¢ï¼šè¨­å®šæ¸…ç†é¸é …
    def setup_clean_options(self):
        """è¨­å®šæ¸…ç†ç›¸é—œé¸é … - ğŸ”¥ æ·»åŠ æ›´å¤šå¯¦ç”¨é¸é …"""
        print("\n" + "="*50)
        print("ğŸ§¹ æ¸…ç†é¸é …è¨­å®š")
        print("="*50)
        
        print(f"ç›®å‰è¨­å®š:")
        print(f"  æª¢æŸ¥ä¹¾æ·¨ç‹€æ…‹: {'æ˜¯' if config_manager.repo_config['check_clean_before_sync'] else 'å¦'}")
        print(f"  è‡ªå‹•æ¸…ç†å·¥ä½œç©ºé–“: {'æ˜¯' if config_manager.repo_config['auto_clean_workspace'] else 'å¦'}")
        print(f"  å‚™ä»½æœ¬åœ°ä¿®æ”¹: {'æ˜¯' if config_manager.repo_config['backup_local_changes'] else 'å¦'}")
        print(f"  ğŸ”¥ æª¢æŸ¥è¶…æ™‚æ™‚è·³é: {'æ˜¯' if config_manager.repo_config['skip_clean_on_timeout'] else 'å¦'}")
        print(f"  ğŸ”¥ åªåšå¿«é€Ÿæ¸…ç†: {'æ˜¯' if config_manager.repo_config['quick_clean_only'] else 'å¦'}")
        print(f"  ğŸ”¥ é«’repoæ™‚è©¢å•ç”¨æˆ¶: {'æ˜¯' if config_manager.repo_config['ask_user_on_dirty'] else 'å¦'}")
        
        if input("\næ˜¯å¦è¦ä¿®æ”¹è¨­å®š? (y/N): ").strip().lower() == 'y':
            
            # æª¢æŸ¥ä¹¾æ·¨ç‹€æ…‹
            check_clean = input(f"æª¢æŸ¥ä¹¾æ·¨ç‹€æ…‹ [{'æ˜¯' if config_manager.repo_config['check_clean_before_sync'] else 'å¦'}] (y/n/enterè·³é): ").strip().lower()
            if check_clean == 'y':
                config_manager.repo_config['check_clean_before_sync'] = True
            elif check_clean == 'n':
                config_manager.repo_config['check_clean_before_sync'] = False
            
            # ğŸ”¥ æ–°å¢ï¼šæª¢æŸ¥è¶…æ™‚æ™‚è·³é
            skip_timeout = input(f"æª¢æŸ¥è¶…æ™‚æ™‚è·³éæ¸…ç† [{'æ˜¯' if config_manager.repo_config['skip_clean_on_timeout'] else 'å¦'}] (y/n/enterè·³é): ").strip().lower()
            if skip_timeout == 'y':
                config_manager.repo_config['skip_clean_on_timeout'] = True
            elif skip_timeout == 'n':
                config_manager.repo_config['skip_clean_on_timeout'] = False
            
            # ğŸ”¥ æ–°å¢ï¼šé«’repoæ™‚è©¢å•ç”¨æˆ¶
            ask_user = input(f"ç™¼ç¾é«’repoæ™‚è©¢å•ç”¨æˆ¶ [{'æ˜¯' if config_manager.repo_config['ask_user_on_dirty'] else 'å¦'}] (y/n/enterè·³é): ").strip().lower()
            if ask_user == 'y':
                config_manager.repo_config['ask_user_on_dirty'] = True
            elif ask_user == 'n':
                config_manager.repo_config['ask_user_on_dirty'] = False
            
            # è‡ªå‹•æ¸…ç†
            auto_clean = input(f"è‡ªå‹•æ¸…ç†å·¥ä½œç©ºé–“ [{'æ˜¯' if config_manager.repo_config['auto_clean_workspace'] else 'å¦'}] (y/n/enterè·³é): ").strip().lower()
            if auto_clean == 'y':
                config_manager.repo_config['auto_clean_workspace'] = True
            elif auto_clean == 'n':
                config_manager.repo_config['auto_clean_workspace'] = False
            
            print("âœ… æ¸…ç†é¸é …å·²æ›´æ–°")
            print("\nğŸ’¡ å»ºè­°é…ç½®ï¼ˆé‡å°å¤§å‹repoï¼‰:")
            print("  - æª¢æŸ¥è¶…æ™‚æ™‚è·³é: æ˜¯")
            print("  - é«’repoæ™‚è©¢å•ç”¨æˆ¶: æ˜¯") 
            print("  - è‡ªå‹•æ¸…ç†å·¥ä½œç©ºé–“: å¦")
    
    # ğŸ”¥ æ–°å¢ï¼šæ‰‹å‹•æ¸…ç†å·¥ä½œç©ºé–“
    def manual_clean_workspace(self):
        """æ‰‹å‹•æ¸…ç†å·¥ä½œç©ºé–“"""
        print("\n" + "="*50)
        print("ğŸ§¹ æ‰‹å‹•æ¸…ç†å·¥ä½œç©ºé–“")
        print("="*50)
        
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return
        
        # é¸æ“‡è¦æ¸…ç†çš„DB
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        unique_dbs = list(set([db.db_info for db in all_db_infos]))
        unique_dbs.sort()
        
        print(f"\næ‰¾åˆ° {len(unique_dbs)} å€‹ DB:")
        for i, db in enumerate(unique_dbs, 1):
            local_path = os.path.join(self.tool.output_dir, 'unknown', db)  # ç°¡åŒ–è·¯å¾‘æª¢æŸ¥
            status = "ğŸŸ¢" if not self.tool.repo_manager.check_repo_exists(local_path) else "ğŸ”¶"
            print(f"{i:3d}. {status} {db}")
        
        print("\nğŸŸ¢ = ç„¡repoç›®éŒ„  ğŸ”¶ = æœ‰repoç›®éŒ„")
        
        choice = input("\né¸æ“‡è¦æ¸…ç†çš„DB (è¼¸å…¥ç·¨è™Ÿï¼Œå¤šå€‹ç”¨é€—è™Ÿåˆ†éš”ï¼Œæˆ–è¼¸å…¥'all'): ").strip()
        
        if choice.lower() == 'all':
            selected_dbs = unique_dbs
        else:
            try:
                indices = [int(x.strip()) for x in choice.split(',')]
                selected_dbs = [unique_dbs[i-1] for i in indices if 1 <= i <= len(unique_dbs)]
            except:
                print("âŒ ç„¡æ•ˆçš„è¼¸å…¥")
                return
        
        if not selected_dbs:
            print("âŒ æ²’æœ‰é¸æ“‡ä»»ä½•DB")
            return
        
        print(f"\næº–å‚™æ¸…ç† {len(selected_dbs)} å€‹DB:")
        for db in selected_dbs:
            print(f"  - {db}")
        
        if input("\nç¢ºèªåŸ·è¡Œæ¸…ç†? (y/N): ").strip().lower() != 'y':
            print("âŒ ç”¨æˆ¶å–æ¶ˆ")
            return
        
        # åŸ·è¡Œæ¸…ç†
        success_count = 0
        for db in selected_dbs:
            try:
                # æ§‹å»ºå¯èƒ½çš„è·¯å¾‘
                possible_paths = []
                for db_info in all_db_infos:
                    if db_info.db_info == db:
                        local_path = os.path.join(self.tool.output_dir, db_info.module, db_info.db_info)
                        if local_path not in possible_paths:
                            possible_paths.append(local_path)
                
                cleaned_any = False
                for local_path in possible_paths:
                    if self.tool.repo_manager.check_repo_exists(local_path):
                        print(f"\nğŸ§¹ æ¸…ç† {db} ({local_path})")
                        
                        # æª¢æŸ¥ç‹€æ…‹
                        clean_status = self.tool.repo_manager.check_repo_clean_status(local_path)
                        print(f"   ç‹€æ…‹: {clean_status['details']}")
                        
                        # å‚™ä»½ä¸¦æ¸…ç†
                        if not clean_status['is_clean']:
                            if config_manager.repo_config['backup_local_changes']:
                                self.tool.repo_manager.backup_local_changes(local_path, db)
                            
                            if self.tool.repo_manager.clean_repo_workspace(local_path, force=True):
                                print(f"   âœ… æ¸…ç†æˆåŠŸ")
                                cleaned_any = True
                            else:
                                print(f"   âŒ æ¸…ç†å¤±æ•—")
                        else:
                            print(f"   âœ… å·²ç¶“æ˜¯ä¹¾æ·¨ç‹€æ…‹")
                            cleaned_any = True
                
                if cleaned_any:
                    success_count += 1
                    
            except Exception as e:
                print(f"   âŒ æ¸…ç† {db} å¤±æ•—: {e}")
        
        print(f"\nğŸ‰ æ¸…ç†å®Œæˆ: {success_count}/{len(selected_dbs)} å€‹DBæ¸…ç†æˆåŠŸ")

    def display_menu(self) -> str:
        """é¡¯ç¤ºä¸»é¸å–®"""
        print("\n" + "="*60)
        print("Manifest å®šç‰ˆå·¥å…· - ä¸»é¸å–® (æ”¹é€²ç‰ˆ + Manifest æ¯”è¼ƒ)")
        
        has_defaults = any([
            config_manager.default_execution_config.get('mapping_table'),
            config_manager.default_execution_config.get('db_type'),
            config_manager.default_execution_config.get('selected_dbs'),
            config_manager.default_execution_config.get('db_versions'),
            config_manager.default_execution_config.get('output_dir')
        ])
        
        if has_defaults:
            print("(ğŸ“Œ å·²è¼‰å…¥é è¨­é…ç½®)")
        
        print("="*60)
        print("1. è¼‰å…¥ mapping table")
        print("2. è¨­å®š SFTP é€£ç·šè³‡è¨Š")
        print("3. é¸æ“‡ DB é¡å‹ (master/premp/mp/mpbackup/all)")
        print("4. é¸æ“‡è¦å®šç‰ˆçš„ DB")
        print("5. è¨­å®š DB ç‰ˆæœ¬")
        print("6. é–‹å§‹åŸ·è¡Œå®šç‰ˆ")
        print("7. é¡¯ç¤ºç›®å‰è¨­å®š")
        print("8. å¿«é€ŸåŸ·è¡Œï¼ˆä½¿ç”¨æ‰€æœ‰é è¨­å€¼ï¼‰")
        print("9. æ¸¬è©¦ JIRA é€£ç·š")
        print("10. æ¸¬è©¦ SFTP é€£ç·š")
        print("11. ğŸ”¥ è¨­å®šæ¸…ç†é¸é …")  # æ–°å¢
        print("12. ğŸ”¥ æ‰‹å‹•æ¸…ç†å·¥ä½œç©ºé–“")  # æ–°å¢
        print("0. çµæŸç¨‹å¼")
        print("="*60)
        
        return input("è«‹é¸æ“‡åŠŸèƒ½: ").strip()
    
    def run_interactive(self):
        """åŸ·è¡Œäº’å‹•å¼ä»‹é¢"""
        print("\næ­¡è¿ä½¿ç”¨ Manifest å®šç‰ˆå·¥å…·ï¼")
        print(f"ç‰ˆæœ¬: {__version__} (æ”¹é€²ç‰ˆ + Manifest æ¯”è¼ƒåŠŸèƒ½)")
        print("æ”¹é€²å…§å®¹: ä¿®å¾© SFTP Garbage packet å•é¡Œã€æ”¹å–„æ—¥èªŒè¼¸å‡ºã€å³æ™‚ manifest æ¯”è¼ƒã€ğŸ”¥ æ–°å¢å·¥ä½œç©ºé–“æ¸…ç†åŠŸèƒ½")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é è¨­é…ç½®
        has_complete_defaults = all([
            config_manager.default_execution_config.get('mapping_table'),
            os.path.exists(config_manager.default_execution_config.get('mapping_table', ''))
        ])
        
        if has_complete_defaults:
            print("\nğŸ“Œ åµæ¸¬åˆ°å®Œæ•´çš„é è¨­é…ç½®")
            if input("æ˜¯å¦è¦ä½¿ç”¨é è¨­é…ç½®å¿«é€ŸåŸ·è¡Œ? (y/N): ").strip().lower() == 'y':
                self.quick_execute_with_defaults()
                return
        
        while True:
            try:
                choice = self.display_menu()
                
                if choice == '0':
                    print("\nğŸ‘‹ å†è¦‹ï¼")
                    break
                elif choice == '1':
                    self.load_mapping_table()
                elif choice == '2':
                    self.setup_sftp()
                elif choice == '3':
                    self.select_db_type()
                elif choice == '4':
                    self.select_dbs()
                elif choice == '5':
                    self.setup_db_versions()
                elif choice == '6':
                    self.execute_pinning()
                elif choice == '7':
                    self.display_current_settings()
                elif choice == '8':
                    self.quick_execute_with_defaults()
                elif choice == '9':
                    self.test_jira_connection()
                elif choice == '10':
                    self.test_sftp_connection()
                elif choice == '11':  # ğŸ”¥ æ–°å¢
                    self.setup_clean_options()
                elif choice == '12':  # ğŸ”¥ æ–°å¢
                    self.manual_clean_workspace()
                else:
                    print("âŒ ç„¡æ•ˆçš„é¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·")
                if input("ç¢ºå®šè¦çµæŸç¨‹å¼å—? (Y/n): ").strip().lower() != 'n':
                    break
            except Exception as e:
                self.logger.error(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                if input("æ˜¯å¦ç¹¼çºŒ? (Y/n): ").strip().lower() == 'n':
                    break
        
        resource_manager.cleanup_all()

# =====================================
# ===== ä¸»ç¨‹å¼ =====
# =====================================

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(
        description='Manifest å®šç‰ˆå·¥å…· - è‡ªå‹•åŒ– repo å®šç‰ˆè™•ç† (æ”¹é€²ç‰ˆ + Manifest æ¯”è¼ƒåŠŸèƒ½)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
ç‰ˆæœ¬: {__version__}
ä½œè€…: {__author__}
æ—¥æœŸ: {__date__}

æ”¹é€²å…§å®¹:
- ä¿®å¾© SFTP "Garbage packet received" éŒ¯èª¤
- æ”¹ç‚ºæ­£å¸¸é †åºæ—¥èªŒè¼¸å‡ºï¼Œä¾¿æ–¼ debug
- æ”¹é€² Excel å ±å‘Šæ ¼å¼ï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰
- è¨˜éŒ„å®Œæ•´ repo init æŒ‡ä»¤å’Œ JIRA é€£çµ
- âœ¨ æ–°å¢ Manifest æ¯”è¼ƒåŠŸèƒ½ï¼šè‡ªå‹•æ¯”è¼ƒåŸå§‹ manifest å’Œå°å‡ºçš„ vp_manifest.xml
- ğŸ”¥ å³æ™‚å ±å‘Šæ›´æ–°ï¼šæ¯å€‹ DB å®Œæˆå¾Œç«‹å³æ›´æ–° Excel å ±å‘Š
- ä¿®å¾©é‡è¤‡æ—¥èªŒè¼¸å‡ºå•é¡Œ

ç¯„ä¾‹:
  # ä½¿ç”¨äº’å‹•å¼ä»‹é¢
  python {sys.argv[0]}
  
  # è™•ç†æŒ‡å®šçš„ DB
  python {sys.argv[0]} -m mapping.xlsx -d DB2302,DB2575 -o ./output
  
  # æ¸¬è©¦æ¨¡å¼
  python {sys.argv[0]} -m mapping.xlsx --dry-run
        """
    )
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('-m', '--mapping', 
                        type=str,
                        help='Mapping table Excel æª”æ¡ˆè·¯å¾‘')
    
    parser.add_argument('-o', '--output', 
                        type=str,
                        help=f'è¼¸å‡ºç›®éŒ„ (é è¨­: {config_manager.path_config["default_output_dir"]})')
    
    parser.add_argument('-t', '--type',
                        choices=['all', 'master', 'premp', 'mp', 'mpbackup'],
                        default='all',
                        help='è¦è™•ç†çš„ DB é¡å‹ (é è¨­: all)')
    
    parser.add_argument('-d', '--dbs', 
                        type=str,
                        help='è¦è™•ç†çš„ DB åˆ—è¡¨ï¼Œé€—è™Ÿåˆ†éš” (å¯åŒ…å«ç‰ˆæœ¬ï¼Œå¦‚: DB2302#3,DB2575)')
    
    parser.add_argument('-v', '--versions', 
                        type=str,
                        help='DB ç‰ˆæœ¬è¨­å®šï¼Œæ ¼å¼: DB2302#3,DB2575#186')
    
    parser.add_argument('--dry-run', 
                        action='store_true',
                        help='æ¸¬è©¦æ¨¡å¼ï¼Œåªé¡¯ç¤ºå°‡è¦åŸ·è¡Œçš„å‹•ä½œï¼Œä¸å¯¦éš›åŸ·è¡Œ')
    
    parser.add_argument('--debug', 
                        action='store_true',
                        help='å•Ÿç”¨ debug æ¨¡å¼ï¼Œé¡¯ç¤ºè©³ç´°æ—¥èªŒ')
    
    parser.add_argument('--version', 
                        action='version',
                        version=f'%(prog)s {__version__}')
    
    # è§£æåƒæ•¸
    args = parser.parse_args()
    
    # è¨­å®šæ—¥èªŒç­‰ç´š
    if args.debug:
        config_manager.log_config['level'] = logging.DEBUG
        logging.getLogger().setLevel(logging.DEBUG)
        for handler in logging.getLogger().handlers:
            handler.setLevel(logging.DEBUG)
        print("ğŸ” Debug æ¨¡å¼å·²å•Ÿç”¨")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¸¬è©¦æ¨¡å¼
    if args.dry_run:
        print("\n" + "="*60)
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ (Dry Run) - ä¸æœƒå¯¦éš›åŸ·è¡Œä»»ä½•æ“ä½œ")
        print("="*60)
    
    # æ±ºå®šåŸ·è¡Œæ¨¡å¼
    if args.mapping:
        # å‘½ä»¤åˆ—æ¨¡å¼
        print("\n" + "="*60)
        print(f"ğŸ“‹ Manifest å®šç‰ˆå·¥å…· v{__version__} - å‘½ä»¤åˆ—æ¨¡å¼ (æ”¹é€²ç‰ˆ + Manifest æ¯”è¼ƒ)")
        print("="*60)
        
        tool = ManifestPinningTool()
        
        if args.dry_run:
            tool.dry_run = True
        
        try:
            # è¼‰å…¥ mapping table
            print(f"\nğŸ“‚ è¼‰å…¥ mapping table: {args.mapping}")
            if not os.path.exists(args.mapping):
                print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.mapping}")
                sys.exit(1)
            
            if not tool.load_mapping_table(args.mapping):
                print("âŒ ç„¡æ³•è¼‰å…¥ mapping table")
                sys.exit(1)
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ mapping table")
            
            # è¨­å®šè¼¸å‡ºç›®éŒ„
            tool.output_dir = args.output or config_manager.path_config['default_output_dir']
            os.makedirs(tool.output_dir, exist_ok=True)
            print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {tool.output_dir}")

            print(f"\nğŸŒ æº–å‚™ SFTP é€£ç·š: {config_manager.sftp_config['host']}")
            print("â„¹ï¸  æ¯å€‹ DB å°‡ä½¿ç”¨ç¨ç«‹çš„ SFTP é€£ç·š")

            try:
                # æ±ºå®šè¦è™•ç†çš„ DB åˆ—è¡¨
                db_list = []
                db_versions = {}
                
                if args.dbs:
                    db_specs = [db.strip() for db in args.dbs.split(',')]
                    
                    for db_spec in db_specs:
                        if '#' in db_spec:
                            db_name, version = db_spec.split('#', 1)
                            db_list.append(db_name)
                            db_versions[db_name] = version
                        else:
                            db_list.append(db_spec)
                    
                    print(f"\nğŸ“Œ ä½¿ç”¨æŒ‡å®šçš„ DB åˆ—è¡¨: {', '.join(db_list)}")
                else:
                    all_db_infos = tool.get_all_dbs(args.type)
                    db_list = list(set([db.db_info for db in all_db_infos]))
                    
                    if args.type == 'all':
                        print(f"\nğŸ“Œ ä½¿ç”¨æ‰€æœ‰ DBï¼Œå…± {len(db_list)} å€‹")
                    else:
                        print(f"\nğŸ“Œ ä½¿ç”¨æ‰€æœ‰ {args.type} é¡å‹çš„ DBï¼Œå…± {len(db_list)} å€‹")
                
                # è™•ç†é¡å¤–çš„ç‰ˆæœ¬è¨­å®š
                if args.versions:
                    version_specs = [v.strip() for v in args.versions.split(',')]
                    for version_spec in version_specs:
                        if '#' in version_spec:
                            db_name, version = version_spec.split('#', 1)
                            db_versions[db_name] = version
                    
                    print(f"ğŸ“Œ è¨­å®šäº† {len(db_versions)} å€‹ DB çš„ç‰ˆæœ¬")
                
                # ç¢ºèªè™•ç†è³‡è¨Š
                print("\n" + "-"*40)
                print("ğŸ“‹ æº–å‚™è™•ç†ä»¥ä¸‹ DB:")
                for i, db in enumerate(db_list, 1):
                    version_info = f" (ç‰ˆæœ¬: {db_versions[db]})" if db in db_versions else " (æœ€æ–°ç‰ˆæœ¬)"
                    print(f"  {i:3d}. {db}{version_info}")
                print("-"*40)
                
                if not db_list:
                    print("âŒ æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
                    sys.exit(1)
                
                # è©¢å•ç¢ºèªï¼ˆé™¤éæ˜¯æ¸¬è©¦æ¨¡å¼ï¼‰
                if not args.dry_run:
                    if sys.stdin.isatty():
                        confirm = input(f"\nç¢ºèªè¦è™•ç† {len(db_list)} å€‹ DB? (Y/n): ").strip().lower()
                        if confirm == 'n':
                            print("âŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                            sys.exit(0)
                
                # é–‹å§‹è™•ç†
                print("\n" + "="*60)
                if args.dry_run:
                    print("ğŸ§ª é–‹å§‹æ¸¬è©¦åŸ·è¡Œï¼ˆä¸æœƒå¯¦éš›åŸ·è¡Œæ“ä½œï¼‰")
                else:
                    print("ğŸš€ é–‹å§‹åŸ·è¡Œå®šç‰ˆè™•ç†")
                print("="*60)
                
                start_time = datetime.now()
                
                # åŸ·è¡Œè™•ç†
                tool.process_selected_dbs(db_list, db_versions)
                
                end_time = datetime.now()
                elapsed_time = end_time - start_time
                
                # ç”¢ç”Ÿå ±å‘Š
                if not args.dry_run:
                    print("\nğŸ“Š ç”¢ç”Ÿè™•ç†å ±å‘Š...")
                    report_path = os.path.join(
                        tool.output_dir, 
                        config_manager.path_config['report_filename']
                    )
                    tool.generate_report(report_path)
                
                # é¡¯ç¤ºçµæœæ‘˜è¦
                print("\n" + "="*60)
                print("âœ¨ è™•ç†å®Œæˆï¼")
                print("="*60)
                print(f"ğŸ“Š ç¸½ DB æ•¸: {tool.report.total_dbs}")
                print(f"âœ… æˆåŠŸ: {tool.report.successful_dbs}")
                print(f"âŒ å¤±æ•—: {tool.report.failed_dbs}")
                print(f"â­ è·³é: {tool.report.skipped_dbs}")
                print(f"ğŸ” æœ‰å·®ç•°: {tool.report.dbs_with_differences}")
                print(f"â±ï¸ ç¸½è€—æ™‚: {elapsed_time}")
                print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {tool.output_dir}")
                if not args.dry_run:
                    print(f"ğŸ“Š å ±å‘Šæª”æ¡ˆ: {report_path}")
                print("="*60)
                
                # å¦‚æœæœ‰å¤±æ•—çš„é …ç›®ï¼Œé¡¯ç¤ºè©³ç´°è³‡è¨Š
                if tool.report.failed_dbs > 0:
                    print("\nâŒ å¤±æ•—çš„ DB:")
                    for db in tool.report.db_details:
                        if db.status == DBStatus.FAILED:
                            print(f"  - {db.module}/{db.db_info}: {db.error_message}")
                
                # å¦‚æœæœ‰å·®ç•°çš„é …ç›®ï¼Œé¡¯ç¤ºä¿¡æ¯
                if tool.report.dbs_with_differences > 0:
                    print(f"\nğŸ” æœ‰ {tool.report.dbs_with_differences} å€‹ DB çš„ manifest æœ‰å·®ç•°:")
                    for db in tool.report.db_details:
                        if db.status == DBStatus.SUCCESS_WITH_DIFF:
                            print(f"  - {db.module}/{db.db_info}: {db.manifest_comparison_summary}")
                
            finally:
                print("\nğŸ“Œ æ¸…ç†è³‡æº...")
                resource_manager.cleanup_all()
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ æ”¶åˆ° Ctrl+Cï¼Œæ¸…ç†æ‰€æœ‰é€²ç¨‹...")
            resource_manager.cleanup_all()
            sys.exit(0)
            
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            if args.debug:
                import traceback
                traceback.print_exc()
            sys.exit(1)
        
        finally:
            resource_manager.cleanup_all()
    
    else:
        # äº’å‹•å¼æ¨¡å¼
        print("\n" + "="*60)
        print(f"ğŸ® Manifest å®šç‰ˆå·¥å…· v{__version__} - äº’å‹•å¼ä»‹é¢ (æ”¹é€²ç‰ˆ + Manifest æ¯”è¼ƒ)")
        print("="*60)
        print("æ”¹é€²å…§å®¹: ä¿®å¾© SFTP Garbage packet å•é¡Œã€æ”¹å–„æ—¥èªŒè¼¸å‡ºã€å³æ™‚ manifest æ¯”è¼ƒ")
        print("æç¤º: ä½¿ç”¨ -h åƒæ•¸æŸ¥çœ‹å‘½ä»¤åˆ—é¸é …")
        print("="*60)
        
        try:
            ui = InteractiveUI()
            
            if args.output:
                ui.tool.output_dir = args.output
            
            if args.dry_run:
                ui.tool.dry_run = True
                print("ğŸ§ª æ¸¬è©¦æ¨¡å¼å·²å•Ÿç”¨")
            
            ui.run_interactive()
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è¦‹ï¼")
            sys.exit(0)
            
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            if args.debug:
                import traceback
                traceback.print_exc()
            sys.exit(1)
        
        finally:
            resource_manager.cleanup_all()

if __name__ == "__main__":
    main()