#!/usr/bin/env python3
"""
Manifest Pinning Tool - è‡ªå‹•åŒ–å®šç‰ˆå·¥å…· (æ”¹é€²ç‰ˆ)
ç”¨æ–¼å¾ SFTP ä¸‹è¼‰ manifest æª”æ¡ˆä¸¦åŸ·è¡Œ repo å®šç‰ˆæ“ä½œ
æ”¹é€²ç‰ˆæœ¬ï¼šç°¡åŒ– SFTPã€æ”¹é€²å ±å‘Šæ ¼å¼ã€æ­£å¸¸æ—¥èªŒè¼¸å‡º
"""

from enum import Enum
import os
import sys
import argparse
import pandas as pd
import paramiko
import subprocess
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import logging
from dataclasses import dataclass, field, asdict
import signal
import atexit
from contextlib import contextmanager
from collections import defaultdict
import random
import socket
from threading import Semaphore, Lock
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

# =====================================
# ===== ç‰ˆæœ¬è³‡è¨Š =====
# =====================================
__version__ = '2.1.1'
__author__ = 'Vince Lin'
__date__ = '2024-12-19'

# =====================================
# ===== é…ç½®ç®¡ç†å™¨ =====
# =====================================

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰é…ç½®çš„å„ªå…ˆç´šå’Œä¾†æº"""
    
    def __init__(self):
        # åŸºç¤é…ç½®
        self.sftp_config = {
            'host': 'mmsftpx.realtek.com',
            'port': 22,
            'username': 'lgwar_user',
            'password': 'Ab!123456',
            'timeout': 30,
            'retry_count': 3,
            'retry_delay': 2
        }
        
        self.jira_config = {
            'site': 'jira.realtek.com',
            'username': 'vince_lin',
            'password': 'Amon200!Amon200!',
            'api_url': 'https://jira.realtek.com/rest/api/2'
        }
        
        self.path_config = {
            'default_output_dir': './DB-source',
            'default_mapping_table': '../all_chip_mapping_table.xlsx',
            'manifest_pattern': r'manifest_(\d+)\.xml',
            'report_filename': 'pinning_report.xlsx'
        }
        
        self.repo_config = {
            'repo_command': 'repo',
            'sync_jobs': 8,
            'sync_retry': 2,
            'init_timeout': 300,
            'sync_timeout': 7200,
            'async_sync': True,
            'show_sync_output': False
        }
        
        self.parallel_config = {
            'max_workers': 4,
            'enable_parallel': True,
        }
        
        self.log_config = {
            'level': logging.INFO,
            'format': '%(asctime)s - [%(levelname)s] - %(name)s - %(message)s',
            'date_format': '%Y-%m-%d %H:%M:%S'
        }
        
        # é è¨­åŸ·è¡Œé…ç½®
        self.default_execution_config = {
            'mapping_table': os.path.join(os.path.dirname(__file__), '..', 'all_chip_mapping_table.xlsx'),
            'db_type': 'all',
            'selected_dbs': ['DB2145', 'DB2858', 'DB2575', 'DB2919'],
            'db_versions': {},
            'output_dir': './DB-source',
            'auto_confirm': False,
            'sftp_override': {}
        }
        
        # é…ç½®å„ªå…ˆç´šè¿½è¹¤
        self.config_sources = {}
        
    def apply_overrides(self, overrides: Dict[str, Any], source: str = 'user'):
        """æ‡‰ç”¨é…ç½®è¦†è“‹ä¸¦è¨˜éŒ„ä¾†æº"""
        for key, value in overrides.items():
            if value is not None:
                self.config_sources[key] = source
                # æ ¹æ“š key æ›´æ–°å°æ‡‰çš„é…ç½®
                if key.startswith('sftp_'):
                    sftp_key = key.replace('sftp_', '')
                    if sftp_key in self.sftp_config:
                        self.sftp_config[sftp_key] = value
                elif key.startswith('repo_'):
                    repo_key = key.replace('repo_', '')
                    if repo_key in self.repo_config:
                        self.repo_config[repo_key] = value
                elif key.startswith('parallel_'):
                    parallel_key = key.replace('parallel_', '')
                    if parallel_key in self.parallel_config:
                        self.parallel_config[parallel_key] = value
    
    def validate_config(self) -> Tuple[bool, List[str]]:
        """é©—è­‰é…ç½®çš„å®Œæ•´æ€§å’Œåˆç†æ€§"""
        errors = []
        
        # é©—è­‰ SFTP é…ç½®
        if not self.sftp_config.get('host'):
            errors.append("SFTP host ä¸èƒ½ç‚ºç©º")
        if not self.sftp_config.get('username'):
            errors.append("SFTP username ä¸èƒ½ç‚ºç©º")
        
        # é©—è­‰è·¯å¾‘é…ç½®
        if not self.path_config.get('default_output_dir'):
            errors.append("è¼¸å‡ºç›®éŒ„ä¸èƒ½ç‚ºç©º")
        
        # é©—è­‰ä¸¦è¡Œé…ç½®
        if self.parallel_config['max_workers'] < 1:
            errors.append("max_workers å¿…é ˆå¤§æ–¼ 0")
        
        return len(errors) == 0, errors

# å…¨åŸŸé…ç½®ç®¡ç†å™¨å¯¦ä¾‹
config_manager = ConfigManager()

# =====================================
# ===== JIRA API å®¢æˆ¶ç«¯ =====
# =====================================

from jira import JIRA
import requests
from urllib.parse import quote
import urllib3

# é—œé–‰ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class JiraAPIClient:
    """ç°¡åŒ–ç‰ˆ JIRA API å®¢æˆ¶ç«¯"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.session = None
        self._connected = False
        self.base_url = f"https://{config_manager.jira_config['site']}"
        
    def connect(self) -> bool:
        """é€£æ¥åˆ° JIRA"""
        try:
            username = config_manager.jira_config['username']
            password = config_manager.jira_config['password']
            
            self.session = requests.Session()
            self.session.auth = (username, password)
            self.session.headers.update({
                'Accept': 'application/json',
                'Content-Type': 'application/json'
            })
            self.session.verify = False
            
            # æ¸¬è©¦é€£æ¥
            test_url = f"{self.base_url}/rest/api/2/myself"
            response = self.session.get(test_url, timeout=30)
            
            if response.status_code == 200:
                self._connected = True
                return True
            else:
                self.logger.error(f"JIRA é€£æ¥å¤±æ•—: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"JIRA é€£æ¥å¤±æ•—: {e}")
            return False

    def search_db_ticket(self, db_name: str, module: str = None) -> Optional[str]:
        """æ ¹æ“š DB å‘½åæ…£ä¾‹æœå°‹å°æ‡‰çš„ JIRA ticket"""
        try:
            if not self._connected:
                if not self.connect():
                    return None
            
            # æ ¹æ“šå‘½åæ…£ä¾‹ç›´æ¥æ§‹å»º ticket key
            db_number = db_name.replace('DB', '')
            possible_tickets = [
                f"MMQCDB-{db_number}",
                f"LGSWRD-{db_number}",
                f"RTK-{db_number}",
                f"DB-{db_number}",
            ]
            
            for ticket_key in possible_tickets:
                if self._check_ticket_exists(ticket_key):
                    return ticket_key
            
            return None
            
        except Exception as e:
            self.logger.error(f"æœå°‹ JIRA ticket å¤±æ•—: {e}")
            return None

    def _check_ticket_exists(self, ticket_key: str) -> bool:
        """æª¢æŸ¥æŒ‡å®šçš„ ticket æ˜¯å¦å­˜åœ¨"""
        try:
            url = f"{self.base_url}/rest/api/2/issue/{ticket_key}"
            response = self.session.get(url, timeout=30)
            return response.status_code == 200
        except:
            return False

    def get_source_command_from_ticket(self, ticket_key: str) -> Optional[str]:
        """å¾ JIRA ticket ä¸­æå– source command"""
        try:
            if not self._connected:
                if not self.connect():
                    return None
            
            url = f"{self.base_url}/rest/api/2/issue/{ticket_key}"
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                fields = data.get('fields', {})
                
                # æª¢æŸ¥æè¿°æ¬„ä½
                description = fields.get('description', '')
                if description:
                    cmd = self._extract_repo_command(description)
                    if cmd:
                        return cmd
                
                # æª¢æŸ¥è©•è«–
                comments_data = fields.get('comment', {})
                comments = comments_data.get('comments', [])
                for comment in comments:
                    body = comment.get('body', '')
                    if body:
                        cmd = self._extract_repo_command(body)
                        if cmd:
                            return cmd
                
                return None
            
            return None
                
        except Exception as e:
            self.logger.error(f"å¾ ticket {ticket_key} ç²å– source command å¤±æ•—: {e}")
            return None

    def _extract_repo_command(self, text: str) -> Optional[str]:
        """æå– repo init å‘½ä»¤ - å„ªå…ˆé¸æ“‡æœ‰ -m åƒæ•¸çš„æŒ‡ä»¤"""
        if not text or 'repo init' not in text:
            return None
        
        lines = text.split('\n')
        found_commands = []
        
        for line in lines:
            line = line.strip()
            
            # å°‹æ‰¾åŒ…å« repo init çš„è¡Œ
            if 'repo init' in line:
                repo_start = line.find('repo init')
                if repo_start != -1:
                    cmd = line[repo_start:].strip()
                    cmd = self._clean_command(cmd)
                    if self._is_valid_repo_command(cmd):
                        found_commands.append(cmd)
        
        if not found_commands:
            return None
        
        # ğŸ”¥ å„ªå…ˆé¸æ“‡æœ‰ "-m" åƒæ•¸çš„æŒ‡ä»¤
        commands_with_m = [cmd for cmd in found_commands if '-m ' in cmd]
        if commands_with_m:
            # å¦‚æœæœ‰å¤šå€‹ï¼Œå–ç¬¬ä¸€å€‹
            return commands_with_m[0]
        
        # å¦‚æœæ²’æœ‰ -m åƒæ•¸çš„ï¼Œè¿”å›ç¬¬ä¸€å€‹æœ‰æ•ˆçš„
        return found_commands[0]

    def _clean_command(self, cmd: str) -> str:
        """æ¸…ç†å‘½ä»¤å­—ç¬¦ä¸²"""
        # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´
        prefixes = ['$', '>', '#', '//', '*', '-', '=>']
        for prefix in prefixes:
            if cmd.startswith(prefix):
                cmd = cmd[len(prefix):].strip()
        
        cmd = cmd.rstrip('\n\r\\')
        return cmd.strip()

    def _is_valid_repo_command(self, cmd: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ repo init å‘½ä»¤"""
        if not cmd or not cmd.startswith('repo init'):
            return False
        
        required_parts = ['-u', '-b']
        return all(part in cmd for part in required_parts)

    def disconnect(self):
        """æ–·é–‹ JIRA é€£æ¥"""
        self._connected = False
        if self.session:
            self.session.close()
            self.session = None

# =====================================
# ===== æ—¥èªŒè¨­å®šå‡½å¼ =====
# =====================================

def setup_logger(name: str = __name__) -> logging.Logger:
    """è¨­å®šæ—¥èªŒè¨˜éŒ„å™¨"""
    logger = logging.getLogger(name)
    logger.setLevel(config_manager.log_config['level'])
    
    if not logger.handlers:
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(config_manager.log_config['level'])
        
        # Formatter
        formatter = logging.Formatter(
            config_manager.log_config['format'],
            datefmt=config_manager.log_config['date_format']
        )
        console_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
    
    return logger

logger = setup_logger(__name__)

# =====================================
# ===== é€²åº¦ç‹€æ…‹å®šç¾© =====
# =====================================

class DBStatus(Enum):
    """DB è™•ç†ç‹€æ…‹"""
    PENDING = "ç­‰å¾…ä¸­"
    DOWNLOADING_MANIFEST = "ä¸‹è¼‰ manifest"
    CHECKING_REPO = "æª¢æŸ¥ repo ç‹€æ…‹"
    REPO_INIT = "åŸ·è¡Œ repo init"
    REPO_SYNC = "åŸ·è¡Œ repo sync"
    EXPORTING = "å°å‡ºç‰ˆæœ¬"
    SUCCESS = "âœ… å®Œæˆ"
    FAILED = "âŒ å¤±æ•—"
    SKIPPED = "â­ï¸ è·³é"

# =====================================
# ===== è³‡æ–™çµæ§‹å®šç¾© =====
# =====================================

@dataclass
class DBInfo:
    """DB è³‡è¨Šè³‡æ–™çµæ§‹"""
    sn: int
    module: str
    db_type: str
    db_info: str
    db_folder: str
    sftp_path: str
    version: Optional[str] = None
    jira_link: Optional[str] = None
    source_command: Optional[str] = None
    manifest_file: Optional[str] = None
    manifest_full_path: Optional[str] = None
    local_path: Optional[str] = None
    has_existing_repo: bool = False
    status: DBStatus = DBStatus.PENDING
    error_message: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    sync_process: Optional[subprocess.Popen] = None
    actual_source_cmd: Optional[str] = None
    sync_log_path: Optional[str] = None

    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        
        # è™•ç† Enum å’Œ datetime
        if isinstance(result['status'], DBStatus):
            result['status'] = result['status'].value
        if result['start_time']:
            result['start_time'] = result['start_time'].strftime('%Y-%m-%d %H:%M:%S')
        if result['end_time']:
            result['end_time'] = result['end_time'].strftime('%Y-%m-%d %H:%M:%S')
        
        # ç§»é™¤ç„¡æ³•åºåˆ—åŒ–çš„ç‰©ä»¶
        result.pop('sync_process', None)
        
        return result

@dataclass
class PinningReport:
    """å®šç‰ˆå ±å‘Šè³‡æ–™çµæ§‹"""
    total_dbs: int = 0
    successful_dbs: int = 0
    failed_dbs: int = 0
    skipped_dbs: int = 0
    db_details: List[DBInfo] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    
    def add_db(self, db_info: DBInfo):
        self.db_details.append(db_info)
        if db_info.status == DBStatus.SUCCESS:
            self.successful_dbs += 1
        elif db_info.status == DBStatus.FAILED:
            self.failed_dbs += 1
        elif db_info.status == DBStatus.SKIPPED:
            self.skipped_dbs += 1
    
    def finalize(self):
        """å®Œæˆå ±å‘Š"""
        self.end_time = datetime.now()
        self.total_dbs = len(self.db_details)

# =====================================
# ===== è³‡æºç®¡ç†å™¨ =====
# =====================================

class ResourceManager:
    """çµ±ä¸€ç®¡ç†æ‰€æœ‰ç³»çµ±è³‡æº"""
    
    def __init__(self):
        self.active_processes = {}
        self.sftp_connections = []
        self.lock = threading.Lock()
        self.logger = setup_logger(self.__class__.__name__)
        
        atexit.register(self.cleanup_all)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
        print(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ¸…ç†æ‰€æœ‰é€²ç¨‹...")
        
        # åŸæœ‰çš„æ¸…ç†
        self.cleanup_all()
        
        # ğŸ”¥ æ–°å¢ï¼šç³»çµ±ç´šå¼·åˆ¶æ¸…ç†
        print("ğŸš¨ åŸ·è¡Œç³»çµ±ç´šæ¸…ç†...")
        os.system("pkill -TERM -f 'repo sync' 2>/dev/null || true")
        os.system("pkill -TERM -f 'unbuffer.*repo' 2>/dev/null || true")
        time.sleep(2)
        os.system("pkill -KILL -f 'repo sync' 2>/dev/null || true")
        os.system("pkill -KILL -f 'unbuffer.*repo' 2>/dev/null || true")
        
        print("âœ… æ¸…ç†å®Œæˆ")
        sys.exit(0)
    
    def register_process(self, name: str, process: subprocess.Popen):
        """è¨»å†Šæ–°çš„å­é€²ç¨‹"""
        with self.lock:
            self.active_processes[name] = process
    
    def unregister_process(self, name: str):
        """å–æ¶ˆè¨»å†Šå­é€²ç¨‹"""
        with self.lock:
            if name in self.active_processes:
                del self.active_processes[name]
    
    def register_sftp(self, connection):
        """è¨»å†Š SFTP é€£ç·š"""
        with self.lock:
            self.sftp_connections.append(connection)
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰è³‡æº"""
        with self.lock:
            # çµ‚æ­¢æ‰€æœ‰å­é€²ç¨‹
            for name, process in self.active_processes.items():
                try:
                    if process.poll() is None:
                        process.terminate()
                        process.wait(timeout=5)
                except:
                    try:
                        process.kill()
                    except:
                        pass
            
            # é—œé–‰æ‰€æœ‰ SFTP é€£ç·š
            for conn in self.sftp_connections:
                try:
                    if hasattr(conn, 'disconnect'):
                        conn.disconnect()
                    elif hasattr(conn, 'close'):
                        conn.close()
                except:
                    pass
            
            self.active_processes.clear()
            self.sftp_connections.clear()

# å…¨åŸŸè³‡æºç®¡ç†å™¨
resource_manager = ResourceManager()

# =====================================
# ===== æ”¹é€²ç‰ˆ SFTP ç®¡ç†å™¨ï¼ˆä¿®å¾© Garbage packet å•é¡Œï¼‰=====
# =====================================

class SFTPManager:
    """æ”¹é€²ç‰ˆ SFTP ç®¡ç†å™¨ - ä¿®å¾© type 3 unimplemented éŒ¯èª¤"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.config = config_manager.sftp_config
        self.sftp = None
        self.transport = None
        self.connected = False
        self._connection_lock = threading.Lock()
        # å¿«å–ä¼ºæœå™¨èƒ½åŠ›
        self._server_capabilities = {
            'supports_listdir_attr': None,
            'supports_stat': None,
            'checked': False
        }
        
    def connect(self) -> bool:
        """å»ºç«‹ SFTP é€£ç·š"""
        with self._connection_lock:
            try:
                # å¦‚æœå·²é€£ç·šï¼Œå…ˆæ¸…ç†
                if self.connected:
                    self.disconnect()
                
                self.logger.info(f"å»ºç«‹ SFTP é€£ç·š: {self.config['host']}:{self.config['port']}")
                
                # é‡è©¦é€£ç·š
                for attempt in range(self.config['retry_count']):
                    try:
                        # å»ºç«‹ Socket é€£ç·š
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(self.config['timeout'])
                        
                        # è¨­å®š socket é¸é …é¿å… Garbage packet
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                        
                        # é€£æ¥åˆ°ä¼ºæœå™¨
                        sock.connect((self.config['host'], self.config['port']))
                        
                        # å»ºç«‹ Transportï¼ˆä¿®å¾© Garbage packet çš„é—œéµï¼‰
                        self.transport = paramiko.Transport(sock)
                        
                        # è¨­å®šè¼ƒä¿å®ˆçš„åƒæ•¸é¿å…å”è­°å•é¡Œ
                        self.transport.set_keepalive(30)
                        self.transport.use_compression(False)  # é—œé–‰å£“ç¸®
                        
                        # é–‹å§‹ SSH æ¡æ‰‹
                        self.transport.start_client()
                        
                        # èªè­‰
                        self.transport.auth_password(
                            self.config['username'],
                            self.config['password']
                        )
                        
                        # å»ºç«‹ SFTP å®¢æˆ¶ç«¯
                        self.sftp = paramiko.SFTPClient.from_transport(self.transport)
                        
                        # æ¸¬è©¦é€£ç·šä¸¦æª¢æ¸¬ä¼ºæœå™¨èƒ½åŠ›
                        self._detect_server_capabilities()
                        
                        self.connected = True
                        resource_manager.register_sftp(self)
                        self.logger.info(f"SFTP é€£ç·šæˆåŠŸ (å˜—è©¦ {attempt + 1})")
                        return True
                        
                    except Exception as e:
                        self.logger.warning(f"SFTP é€£ç·šå˜—è©¦ {attempt + 1} å¤±æ•—: {e}")
                        self._cleanup_failed_connection()
                        
                        if attempt < self.config['retry_count'] - 1:
                            time.sleep(self.config['retry_delay'])
                
                self.logger.error("SFTP é€£ç·šå¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                return False
                
            except Exception as e:
                self.logger.error(f"SFTP é€£ç·šéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                self._cleanup_failed_connection()
                return False
    
    def _detect_server_capabilities(self):
        """æª¢æ¸¬ SFTP ä¼ºæœå™¨æ”¯æ´çš„åŠŸèƒ½"""
        try:
            self.logger.debug("æª¢æ¸¬ SFTP ä¼ºæœå™¨èƒ½åŠ›...")
            
            # æ¸¬è©¦åŸºæœ¬ listdir
            try:
                self.sftp.listdir('.')
                self.logger.debug("âœ… åŸºæœ¬ listdir æ”¯æ´")
            except Exception as e:
                self.logger.warning(f"âŒ åŸºæœ¬ listdir ä¸æ”¯æ´: {e}")
                raise Exception("ä¼ºæœå™¨ä¸æ”¯æ´åŸºæœ¬ SFTP æ“ä½œ")
            
            # æ¸¬è©¦ listdir_attr
            try:
                self.sftp.listdir_attr('.')
                self._server_capabilities['supports_listdir_attr'] = True
                self.logger.debug("âœ… listdir_attr æ”¯æ´")
            except Exception as e:
                self._server_capabilities['supports_listdir_attr'] = False
                self.logger.debug(f"âŒ listdir_attr ä¸æ”¯æ´: {e}")
            
            # æ¸¬è©¦ stat
            try:
                self.sftp.stat('.')
                self._server_capabilities['supports_stat'] = True
                self.logger.debug("âœ… stat æ”¯æ´")
            except Exception as e:
                self._server_capabilities['supports_stat'] = False
                self.logger.debug(f"âŒ stat ä¸æ”¯æ´: {e}")
            
            self._server_capabilities['checked'] = True
            self.logger.info(f"ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬å®Œæˆ: listdir_attr={self._server_capabilities['supports_listdir_attr']}, stat={self._server_capabilities['supports_stat']}")
            
        except Exception as e:
            self.logger.warning(f"ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬å¤±æ•—: {e}")
            # è¨­å®šç‚ºæœ€ä¿å®ˆçš„æ¨¡å¼
            self._server_capabilities = {
                'supports_listdir_attr': False,
                'supports_stat': False,
                'checked': True
            }
    
    def _cleanup_failed_connection(self):
        """æ¸…ç†å¤±æ•—çš„é€£ç·š"""
        try:
            if self.sftp:
                self.sftp.close()
                self.sftp = None
            if self.transport:
                self.transport.close()
                self.transport = None
            self.connected = False
            self._server_capabilities['checked'] = False
        except:
            pass
    
    def disconnect(self):
        """å®‰å…¨æ–·é–‹ SFTP é€£ç·š"""
        with self._connection_lock:
            try:
                if self.sftp:
                    self.sftp.close()
                    self.sftp = None
                if self.transport:
                    self.transport.close()
                    self.transport = None
                self.connected = False
                self._server_capabilities['checked'] = False
                self.logger.info("SFTP é€£ç·šå·²é—œé–‰")
            except Exception as e:
                self.logger.warning(f"é—œé–‰ SFTP é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _ensure_connected(self) -> bool:
        """ç¢ºä¿é€£ç·šæœ‰æ•ˆ"""
        if not self.connected or not self.sftp or not self.transport:
            return self.connect()
        
        # æ¸¬è©¦é€£ç·šæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        try:
            self.sftp.listdir('.')
            return True
        except:
            self.logger.info("SFTP é€£ç·šå·²æ–·é–‹ï¼Œé‡æ–°é€£ç·š...")
            return self.connect()
    
    def get_latest_version(self, sftp_path: str) -> Tuple[str, str, str]:
        """
        å–å¾—æœ€æ–°ç‰ˆæœ¬è³‡è¨Šï¼ˆç‰ˆè™Ÿæœ€å¤§çš„ï¼‰
        
        Args:
            sftp_path: SFTP è·¯å¾‘
            
        Returns:
            (db_folder, db_version, full_path)
        """
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            # å¾è·¯å¾‘ä¸­æå–åŸºç¤è³‡è¨Š
            path_parts = sftp_path.rstrip('/').split('/')
            db_folder = path_parts[-1] if path_parts else ''
            
            # åˆ—å‡ºç›®éŒ„å…§å®¹
            try:
                items = self._safe_listdir(sftp_path)
            except:
                self.logger.warning(f"ç„¡æ³•åˆ—å‡ºç›®éŒ„: {sftp_path}")
                return db_folder, '', sftp_path
            
            # éæ¿¾ç‰ˆæœ¬è³‡æ–™å¤¾ï¼ˆæ ¼å¼: æ•¸å­—é–‹é ­ï¼Œå¦‚ 536_all_202507312300ï¼‰
            version_folders = []
            for item in items:
                # æª¢æŸ¥æ˜¯å¦ç‚ºç‰ˆæœ¬è³‡æ–™å¤¾æ ¼å¼
                match = re.match(r'^(\d+)', item)
                if match:
                    version_num = int(match.group(1))
                    version_folders.append((version_num, item))
            
            if not version_folders:
                self.logger.warning(f"æ‰¾ä¸åˆ°ç‰ˆæœ¬è³‡æ–™å¤¾: {sftp_path}")
                return db_folder, '', sftp_path
            
            # æŒ‰ç‰ˆæœ¬è™Ÿæ’åºï¼Œå–æœ€å¤§çš„ï¼ˆæœ€æ–°çš„ï¼‰
            version_folders.sort(key=lambda x: x[0], reverse=True)
            latest_version = version_folders[0][1]
            full_path = f"{sftp_path}/{latest_version}"
            
            self.logger.info(f"æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬: {latest_version} (ç‰ˆè™Ÿ: {version_folders[0][0]}) in {sftp_path}")
            return db_folder, latest_version, full_path
            
        except Exception as e:
            self.logger.error(f"å–å¾—ç‰ˆæœ¬è³‡è¨Šå¤±æ•—: {str(e)}")
            return '', '', sftp_path
    
    def get_specific_version(self, sftp_path: str, db_info: str) -> Tuple[str, str, str]:
        """
        å–å¾—ç‰¹å®šç‰ˆæœ¬è³‡è¨Š
        
        Args:
            sftp_path: SFTP åŸºç¤è·¯å¾‘
            db_info: DBè³‡è¨Š (æ ¼å¼: DB2302#196)
            
        Returns:
            (db_folder, db_version, full_path)
        """
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            # è§£æ db_info
            if '#' not in db_info:
                self.logger.warning(f"DBè³‡è¨Šæ ¼å¼éŒ¯èª¤: {db_info}")
                return self.get_latest_version(sftp_path)
            
            db_number, version_prefix = db_info.split('#')
            
            # åˆ—å‡ºç›®éŒ„æ‰¾åˆ°å°æ‡‰çš„ DB è³‡æ–™å¤¾
            parent_path = '/'.join(sftp_path.rstrip('/').split('/')[:-1])
            items = self._safe_listdir(parent_path)
            
            db_folder = None
            for item in items:
                if item.startswith(f"{db_number}_"):
                    db_folder = item
                    break
            
            if not db_folder:
                self.logger.warning(f"æ‰¾ä¸åˆ° DB è³‡æ–™å¤¾: {db_number}")
                return '', '', sftp_path
            
            # å»ºæ§‹å®Œæ•´è·¯å¾‘
            db_path = f"{parent_path}/{db_folder}"
            
            # åˆ—å‡ºç‰ˆæœ¬è³‡æ–™å¤¾
            version_items = self._safe_listdir(db_path)
            
            # æ‰¾åˆ°å°æ‡‰ç‰ˆæœ¬
            for version_item in version_items:
                if version_item.startswith(f"{version_prefix}_"):
                    full_path = f"{db_path}/{version_item}"
                    self.logger.info(f"æ‰¾åˆ°æŒ‡å®šç‰ˆæœ¬: {version_item}")
                    return db_folder, version_item, full_path
            
            self.logger.warning(f"æ‰¾ä¸åˆ°æŒ‡å®šç‰ˆæœ¬: {version_prefix}")
            return db_folder, '', db_path
            
        except Exception as e:
            self.logger.error(f"å–å¾—ç‰¹å®šç‰ˆæœ¬å¤±æ•—: {str(e)}")
            return '', '', sftp_path
    
    def find_latest_manifest(self, path: str, db_name: str = None, target_version: str = None) -> Optional[Tuple[str, str]]:
        """æ”¹é€²ç‰ˆï¼šå¿«é€Ÿæœå°‹ manifest æª”æ¡ˆï¼ˆå¼·åŒ–éŒ¯èª¤è™•ç†ï¼‰"""
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            if target_version:
                self.logger.info(f"å¿«é€Ÿæœå°‹æŒ‡å®šç‰ˆæœ¬ {target_version}: {path}")
                return self._find_specific_version_manifest(path, target_version)
            else:
                self.logger.info(f"å¿«é€Ÿæœå°‹æœ€æ–°ç‰ˆæœ¬: {path}")
                return self._find_latest_version_manifest(path)
                
        except Exception as e:
            self.logger.error(f"æœç´¢ manifest å¤±æ•—: {e}")
            return None

    def _find_latest_version_manifest(self, base_path: str) -> Optional[Tuple[str, str]]:
        """æœå°‹æœ€æ–°ç‰ˆæœ¬çš„ manifest - å¼·åŒ– type 3 éŒ¯èª¤è™•ç†"""
        try:
            # åˆ—å‡ºç›®éŒ„
            version_dirs = []
            try:
                items = self._safe_listdir_with_details(base_path)
                
                for item_info in items:
                    try:
                        if item_info.get('is_dir', False):
                            dir_name = item_info['name']
                            version_num = self._extract_version_number(dir_name)
                            if version_num:
                                version_dirs.append({
                                    'name': dir_name,
                                    'version': int(version_num),
                                    'mtime': item_info.get('mtime', 0)
                                })
                    except Exception as e:
                        self.logger.debug(f"è·³éå•é¡Œç›®éŒ„: {item_info.get('name', 'unknown')}, éŒ¯èª¤: {e}")
                        continue
                        
            except Exception as e:
                self.logger.error(f"åˆ—å‡ºç›®éŒ„å¤±æ•—: {e}")
                return None
            
            if not version_dirs:
                raise Exception(f"åœ¨ {base_path} ä¸­æ²’æœ‰æ‰¾åˆ°ç‰ˆæœ¬ç›®éŒ„")
            
            # æŒ‰ç‰ˆæœ¬è™Ÿé™åºæ’åˆ—ï¼ˆæœ€æ–°ç‰ˆæœ¬åœ¨å‰ï¼‰
            version_dirs.sort(key=lambda x: (x['version'], x['mtime']), reverse=True)
            self.logger.info(f"æ‰¾åˆ° {len(version_dirs)} å€‹ç‰ˆæœ¬ç›®éŒ„ï¼Œæœ€æ–°ç‰ˆæœ¬: {version_dirs[0]['version']}")
            
            # åªæª¢æŸ¥å‰3å€‹æœ€æ–°ç‰ˆæœ¬
            for version_dir in version_dirs[:3]:
                self.logger.info(f"æª¢æŸ¥ç‰ˆæœ¬ {version_dir['version']} ({version_dir['name']})")
                manifest_path = self._try_get_manifest_from_dir(
                    base_path, 
                    version_dir['name'], 
                    str(version_dir['version'])
                )
                if manifest_path:
                    return manifest_path
            
            raise Exception("åœ¨æœ€æ–°ç‰ˆæœ¬ç›®éŒ„ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ manifest")
            
        except Exception as e:
            self.logger.error(f"æœå°‹æœ€æ–°ç‰ˆæœ¬å¤±æ•—: {e}")
            return None
        
    def _find_specific_version_manifest(self, base_path: str, version: str) -> Optional[Tuple[str, str]]:
        """æœå°‹æŒ‡å®šç‰ˆæœ¬çš„ manifest - å¼·åŒ– type 3 éŒ¯èª¤è™•ç†"""
        try:
            self.logger.info(f"æœå°‹æŒ‡å®šç‰ˆæœ¬: {version}")
            
            # å˜—è©¦åˆ—å‡ºç›®éŒ„ï¼Œå¦‚æœå¤±æ•—å°±ç”¨ç›´æ¥è·¯å¾‘æ–¹å¼
            matching_dirs = []
            try:
                items = self._safe_listdir_with_details(base_path)
                
                for item_info in items:
                    try:
                        # æª¢æŸ¥æ˜¯å¦ç‚ºç›®éŒ„
                        if item_info.get('is_dir', False):
                            dir_name = item_info['name']
                            if self._matches_version(dir_name, version):
                                matching_dirs.append({
                                    'name': dir_name,
                                    'mtime': item_info.get('mtime', 0)
                                })
                                self.logger.debug(f"æ‰¾åˆ°åŒ¹é…ç‰ˆæœ¬ç›®éŒ„: {dir_name}")
                    except Exception as e:
                        self.logger.debug(f"è·³éå•é¡Œé …ç›®: {item_info.get('name', 'unknown')}, éŒ¯èª¤: {e}")
                        continue
                        
            except Exception as e:
                self.logger.warning(f"åˆ—å‡ºç›®éŒ„å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è·¯å¾‘: {e}")
                return self._try_direct_version_paths(base_path, version)
            
            if not matching_dirs:
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦ç›´æ¥æ§‹å»ºè·¯å¾‘
                return self._try_direct_version_paths(base_path, version)
            
            # æŒ‰æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
            matching_dirs.sort(key=lambda x: x['mtime'], reverse=True)
            
            # å˜—è©¦æ¯å€‹åŒ¹é…çš„ç‰ˆæœ¬ç›®éŒ„
            for dir_info in matching_dirs:
                manifest_path = self._try_get_manifest_from_dir(base_path, dir_info['name'], version)
                if manifest_path:
                    return manifest_path
            
            # å¦‚æœåŒ¹é…ç›®éŒ„éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è·¯å¾‘
            return self._try_direct_version_paths(base_path, version)
            
        except Exception as e:
            self.logger.error(f"æœå°‹æŒ‡å®šç‰ˆæœ¬å¤±æ•—: {e}")
            return None

    def _safe_listdir(self, path: str) -> list:
        """æœ€å®‰å…¨çš„åˆ—ç›®éŒ„æ–¹æ³• - åªä½¿ç”¨åŸºæœ¬ listdir"""
        try:
            return self.sftp.listdir(path)
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºç›®éŒ„å¤±æ•—: {path}, éŒ¯èª¤: {e}")
            raise
    
    def _safe_listdir_with_details(self, path: str) -> list:
        """å®‰å…¨çš„åˆ—å‡ºç›®éŒ„å…§å®¹ï¼Œæ ¹æ“šä¼ºæœå™¨èƒ½åŠ›é¸æ“‡æ–¹æ³•"""
        try:
            result = []
            
            # å¦‚æœæ”¯æ´ listdir_attrï¼Œå„ªå…ˆä½¿ç”¨
            if self._server_capabilities.get('supports_listdir_attr', False):
                try:
                    items = self.sftp.listdir_attr(path)
                    for item in items:
                        result.append({
                            'name': item.filename,
                            'is_dir': self._is_directory_from_stat(item),
                            'mtime': getattr(item, 'st_mtime', 0),
                            'size': getattr(item, 'st_size', 0)
                        })
                    return result
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self.logger.warning(f"listdir_attr ä¸æ”¯æ´ï¼Œåˆ‡æ›åˆ°åŸºæœ¬æ¨¡å¼: {e}")
                        self._server_capabilities['supports_listdir_attr'] = False
                    else:
                        raise
            
            # å›é€€åˆ°åŸºæœ¬ listdir + å€‹åˆ¥ stat
            self.logger.debug("ä½¿ç”¨åŸºæœ¬ listdir æ¨¡å¼")
            filenames = self.sftp.listdir(path)
            
            for filename in filenames:
                item_info = {
                    'name': filename,
                    'is_dir': False,  # é è¨­ç‚ºæª”æ¡ˆ
                    'mtime': 0,
                    'size': 0
                }
                
                # å˜—è©¦ç²å–è©³ç´°è³‡è¨Š
                if self._server_capabilities.get('supports_stat', True):
                    try:
                        full_path = f"{path}/{filename}"
                        stat_info = self.sftp.stat(full_path)
                        item_info.update({
                            'is_dir': self._is_directory_from_stat(stat_info),
                            'mtime': getattr(stat_info, 'st_mtime', 0),
                            'size': getattr(stat_info, 'st_size', 0)
                        })
                    except Exception as e:
                        error_msg = str(e).lower()
                        if 'unimplemented' in error_msg or 'type 3' in error_msg:
                            self.logger.debug(f"stat ä¸æ”¯æ´: {filename}")
                            self._server_capabilities['supports_stat'] = False
                        else:
                            self.logger.debug(f"stat å¤±æ•—: {filename}, {e}")
                        
                        # ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„
                        item_info['is_dir'] = self._guess_is_directory(filename)
                else:
                    # ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•
                    item_info['is_dir'] = self._guess_is_directory(filename)
                
                result.append(item_info)
            
            return result
            
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºç›®éŒ„è©³ç´°è³‡è¨Šå¤±æ•—: {path}, éŒ¯èª¤: {e}")
            raise
    
    def _is_directory_from_stat(self, stat_obj) -> bool:
        """å¾ stat å°è±¡åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„"""
        try:
            return bool(stat_obj.st_mode & 0o40000)
        except:
            return False
    
    def _guess_is_directory(self, filename: str) -> bool:
        """å•Ÿç™¼å¼æ–¹æ³•çŒœæ¸¬æ˜¯å¦ç‚ºç›®éŒ„"""
        # å¦‚æœæª”ååŒ…å«æ˜é¡¯çš„å‰¯æª”åï¼Œå¯èƒ½æ˜¯æª”æ¡ˆ
        common_extensions = ['.xml', '.txt', '.log', '.zip', '.tar', '.gz', '.json', '.csv']
        filename_lower = filename.lower()
        
        for ext in common_extensions:
            if filename_lower.endswith(ext):
                return False
        
        # å¦‚æœçœ‹èµ·ä¾†åƒç‰ˆæœ¬ç›®éŒ„æ ¼å¼ï¼Œå‡è¨­æ˜¯ç›®éŒ„
        if re.match(r'^\d+(_all)?(_\d{12})?(_.*)?$', filename):
            return True
        
        # é è¨­å‡è¨­æ˜¯ç›®éŒ„ï¼ˆåœ¨æœå°‹ manifest çš„æƒ…å¢ƒä¸‹æ¯”è¼ƒå®‰å…¨ï¼‰
        return True

    def _try_direct_version_paths(self, base_path: str, version: str) -> Optional[Tuple[str, str]]:
        """ç•¶æ‰¾ä¸åˆ°ç‰ˆæœ¬ç›®éŒ„æ™‚ï¼Œå˜—è©¦ç›´æ¥æ§‹å»ºå¯èƒ½çš„è·¯å¾‘"""
        try:
            import datetime
            today = datetime.datetime.now()
            
            self.logger.info(f"å˜—è©¦ç›´æ¥è·¯å¾‘æœå°‹ç‰ˆæœ¬ {version}")
            
            # ç”Ÿæˆå¯èƒ½çš„æ™‚é–“æˆ³ï¼ˆæœ€è¿‘30å¤©ï¼Œæ¯3å¤©ä¸€å€‹ï¼‰
            for days_back in range(0, 30, 3):
                date = today - datetime.timedelta(days=days_back)
                
                # ç”Ÿæˆå¤šå€‹å¯èƒ½çš„æ™‚é–“æ ¼å¼
                timestamps = [
                    date.strftime("%Y%m%d0000"),  # 202508170000
                    date.strftime("%Y%m%d1200"),  # 202508171200
                    date.strftime("%Y%m%d1800"),  # 202508171800
                    date.strftime("%Y%m%d2300"),  # 202508172300
                ]
                
                for timestamp in timestamps:
                    # å˜—è©¦ä¸åŒçš„ç›®éŒ„æ ¼å¼
                    possible_dirs = [
                        f"{version}_{timestamp}",
                        f"{version}_all_{timestamp}",
                        f"{version}_all_{timestamp}_no-release",
                        f"{version}_all_{timestamp}_backup",
                        f"{version}_{timestamp}_backup",
                    ]
                    
                    for dir_name in possible_dirs:
                        try:
                            manifest_path = self._try_get_manifest_from_dir(base_path, dir_name, version)
                            if manifest_path:
                                self.logger.info(f"âœ… é€éç›´æ¥è·¯å¾‘æ‰¾åˆ°: {dir_name}")
                                return manifest_path
                        except Exception as e:
                            self.logger.debug(f"ç›´æ¥è·¯å¾‘å¤±æ•—: {dir_name}, {e}")
                            continue
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ç›´æ¥è·¯å¾‘æœå°‹å¤±æ•—: {e}")
            return None
            
    def _try_get_manifest_from_dir(self, base_path: str, dir_name: str, version: str) -> Optional[Tuple[str, str]]:
        """å˜—è©¦å¾æŒ‡å®šç›®éŒ„ç²å– manifest - ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹æ³•"""
        try:
            version_path = f"{base_path}/{dir_name}"
            
            # ç›´æ¥æ§‹å»ºå¯èƒ½çš„ manifest æª”æ¡ˆå
            possible_manifests = [
                f"manifest_{version}.xml",
                "manifest.xml",
                f"default_{version}.xml", 
                "default.xml",
                f"{version}.xml"
            ]
            
            for manifest_name in possible_manifests:
                manifest_full_path = f"{version_path}/{manifest_name}"
                
                try:
                    # ä½¿ç”¨æœ€åŸºæœ¬çš„æª¢æŸ¥æ–¹æ³•
                    if self._file_exists_and_valid(manifest_full_path):
                        self.logger.info(f"âœ… æ‰¾åˆ°æœ‰æ•ˆ manifest: {manifest_name} (è·¯å¾‘: {dir_name})")
                        return manifest_full_path, manifest_name
                        
                except Exception as e:
                    self.logger.debug(f"æª¢æŸ¥æª”æ¡ˆå¤±æ•—: {manifest_name}, {e}")
                    continue
            
            return None
            
        except Exception as e:
            self.logger.debug(f"æª¢æŸ¥ç›®éŒ„å¤±æ•—: {dir_name}, {e}")
            return None

    def _file_exists_and_valid(self, file_path: str) -> bool:
        """æœ€åŸºæœ¬çš„æª”æ¡ˆå­˜åœ¨æ€§å’Œæœ‰æ•ˆæ€§æª¢æŸ¥"""
        try:
            # å¦‚æœ stat å¯ç”¨ï¼Œä½¿ç”¨ stat
            if self._server_capabilities.get('supports_stat', True):
                try:
                    stat_info = self.sftp.stat(file_path)
                    file_size = getattr(stat_info, 'st_size', 0)
                    return file_size > 1000  # manifest æª”æ¡ˆæ‡‰è©²å¤§æ–¼ 1KB
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self._server_capabilities['supports_stat'] = False
                    elif 'no such file' in error_msg or 'not found' in error_msg:
                        return False
                    else:
                        raise
            
            # å¦‚æœ stat ä¸å¯ç”¨ï¼Œå˜—è©¦æ‰“é–‹æª”æ¡ˆä¾†æª¢æŸ¥
            try:
                with self.sftp.open(file_path, 'r') as f:
                    # è®€å–å‰å¹¾å€‹å­—ç¯€æª¢æŸ¥æ˜¯å¦åƒ XML
                    header = f.read(100).decode('utf-8', errors='ignore')
                    return '<?xml' in header or '<manifest' in header
            except Exception:
                return False
                
        except Exception:
            return False
                    
    def _matches_version(self, dir_name: str, target_version: str) -> bool:
        """æª¢æŸ¥ç›®éŒ„åæ˜¯å¦åŒ¹é…æŒ‡å®šç‰ˆæœ¬"""
        patterns = [
            rf'^{target_version}_\d{{12}}$',                    # 5_202508170000
            rf'^{target_version}_all_\d{{12}}$',                # 702_all_202508092300  
            rf'^{target_version}_all_\d{{12}}_.*$',             # 669_all_202507142300_no-release
            rf'^{target_version}_\d{{12}}_.*$',                 # 5_202508170000_backup
        ]
        
        for pattern in patterns:
            if re.match(pattern, dir_name):
                return True
        return False
    
    def _extract_version_number(self, name: str) -> Optional[str]:
        """æå–ç‰ˆæœ¬è™Ÿ"""
        patterns = [
            r'^(\d+)_all_\d{12}',      # 702_all_202508092300
            r'^(\d+)_all_\d{12}_',     # 669_all_202507142300_no-release
            r'^(\d+)_\d{12}',          # 5_202508170000
            r'^(\d+)_\d{12}_',         # 5_202508170000_backup
            r'^(\d+)$',                # 5
            r'^v(\d+)',                # v5
        ]
        
        for pattern in patterns:
            match = re.search(pattern, name)
            if match:
                return match.group(1)
        
        return None
    
    def download_file(self, remote_path: str, local_path: str) -> bool:
        """ä¸‹è¼‰æª”æ¡ˆ"""
        try:
            if not self._ensure_connected():
                return False
            
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            filename = os.path.basename(remote_path)
            self.logger.info(f"é–‹å§‹ä¸‹è¼‰: {filename}")
            
            # ç²å–æª”æ¡ˆå¤§å°ï¼ˆå¦‚æœæ”¯æ´çš„è©±ï¼‰
            file_size = 0
            if self._server_capabilities.get('supports_stat', True):
                try:
                    file_stat = self.sftp.stat(remote_path)
                    file_size = file_stat.st_size
                    self.logger.debug(f"æª”æ¡ˆå¤§å°: {file_size} bytes")
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self._server_capabilities['supports_stat'] = False
                        self.logger.debug("stat ä¸æ”¯æ´ï¼Œè·³éæª”æ¡ˆå¤§å°æª¢æŸ¥")
                    else:
                        self.logger.debug(f"ç„¡æ³•ç²å–æª”æ¡ˆå¤§å°: {e}")
            
            # ä¸‹è¼‰æª”æ¡ˆ
            self.sftp.get(remote_path, local_path)
            
            # é©—è­‰ä¸‹è¼‰
            if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                actual_size = os.path.getsize(local_path)
                self.logger.info(f"ä¸‹è¼‰å®Œæˆ: {filename} ({actual_size} bytes)")
                
                # é©—è­‰æª”æ¡ˆå¤§å°æ˜¯å¦ä¸€è‡´ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                if file_size > 0 and actual_size != file_size:
                    self.logger.warning(f"æª”æ¡ˆå¤§å°ä¸ä¸€è‡´: é æœŸ {file_size}, å¯¦éš› {actual_size}")
                
                return True
            else:
                self.logger.error(f"ä¸‹è¼‰çš„æª”æ¡ˆç„¡æ•ˆæˆ–ç‚ºç©º: {local_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {e}")
            return False

# =====================================
# ===== Source Command ç®¡ç†å™¨ =====
# =====================================

class SourceCommandManager:
    """ç®¡ç†ä¸åŒ DB çš„ source command"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.jira_client = JiraAPIClient()
        self.cache = {}
        
    def get_source_command(self, db_info: DBInfo, mapping_df: pd.DataFrame = None) -> Optional[str]:
        """æ ¹æ“š DB è³‡è¨Šç²å–å°æ‡‰çš„ source command"""
        db_name = db_info.db_info
        
        # æª¢æŸ¥å¿«å–
        if db_name in self.cache:
            self.logger.info(f"ä½¿ç”¨å¿«å–çš„ source command for {db_name}")
            return self.cache[db_name]
        
        # å¾ JIRA æœå°‹
        self.logger.info(f"å¾ JIRA æœå°‹ {db_name} çš„ source command")
        ticket_key = self.jira_client.search_db_ticket(db_name, db_info.module)
        if ticket_key:
            # æ›´æ–° jira_link
            db_info.jira_link = f"https://{config_manager.jira_config['site']}/browse/{ticket_key}"
            self.logger.info(f"æ‰¾åˆ° JIRA ticket: {ticket_key}")
            
            cmd = self.jira_client.get_source_command_from_ticket(ticket_key)
            if cmd:
                self.cache[db_name] = cmd
                self.logger.info(f"æˆåŠŸå¾ JIRA ç²å– source command")
                return cmd
        
        self.logger.warning(f"ç„¡æ³•å¾ JIRA ç²å– {db_name} çš„ source command")
        return None
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰å¿«å–"""
        self.cache.clear()

# =====================================
# ===== Mapping Table è®€å–å™¨ =====
# =====================================

class MappingTableReader:
    """è®€å–å’Œè§£æ mapping table çš„é¡åˆ¥"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.df = None
        
    def load_excel(self, file_path: str) -> bool:
        """è¼‰å…¥ Excel æª”æ¡ˆ"""
        try:
            self.df = pd.read_excel(file_path)
            self.logger.info(f"æˆåŠŸè¼‰å…¥ mapping table: {len(self.df)} ç­†è³‡æ–™")
            return True
        except Exception as e:
            self.logger.error(f"è¼‰å…¥ Excel å¤±æ•—: {str(e)}")
            return False
    
    def get_db_info_list(self, db_type: str = 'all') -> List[DBInfo]:
        """å–å¾— DB è³‡è¨Šåˆ—è¡¨"""
        db_list = []
        
        if self.df is None:
            return db_list
        
        type_columns = {
            'master': ('DB_Type', 'DB_Info', 'DB_Folder', 'SftpPath'),
            'premp': ('premp_DB_Type', 'premp_DB_Info', 'premp_DB_Folder', 'premp_SftpPath'),
            'mp': ('mp_DB_Type', 'mp_DB_Info', 'mp_DB_Folder', 'mp_SftpPath'),
            'mpbackup': ('mpbackup_DB_Type', 'mpbackup_DB_Info', 'mpbackup_DB_Folder', 'mpbackup_SftpPath')
        }
        
        types_to_process = type_columns.keys() if db_type == 'all' else [db_type]
        
        for idx, row in self.df.iterrows():
            for dtype in types_to_process:
                if dtype not in type_columns:
                    continue
                    
                cols = type_columns[dtype]
                db_info_col = cols[1]
                
                if db_info_col in row and pd.notna(row[db_info_col]):
                    db_info = DBInfo(
                        sn=row['SN'] if 'SN' in row else idx + 1,
                        module=row['Module'] if 'Module' in row else '',
                        db_type=dtype,
                        db_info=str(row[db_info_col]),
                        db_folder=str(row[cols[2]]) if cols[2] in row and pd.notna(row[cols[2]]) else '',
                        sftp_path=str(row[cols[3]]) if cols[3] in row and pd.notna(row[cols[3]]) else ''
                    )
                    db_list.append(db_info)
        
        return db_list

# =====================================
# ===== Repo ç®¡ç†å™¨ =====
# =====================================

class RepoManager:
    """Repo æŒ‡ä»¤ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.lock = threading.Lock()
    
    def check_repo_exists(self, work_dir: str) -> bool:
        """æª¢æŸ¥ .repo ç›®éŒ„æ˜¯å¦å­˜åœ¨"""
        repo_dir = os.path.join(work_dir, '.repo')
        exists = os.path.exists(repo_dir)
        self.logger.info(f"æª¢æŸ¥ .repo ç›®éŒ„: {work_dir} -> {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
        return exists
    
    def run_command(self, cmd: str, cwd: str = None, timeout: int = None) -> Tuple[bool, str]:
        """åŒæ­¥åŸ·è¡ŒæŒ‡ä»¤"""
        try:
            self.logger.debug(f"åŸ·è¡ŒæŒ‡ä»¤: {cmd}")
            result = subprocess.run(
                cmd,
                shell=True,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            if result.returncode == 0:
                self.logger.debug(f"æŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
                return True, result.stdout
            else:
                self.logger.warning(f"æŒ‡ä»¤åŸ·è¡Œå¤±æ•— (è¿”å›ç¢¼: {result.returncode})")
                return False, result.stderr
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"æŒ‡ä»¤åŸ·è¡Œè¶…æ™‚")
            return False, "Command timeout"
        except Exception as e:
            self.logger.error(f"æŒ‡ä»¤åŸ·è¡Œç•°å¸¸: {e}")
            return False, str(e)
    
    def repo_init(self, work_dir: str, init_cmd: str) -> bool:
        """åŸ·è¡Œ repo init - å¢å¼·æ¸…ç†å’ŒéŒ¯èª¤æ¢å¾©"""
        # ğŸ”¥ å¾¹åº•æ¸…ç†å¯èƒ½å­˜åœ¨çš„èˆŠ .repo ç›®éŒ„å’Œç›¸é—œæª”æ¡ˆ
        repo_dir = os.path.join(work_dir, '.repo')
        if os.path.exists(repo_dir):
            self.logger.info(f"ç™¼ç¾èˆŠçš„ .repo ç›®éŒ„ï¼ŒåŸ·è¡Œå¾¹åº•æ¸…ç†: {repo_dir}")
            import shutil
            try:
                # å…ˆå˜—è©¦æ­£å¸¸åˆªé™¤
                shutil.rmtree(repo_dir)
                self.logger.info("âœ… èˆŠ .repo ç›®éŒ„æ¸…ç†æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"æ­£å¸¸æ¸…ç†å¤±æ•—ï¼Œå˜—è©¦å¼·åˆ¶æ¸…ç†: {e}")
                # å¼·åˆ¶æ¸…ç†
                try:
                    import subprocess
                    if os.name == 'posix':  # Linux/Unix
                        subprocess.run(['rm', '-rf', repo_dir], check=True)
                        self.logger.info("âœ… å¼·åˆ¶æ¸…ç†æˆåŠŸ")
                    else:
                        raise Exception("ç„¡æ³•å¼·åˆ¶æ¸…ç†")
                except Exception as e2:
                    self.logger.error(f"âŒ å¼·åˆ¶æ¸…ç†ä¹Ÿå¤±æ•—: {e2}")
                    return False
        
        # ğŸ”¥ æ¸…ç†å¯èƒ½å­˜åœ¨çš„å…¶ä»–ç›¸é—œæª”æ¡ˆ
        cleanup_patterns = [
            '.repo_*',
            'repo_*',
            '.repopickle_*'
        ]
        
        for pattern in cleanup_patterns:
            try:
                import glob
                for item in glob.glob(os.path.join(work_dir, pattern)):
                    if os.path.isfile(item):
                        os.remove(item)
                    elif os.path.isdir(item):
                        shutil.rmtree(item)
                    self.logger.debug(f"æ¸…ç†: {item}")
            except Exception as e:
                self.logger.debug(f"æ¸…ç† {pattern} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # ğŸ”¥ å¢åŠ é‡è©¦æ©Ÿåˆ¶å’Œæ›´é•·çš„è¶…æ™‚æ™‚é–“
        max_retries = 3
        timeout_values = [180, 300, 600]  # 3åˆ†é˜ã€5åˆ†é˜ã€10åˆ†é˜
        for attempt in range(max_retries):
            timeout = timeout_values[attempt]
            self.logger.info(f"åŸ·è¡Œ repo init (å˜—è©¦ {attempt + 1}/{max_retries}ï¼Œè¶…æ™‚: {timeout}ç§’): {init_cmd}")
            
            success, output = self.run_command(
                init_cmd,
                cwd=work_dir,
                timeout=timeout
            )
            
            if success:
                self.logger.info("âœ… Repo init åŸ·è¡ŒæˆåŠŸ")
                return True
            else:
                self.logger.warning(f"âŒ Repo init å˜—è©¦ {attempt + 1} å¤±æ•—: {output}")
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæ¸…ç†ä¸¦é‡è©¦
                if attempt < max_retries - 1:
                    self.logger.info(f"æº–å‚™é‡è©¦ï¼Œå…ˆæ¸…ç†ç’°å¢ƒ...")
                    repo_dir = os.path.join(work_dir, '.repo')
                    if os.path.exists(repo_dir):
                        try:
                            import shutil
                            shutil.rmtree(repo_dir)
                        except:
                            pass
                    time.sleep(15)  # ç­‰å¾… 15 ç§’å¾Œé‡è©¦
        
        self.logger.error("âŒ Repo init æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
        return False
    
    def apply_manifest(self, work_dir: str, manifest_file: str) -> bool:
        """æ‡‰ç”¨ manifest æª”æ¡ˆ - æ”¹é€²ç‰ˆæœ¬"""
        try:
            manifest_name = os.path.basename(manifest_file)
            self.logger.info(f"æ‡‰ç”¨ manifest: {manifest_name}")
            
            repo_dir = os.path.join(work_dir, '.repo')
            manifests_dir = os.path.join(repo_dir, 'manifests')
            
            if not os.path.exists(manifests_dir):
                self.logger.error(f"Manifests ç›®éŒ„ä¸å­˜åœ¨: {manifests_dir}")
                return False
            
            # è¤‡è£½ manifest æª”æ¡ˆ
            dest_file = os.path.join(manifests_dir, manifest_name)
            import shutil
            shutil.copy2(manifest_file, dest_file)
            self.logger.debug(f"è¤‡è£½ manifest: {manifest_file} -> {dest_file}")
            
            # ğŸ”¥ ä¿®æ”¹ï¼šå¢åŠ è¶…æ™‚æ™‚é–“ä¸¦æ·»åŠ é‡è©¦æ©Ÿåˆ¶
            switch_cmd = f"{config_manager.repo_config['repo_command']} init -m {manifest_name}"
            self.logger.info(f"åˆ‡æ› manifest: {switch_cmd}")
            
            # ğŸ”¥ å˜—è©¦å¤šæ¬¡ï¼Œå¢åŠ è¶…æ™‚æ™‚é–“
            max_attempts = 3
            timeout_values = [120, 180, 300]  # 2åˆ†é˜ã€3åˆ†é˜ã€5åˆ†é˜
            
            for attempt in range(max_attempts):
                timeout = timeout_values[attempt]
                self.logger.info(f"å˜—è©¦ {attempt + 1}/{max_attempts}ï¼Œè¶…æ™‚è¨­å®š: {timeout}ç§’")
                
                success, output = self.run_command(
                    switch_cmd,
                    cwd=work_dir,
                    timeout=timeout
                )
                
                if success:
                    self.logger.info(f"âœ… æˆåŠŸåˆ‡æ›åˆ° manifest: {manifest_name}")
                    return True
                else:
                    self.logger.warning(f"âŒ å˜—è©¦ {attempt + 1} å¤±æ•—: {output}")
                    
                    # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡ï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è©¦
                    if attempt < max_attempts - 1:
                        import time
                        wait_time = 10 * (attempt + 1)  # 10ç§’ã€20ç§’
                        self.logger.info(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
            
            self.logger.error(f"âŒ æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œç„¡æ³•åˆ‡æ› manifest")
            return False
            
        except Exception as e:
            self.logger.error(f"æ‡‰ç”¨ manifest å¤±æ•—: {str(e)}")
            return False
    
    def start_repo_sync_async(self, work_dir: str, db_name: str) -> subprocess.Popen:
        """ğŸ¯ ä¿®å¾©ç‰ˆæœ¬ - ä½¿ç”¨ unbuffer ç¢ºä¿å¯¦æ™‚è¼¸å‡º"""
        try:
            self.logger.info(f"{db_name}: å•Ÿå‹• unbuffer ç‰ˆæœ¬ repo sync")
            
            # æª¢æŸ¥å·¥ä½œç›®éŒ„
            if not os.path.exists(os.path.join(work_dir, '.repo')):
                raise Exception(f"å·¥ä½œç›®éŒ„æ²’æœ‰ .repo: {work_dir}")
            
            # æª¢æŸ¥ unbuffer æ˜¯å¦å¯ç”¨
            try:
                subprocess.run(['which', 'unbuffer'], check=True, capture_output=True, timeout=5)
                use_unbuffer = True
            except:
                use_unbuffer = False
                self.logger.warning(f"{db_name}: unbuffer ä¸å¯ç”¨ï¼Œä½¿ç”¨ script æ–¹æ³•")
            
            # å»ºç«‹æ—¥èªŒ
            log_dir = os.path.join(work_dir, 'logs')
            os.makedirs(log_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            method_name = "unbuffer" if use_unbuffer else "script"
            log_file = os.path.join(log_dir, f'repo_sync_{method_name}_{timestamp}.log')
            
            # å¯«å…¥åˆå§‹ä¿¡æ¯
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"=== Fixed Repo Sync Log for {db_name} ===\n")
                f.write(f"é–‹å§‹æ™‚é–“: {datetime.now()}\n")
                f.write(f"å·¥ä½œç›®éŒ„: {work_dir}\n")
                f.write(f"ä½¿ç”¨æ–¹æ³•: {method_name}\n\n")
            
            # æº–å‚™å‘½ä»¤
            repo_cmd = config_manager.repo_config['repo_command']
            jobs = min(config_manager.repo_config['sync_jobs'], 4)
            
            if use_unbuffer:
                # ğŸ¯ ä½¿ç”¨ unbuffer æ–¹æ³•ï¼ˆå·²é©—è­‰æœ‰æ•ˆï¼‰
                cmd_parts = [
                    'unbuffer',
                    repo_cmd, 'sync', 
                    f'-j{jobs}', 
                    '--verbose', 
                    '--force-sync'
                ]
                
                process = subprocess.Popen(
                    cmd_parts,
                    cwd=work_dir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=0,
                    preexec_fn=os.setsid
                )
            else:
                # å‚™ç”¨ï¼šä½¿ç”¨ script æ–¹æ³•
                shell_cmd = f"""
    cd "{work_dir}"
    echo "[SCRIPT] é€²ç¨‹å•Ÿå‹•ï¼ŒPID: $$" >> "{log_file}"
    script -fq /dev/null -c "{repo_cmd} sync -j{jobs} --verbose --force-sync" | tee -a "{log_file}"
    echo "[SCRIPT] é€²ç¨‹çµæŸï¼Œæ™‚é–“: $(date)" >> "{log_file}"
    """
                
                process = subprocess.Popen(
                    ['bash', '-c', shell_cmd],
                    cwd=work_dir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
            
            # ğŸ¯ å‰µå»ºå¯¦æ™‚æ—¥èªŒå¯«å…¥ç·šç¨‹ï¼ˆé—œéµæ”¹é€²ï¼‰
            def log_writer():
                try:
                    with open(log_file, 'a', encoding='utf-8', buffering=1) as f:
                        f.write(f"[UNBUFFER] é€²ç¨‹å•Ÿå‹•ï¼ŒPID: {process.pid}\n")
                        f.write(f"[UNBUFFER] é–‹å§‹æ™‚é–“: {datetime.now()}\n\n")
                        f.flush()
                        
                        # ğŸ”¥ é€²åº¦è¿½è¹¤è®Šæ•¸
                        last_reported_progress = -1
                        last_report_time = datetime.now()
                        message_count = 0
                        
                        if use_unbuffer:
                            while True:
                                line = process.stdout.readline()
                                if line:
                                    # ğŸ“ æ‰€æœ‰å…§å®¹éƒ½å¯«å…¥æ–‡ä»¶ï¼ˆä¸è®Šï¼‰
                                    f.write(line)
                                    f.flush()
                                    
                                    # ğŸ”¥ æ™ºèƒ½éæ¿¾ console è¼¸å‡º
                                    line_clean = line.strip()
                                    message_count += 1
                                    
                                    # âœ… åªå ±å‘Šé‡è¦çš„é€²åº¦è®ŠåŒ–
                                    if "Syncing:" in line_clean and "%" in line_clean:
                                        import re
                                        progress_match = re.search(r'Syncing:\s*(\d+)%\s*\((\d+)/(\d+)\)', line_clean)
                                        if progress_match:
                                            current_progress = int(progress_match.group(1))
                                            current_count = int(progress_match.group(2))
                                            total_count = int(progress_match.group(3))
                                            
                                            # ğŸ¯ åªåœ¨ä»¥ä¸‹æƒ…æ³å ±å‘Šé€²åº¦ï¼š
                                            should_report = (
                                                last_reported_progress == -1 or  # ç¬¬ä¸€æ¬¡
                                                current_progress - last_reported_progress >= 5 or  # é€²åº¦å¢åŠ 5%ä»¥ä¸Š
                                                current_progress % 10 == 0 or  # æ¯10%é‡Œç¨‹ç¢‘
                                                current_progress >= 95  # æ¥è¿‘å®Œæˆæ™‚
                                            )
                                            
                                            if should_report:
                                                # è¨ˆç®—é€Ÿåº¦
                                                current_time = datetime.now()
                                                elapsed = (current_time - last_report_time).total_seconds()
                                                
                                                speed_info = ""
                                                if last_reported_progress > 0 and elapsed > 0:
                                                    progress_diff = current_progress - last_reported_progress
                                                    speed = progress_diff / (elapsed / 60)  # %/åˆ†é˜
                                                    if speed > 0:
                                                        remaining_time = (100 - current_progress) / speed
                                                        speed_info = f" (é è¨ˆå‰©é¤˜: {remaining_time:.0f}åˆ†é˜)"
                                                
                                                # ğŸ“Š ç°¡æ½”çš„é€²åº¦å ±å‘Š
                                                # self.logger.info(
                                                #    f"{db_name}: {current_progress}% "
                                                #    f"({current_count}/{total_count}){speed_info}"
                                                #)
                                                
                                                last_reported_progress = current_progress
                                                last_report_time = current_time
                                    
                                    # âš ï¸ å ±å‘ŠéŒ¯èª¤å’Œè­¦å‘Š
                                    elif any(keyword in line_clean.lower() for keyword in 
                                        ['error:', 'fatal:', 'failed', 'timeout', 'exception', 'abort']):
                                        self.logger.warning(f"{db_name}: {line_clean}")
                                    
                                    # ğŸ‰ å ±å‘Šé‡è¦é‡Œç¨‹ç¢‘
                                    elif any(phrase in line_clean.lower() for phrase in
                                        ['sync has finished', 'completed successfully', 'repo sync complete']):
                                        self.logger.info(f"{db_name}: åŒæ­¥å®Œæˆï¼")
                                    
                                    # ğŸš« ä¸å ±å‘Šçš„å…§å®¹ï¼š
                                    # - "Skipped fetching project"
                                    # - "fetching project" 
                                    # - "..working.."
                                    # - é‡è¤‡çš„é€²åº¦ä¿¡æ¯
                                    
                                elif process.poll() is not None:
                                    break
                        
                        return_code = process.poll()
                        f.write(f"\n[UNBUFFER] é€²ç¨‹çµæŸï¼Œè¿”å›ç¢¼: {return_code}\n")
                        f.write(f"[UNBUFFER] çµæŸæ™‚é–“: {datetime.now()}\n")
                        f.write(f"[UNBUFFER] ç¸½è™•ç†æ¶ˆæ¯æ•¸: {message_count}\n")
                        f.flush()
                        
                        # ğŸ“ˆ æœ€çµ‚å ±å‘Š
                        if return_code == 0:
                            self.logger.info(f"{db_name}: âœ… åŒæ­¥æˆåŠŸå®Œæˆ")
                        else:
                            self.logger.error(f"{db_name}: âŒ åŒæ­¥å¤±æ•— (è¿”å›ç¢¼: {return_code})")
                        
                except Exception as e:
                    self.logger.error(f"{db_name}: æ—¥èªŒå¯«å…¥éŒ¯èª¤: {e}")
            
            # å•Ÿå‹•æ—¥èªŒç·šç¨‹
            if use_unbuffer:  # åªæœ‰ unbuffer éœ€è¦æ—¥èªŒç·šç¨‹
                log_thread = threading.Thread(target=log_writer, daemon=True)
                log_thread.start()
                process._log_thread = log_thread
            
            # é©—è­‰å•Ÿå‹•
            time.sleep(2)
            if process.poll() is not None:
                raise Exception(f"é€²ç¨‹ç«‹å³å¤±æ•—ï¼Œè¿”å›ç¢¼: {process.poll()}")
            
            # ä¿å­˜é€²ç¨‹ä¿¡æ¯
            process._log_file_path = log_file
            process._db_name = db_name
            process._start_time = datetime.now()
            
            resource_manager.register_process(db_name, process)
            self.logger.info(f"{db_name}: repo sync å•Ÿå‹•æˆåŠŸ (PID: {process.pid})")
            
            return process
            
        except Exception as e:
            self.logger.error(f"{db_name}: å•Ÿå‹•å¤±æ•—: {e}")
            return None
    
    def check_process_status(self, db_name: str, process: subprocess.Popen) -> Optional[int]:
        """æ”¹é€²çš„é€²ç¨‹ç‹€æ…‹æª¢æŸ¥"""
        with self.lock:
            if process:
                try:
                    poll = process.poll()
                    
                    # ğŸ”¥ åŠ å¼·æ—¥èªŒ
                    self.logger.debug(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹ PID={process.pid}, poll={poll}")
                    
                    if poll is not None:
                        # ğŸ”¥ é€²ç¨‹å·²çµæŸï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯
                        self.logger.info(f"{db_name}: é€²ç¨‹å·²çµæŸï¼Œè¿”å›ç¢¼={poll}")
                        
                        # æ­£ç¢ºé—œé–‰æ–‡ä»¶å¥æŸ„
                        if hasattr(process, '_log_file_handle') and process._log_file_handle:
                            try:
                                process._log_file_handle.flush()
                                process._log_file_handle.close()
                                self.logger.debug(f"{db_name}: æ—¥èªŒæ–‡ä»¶å¥æŸ„å·²é—œé–‰")
                            except Exception as e:
                                self.logger.warning(f"{db_name}: é—œé–‰æ—¥èªŒå¥æŸ„å¤±æ•—: {e}")
                        
                        resource_manager.unregister_process(db_name)
                        
                        # ğŸ”¥ å¯«å…¥å®Œæˆæ¨™è¨˜åˆ°æ—¥èªŒ
                        if hasattr(process, '_log_file_path'):
                            try:
                                with open(process._log_file_path, 'a') as f:
                                    f.write(f"\n=== é€²ç¨‹çµæŸ ===\n")
                                    f.write(f"è¿”å›ç¢¼: {poll}\n")
                                    f.write(f"çµæŸæ™‚é–“: {datetime.now()}\n")
                            except:
                                pass
                        
                        return poll
                    else:
                        # ğŸ”¥ é€²ç¨‹ä»åœ¨é‹è¡Œï¼Œä½†é©—è­‰ PID æ˜¯å¦çœŸçš„å­˜åœ¨
                        try:
                            os.kill(process.pid, 0)  # æª¢æŸ¥é€²ç¨‹æ˜¯å¦å­˜åœ¨
                            self.logger.debug(f"{db_name}: é€²ç¨‹ {process.pid} ç¢ºå¯¦åœ¨é‹è¡Œ")
                        except OSError:
                            self.logger.warning(f"{db_name}: é€²ç¨‹ {process.pid} ä¸å­˜åœ¨ä½† poll() è¿”å› None")
                            return -1  # é€²ç¨‹ç•°å¸¸æ¶ˆå¤±
                    
                    return None
                    
                except Exception as e:
                    self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
                    return -1
            
            return None
    
    def export_manifest(self, work_dir: str, output_file: str = "vp_manifest.xml") -> bool:
        """å°å‡º manifest - å¢åŠ è¶…æ™‚æ™‚é–“"""
        cmd = f"{config_manager.repo_config['repo_command']} manifest -r -o {output_file}"
        self.logger.info(f"å°å‡º manifest: {cmd}")
        
        # ğŸ”¥ å¢åŠ è¶…æ™‚æ™‚é–“åˆ° 5 åˆ†é˜ï¼Œä¸¦æ·»åŠ é‡è©¦æ©Ÿåˆ¶
        max_attempts = 3
        timeout_values = [180, 300, 600]  # 3åˆ†é˜ã€5åˆ†é˜ã€10åˆ†é˜
        
        for attempt in range(max_attempts):
            timeout = timeout_values[attempt]
            self.logger.info(f"å°å‡º manifest å˜—è©¦ {attempt + 1}/{max_attempts}ï¼Œè¶…æ™‚è¨­å®š: {timeout}ç§’")
            
            success, output = self.run_command(cmd, cwd=work_dir, timeout=timeout)
            
            if success:
                output_path = os.path.join(work_dir, output_file)
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    self.logger.info(f"âœ… æˆåŠŸå°å‡º manifest: {output_path} ({file_size} bytes)")
                    return True
                else:
                    self.logger.warning(f"âŒ å°å‡ºæª”æ¡ˆä¸å­˜åœ¨: {output_path}")
            else:
                self.logger.warning(f"âŒ å°å‡ºå˜—è©¦ {attempt + 1} å¤±æ•—: {output}")
                
                # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡ï¼Œç­‰å¾…å¾Œé‡è©¦
                if attempt < max_attempts - 1:
                    wait_time = 10 * (attempt + 1)
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
        
        self.logger.error(f"âŒ æ‰€æœ‰å°å‡ºå˜—è©¦éƒ½å¤±æ•—")
        return False

# =====================================
# ===== ä¸»è¦è™•ç†é¡åˆ¥ =====
# =====================================

class ManifestPinningTool:
    """Manifest å®šç‰ˆå·¥å…·ï¼ˆæ”¹é€²ç‰ˆ + é›¶å¤±æ•—æ©Ÿåˆ¶ï¼‰"""

    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.repo_manager = RepoManager()
        self.mapping_reader = MappingTableReader()
        self.source_cmd_manager = SourceCommandManager()
        self.report = PinningReport()
        self.output_dir = config_manager.path_config['default_output_dir']
        self.dry_run = False
        self.zero_fail_mode = False  # ğŸ”¥ é›¶å¤±æ•—æ¨¡å¼é–‹é—œ
        
        # ç·šç¨‹å®‰å…¨é–
        self._sftp_lock = threading.Lock()

    def _monitor_failure_rate_and_auto_enable_zero_fail(self, active_syncs: List[DBInfo]):
        """ç›£æ§å¤±æ•—ç‡ä¸¦è‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼"""
        if self.zero_fail_mode:
            return  # å·²ç¶“å•Ÿç”¨äº†
        
        total_dbs = len(active_syncs)
        failed_dbs = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        
        if total_dbs > 0:
            failure_rate = (failed_dbs / total_dbs) * 100
            
            # ğŸ”¥ ç•¶å¤±æ•—ç‡è¶…é 30% æ™‚è‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
            if failure_rate >= 30.0:
                self.logger.warning(f"ğŸš¨ å¤±æ•—ç‡é”åˆ° {failure_rate:.1f}%ï¼Œè‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
                self._enable_zero_fail_mode_dynamically()
                
                # ç«‹å³è™•ç†æ‰€æœ‰å¤±æ•—çš„ DB
                self._rescue_failed_dbs_immediately(active_syncs)
                
                return True
        
        return False
        
    def _enable_zero_fail_mode_dynamically(self):
        """é‹è¡Œæ™‚å‹•æ…‹å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼"""
        if not self.zero_fail_mode:
            self.zero_fail_mode = True
            self.logger.warning("ğŸ¯ é›¶å¤±æ•—æ¨¡å¼å·²å‹•æ…‹å•Ÿç”¨ - ä¸å…è¨±ä»»ä½• repo sync å¤±æ•—")
            
            # é€šçŸ¥é›¶å¤±æ•—æ¨¡å¼å·²å•Ÿç”¨
            print("\n" + "="*80)
            print("ğŸš¨ é›¶å¤±æ•—æ¨¡å¼å·²å‹•æ…‹å•Ÿç”¨")
            print("ğŸ“‹ æ¥ä¸‹ä¾†å°‡åŸ·è¡Œä»¥ä¸‹ç­–ç•¥ï¼š")
            print("   â€¢ è‡ªå‹•ä¿®å¾©æ‰€æœ‰å¤±æ•—çš„é …ç›®")
            print("   â€¢ ä½¿ç”¨å¤šå±¤æ•‘æ´ç­–ç•¥")
            print("   â€¢ å¿…è¦æ™‚åŸ·è¡Œæ ¸æ­¦ç´šé‡å»º")
            print("   â€¢ ä¸å…è¨±ä»»ä½• DB æœ€çµ‚å¤±æ•—")
            print("="*80)
    
    def _notify_zero_fail_mode_enabled(self):
        """é€šçŸ¥é›¶å¤±æ•—æ¨¡å¼å·²å•Ÿç”¨"""
        print("\n" + "="*60)
        print("ğŸš¨ é›¶å¤±æ•—æ¨¡å¼å·²å‹•æ…‹å•Ÿç”¨")
        print("ğŸ“‹ æ¥ä¸‹ä¾†å°‡åŸ·è¡Œä»¥ä¸‹ç­–ç•¥ï¼š")
        print("   â€¢ è‡ªå‹•ä¿®å¾©æ‰€æœ‰å¤±æ•—çš„é …ç›®")
        print("   â€¢ ä½¿ç”¨å¤šå±¤æ•‘æ´ç­–ç•¥")
        print("   â€¢ å¿…è¦æ™‚åŸ·è¡Œæ ¸æ­¦ç´šé‡å»º")
        print("   â€¢ ä¸å…è¨±ä»»ä½• DB æœ€çµ‚å¤±æ•—")
        print("="*60)
    
    def _rescue_failed_dbs_immediately(self, active_syncs: List[DBInfo]):
        """ç«‹å³æ¶æ•‘æ‰€æœ‰å¤±æ•—çš„ DB"""
        failed_dbs = [db for db in active_syncs if db.status == DBStatus.FAILED]
        
        if failed_dbs:
            self.logger.warning(f"ğŸš¨ ç«‹å³æ¶æ•‘ {len(failed_dbs)} å€‹å¤±æ•—çš„ DB")
            
            for db_info in failed_dbs:
                self.logger.info(f"{db_info.db_info}: ğŸ› ï¸ é–‹å§‹é›¶å¤±æ•—æ•‘æ´")
                
                # åˆ†æå¤±æ•—åŸå› ä¸¦ä¿®å¾©
                success_rate, failed_projects = self._analyze_sync_result(db_info)
                if failed_projects:
                    if self._enhanced_repair_failed_projects_zero_tolerance(db_info, failed_projects):
                        db_info.status = DBStatus.SUCCESS
                        db_info.end_time = datetime.now()
                        self.logger.info(f"{db_info.db_info}: âœ… é›¶å¤±æ•—æ•‘æ´æˆåŠŸ")
                    else:
                        self.logger.warning(f"{db_info.db_info}: âš ï¸ æ¨™æº–æ•‘æ´å¤±æ•—ï¼Œæº–å‚™æ ¸æ­¦ç´šé‡å»º")
                            
    def enable_zero_fail_mode(self):
        """å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼"""
        self.zero_fail_mode = True
        self.logger.info("ğŸ¯ é›¶å¤±æ•—æ¨¡å¼å·²å•Ÿç”¨ - ä¸å…è¨±ä»»ä½• repo sync å¤±æ•—")

    def load_mapping_table(self, file_path: str) -> bool:
        """è¼‰å…¥ mapping table"""
        return self.mapping_reader.load_excel(file_path)

    def get_all_dbs(self, db_type: str = 'all') -> List[DBInfo]:
        """å–å¾—æ‰€æœ‰ DB è³‡è¨Š"""
        return self.mapping_reader.get_db_info_list(db_type)

    def process_db_phase1(self, db_info: DBInfo) -> DBInfo:
        """æ”¹é€²ç‰ˆ Phase 1 è™•ç† - ç·šç¨‹å®‰å…¨"""
        db_info.start_time = datetime.now()
        local_sftp_manager = None
        
        try:
            self.logger.info(f"é–‹å§‹è™•ç† {db_info.db_info} (Phase 1)")
            
            # å»ºç«‹æœ¬åœ°ç›®éŒ„
            local_path = os.path.join(self.output_dir, db_info.module, db_info.db_info)
            os.makedirs(local_path, exist_ok=True)
            db_info.local_path = local_path
            
            # æª¢æŸ¥ç£ç¢Ÿç©ºé–“ï¼ˆè‡³å°‘éœ€è¦ 15GBï¼Œå› ç‚ºè¶…æ™‚å•é¡Œå¯èƒ½æ˜¯ç©ºé–“ä¸è¶³ï¼‰
            import shutil
            free_space = shutil.disk_usage(local_path).free
            required_space = 15 * 1024 * 1024 * 1024  # 15GB
            
            if free_space < required_space:
                raise Exception(f"ç£ç¢Ÿç©ºé–“ä¸è¶³: å¯ç”¨ {free_space/1024/1024/1024:.1f}GBï¼Œå»ºè­°è‡³å°‘ 15GB")
            
            self.logger.debug(f"{db_info.db_info}: ç£ç¢Ÿç©ºé–“æª¢æŸ¥é€šé ({free_space/1024/1024/1024:.1f}GB å¯ç”¨)")
            
            # Step 1: SFTP æ“ä½œ
            with self._sftp_lock:
                self.logger.info(f"{db_info.db_info}: å¿«é€Ÿæœå°‹ manifest (ç·šç¨‹å®‰å…¨)")
                
                local_sftp_manager = SFTPManager()
                
                if not local_sftp_manager.connect():
                    raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
                
                target_version = db_info.version
                result = local_sftp_manager.find_latest_manifest(
                    db_info.sftp_path, 
                    db_info.db_info,
                    target_version
                )
                
                if not result:
                    raise Exception("æ‰¾ä¸åˆ° manifest æª”æ¡ˆ")
                
                manifest_full_path, manifest_name = result
                db_info.manifest_full_path = manifest_full_path
                db_info.manifest_file = manifest_name
                
                if not db_info.version:
                    match = re.match(r'manifest_(\d+)\.xml', manifest_name)
                    if match:
                        db_info.version = match.group(1)
                        self.logger.info(f"{db_info.db_info}: æª¢æ¸¬åˆ°ç‰ˆæœ¬ {db_info.version}")
                
                local_manifest = os.path.join(local_path, manifest_name)
                if not local_sftp_manager.download_file(manifest_full_path, local_manifest):
                    raise Exception("ä¸‹è¼‰ manifest å¤±æ•—")
                
                self.logger.info(f"{db_info.db_info}: manifest ä¸‹è¼‰å®Œæˆ: {manifest_name}")
                
                local_sftp_manager.disconnect()
                self.logger.info(f"{db_info.db_info}: âœ… SFTP é€£ç·šå·²ç«‹å³æ–·é–‹")
                local_sftp_manager = None
            
            # Step 2: æª¢æŸ¥ repo ç‹€æ…‹
            db_info.has_existing_repo = self.repo_manager.check_repo_exists(local_path)
            
            # Step 3: ç²å– source command
            self.logger.info(f"{db_info.db_info}: ç²å– source command")
            source_cmd = self.source_cmd_manager.get_source_command(db_info, self.mapping_reader.df)
            if not source_cmd:
                raise Exception("ç„¡æ³•å–å¾— source command")
            
            db_info.actual_source_cmd = source_cmd
            self.logger.info(f"{db_info.db_info}: source command ç²å–æˆåŠŸ")
            
            # Step 4: åŸ·è¡Œ repo initï¼ˆå¦‚æœéœ€è¦ï¼‰
            self.logger.info(f"{db_info.db_info}: åŸ·è¡Œ repo åˆå§‹åŒ–")
            if not db_info.has_existing_repo:
                self.logger.info(f"{db_info.db_info}: .repo ç›®éŒ„ä¸å­˜åœ¨ï¼ŒåŸ·è¡Œå®Œæ•´ repo init")
                if not self.repo_manager.repo_init(local_path, source_cmd):
                    raise Exception("Repo init å¤±æ•—")
            else:
                self.logger.info(f"{db_info.db_info}: .repo ç›®éŒ„å­˜åœ¨ï¼Œè·³é repo init")
            
            # ğŸ”¥ Step 5: æ‡‰ç”¨ manifestï¼ˆé€™æ˜¯é—œéµæ­¥é©Ÿï¼‰
            self.logger.info(f"{db_info.db_info}: é–‹å§‹æ‡‰ç”¨ manifestï¼ˆå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰")
            if not self.repo_manager.apply_manifest(local_path, local_manifest):
                raise Exception("å¥—ç”¨ manifest å¤±æ•—")
            
            self.logger.info(f"{db_info.db_info}: âœ… repo åˆå§‹åŒ–å®Œæˆ")
            
            # Step 6: å•Ÿå‹• repo sync
            if not self.dry_run:
                self.logger.info(f"{db_info.db_info}: å•Ÿå‹• repo sync")
                process = self.repo_manager.start_repo_sync_async(local_path, db_info.db_info)
                if not process:
                    raise Exception("å•Ÿå‹• repo sync å¤±æ•—")
                
                db_info.sync_process = process
                self.logger.info(f"{db_info.db_info}: repo sync å·²å•Ÿå‹• (PID: {process.pid})")
            else:
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                self.logger.info(f"{db_info.db_info}: æ¸¬è©¦æ¨¡å¼ - è·³é repo sync")
            
            self.logger.info(f"{db_info.db_info}: âœ… Phase 1 å®Œæˆ")
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            db_info.end_time = datetime.now()
            self.logger.error(f"{db_info.db_info}: âŒ Phase 1 å¤±æ•— - {str(e)}")
        finally:
            # ç¢ºä¿ SFTP é€£ç·šä¸€å®šæœƒè¢«æ–·é–‹
            if local_sftp_manager:
                try:
                    local_sftp_manager.disconnect()
                    self.logger.debug(f"{db_info.db_info}: ç¢ºä¿ SFTP é€£ç·šå·²æ–·é–‹")
                except:
                    pass
        
        return db_info

    def process_db_phase2(self, db_info: DBInfo) -> DBInfo:
        """è™•ç† DB çš„ç¬¬äºŒéšæ®µï¼šå®Œæˆå·¥ä½œ - æ”¯æ´éƒ¨åˆ†æˆåŠŸ"""
        if self.zero_fail_mode:
            return self.process_db_phase2_zero_fail(db_info)
        
        try:
            self.logger.info(f"{db_info.db_info}: é–‹å§‹ Phase 2")
            
            if self.dry_run:
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                self.logger.info(f"{db_info.db_info}: æ¸¬è©¦æ¨¡å¼å®Œæˆ")
                return db_info
            
            # æª¢æŸ¥ sync é€²ç¨‹ç‹€æ…‹
            sync_result = None
            if db_info.sync_process:
                poll = self.repo_manager.check_process_status(
                    db_info.db_info, 
                    db_info.sync_process
                )
                
                if poll is None:
                    self.logger.debug(f"{db_info.db_info}: repo sync ä»åœ¨åŸ·è¡Œä¸­")
                    return db_info  # é‚„åœ¨åŸ·è¡Œä¸­
                
                sync_result = poll
                
                # è¨˜éŒ„ sync log è·¯å¾‘
                if hasattr(db_info.sync_process, '_log_file_path'):
                    db_info.sync_log_path = db_info.sync_process._log_file_path
            
            # ğŸ”¥ æ™ºèƒ½åˆ¤æ–·ï¼šæª¢æŸ¥æ˜¯å¦ç‚ºéƒ¨åˆ†æˆåŠŸ
            if sync_result == 1:  # è¿”å›ç¢¼ 1 å¯èƒ½æ˜¯éƒ¨åˆ†å¤±æ•—
                success_rate, failed_projects = self._analyze_sync_result(db_info)
                
                if success_rate >= 95.0:  # ğŸ”¥ 95% ä»¥ä¸ŠæˆåŠŸç‡å°±ç®—æˆåŠŸ
                    db_info.status = DBStatus.SUCCESS
                    db_info.end_time = datetime.now()
                    
                    warning_msg = f"éƒ¨åˆ†æˆåŠŸ ({success_rate:.1f}%)ï¼Œå¤±æ•—é …ç›®: {len(failed_projects)} å€‹"
                    if failed_projects:
                        warning_msg += f" - {', '.join(failed_projects[:3])}"
                        if len(failed_projects) > 3:
                            warning_msg += f" ç­‰ {len(failed_projects)} å€‹"
                    
                    db_info.error_message = warning_msg
                    
                    elapsed = db_info.end_time - db_info.start_time
                    self.logger.info(f"{db_info.db_info}: âœ… éƒ¨åˆ†æˆåŠŸå®Œæˆ ({success_rate:.1f}%) (è€—æ™‚: {elapsed})")
                    
                    # å˜—è©¦å°å‡º manifest
                    self.logger.info(f"{db_info.db_info}: å˜—è©¦å°å‡ºç‰ˆæœ¬è³‡è¨Š...")
                    export_success = self.repo_manager.export_manifest(db_info.local_path)
                    
                    if not export_success:
                        self.logger.warning(f"{db_info.db_info}: âš ï¸ manifest å°å‡ºå¤±æ•—ï¼Œä½† sync éƒ¨åˆ†æˆåŠŸ")
                    
                    return db_info
                else:
                    # æˆåŠŸç‡å¤ªä½ï¼Œç®—å¤±æ•—
                    raise Exception(f"åŒæ­¥æˆåŠŸç‡å¤ªä½ ({success_rate:.1f}%)ï¼Œå¤±æ•—é …ç›®: {len(failed_projects)} å€‹")
            
            elif sync_result == 0:
                # å®Œå…¨æˆåŠŸ
                db_info.status = DBStatus.SUCCESS
                db_info.end_time = datetime.now()
                
                elapsed = db_info.end_time - db_info.start_time
                self.logger.info(f"{db_info.db_info}: âœ… å®Œå…¨æˆåŠŸ (è€—æ™‚: {elapsed})")
                
                # å˜—è©¦å°å‡º manifest
                self.logger.info(f"{db_info.db_info}: å˜—è©¦å°å‡ºç‰ˆæœ¬è³‡è¨Š...")
                export_success = self.repo_manager.export_manifest(db_info.local_path)
                
                if not export_success:
                    self.logger.warning(f"{db_info.db_info}: âš ï¸ manifest å°å‡ºå¤±æ•—ï¼Œä½† sync å·²æˆåŠŸ")
                    if not db_info.error_message:
                        db_info.error_message = "Sync æˆåŠŸä½† manifest å°å‡ºå¤±æ•—"
            
            else:
                # å…¶ä»–éŒ¯èª¤ç¢¼ï¼Œç®—å¤±æ•—
                raise Exception(f"Repo sync å¤±æ•— (è¿”å›ç¢¼: {sync_result})")
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            db_info.end_time = datetime.now()
            self.logger.error(f"{db_info.db_info}: Phase 2 å¤±æ•— - {str(e)}")
        
        return db_info

    def process_db_phase2_zero_fail(self, db_info: DBInfo) -> DBInfo:
        """é›¶å¤±æ•—çš„ Phase 2 è™•ç†"""
        try:
            self.logger.info(f"{db_info.db_info}: é–‹å§‹é›¶å¤±æ•— Phase 2")
            
            if self.dry_run:
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                return db_info
            
            # æª¢æŸ¥ sync é€²ç¨‹ç‹€æ…‹
            sync_result = None
            if db_info.sync_process:
                poll = self.repo_manager.check_process_status(
                    db_info.db_info, 
                    db_info.sync_process
                )
                
                if poll is None:
                    return db_info  # é‚„åœ¨åŸ·è¡Œä¸­
                
                sync_result = poll
                
                if hasattr(db_info.sync_process, '_log_file_path'):
                    db_info.sync_log_path = db_info.sync_process._log_file_path
            
            # ğŸ”¥ é›¶å®¹å¿ï¼šç„¡è«–è¿”å›ç¢¼å¦‚ä½•ï¼Œéƒ½è¦æª¢æŸ¥å¯¦éš›çµæœ
            success_rate, failed_projects = self._analyze_sync_result(db_info)
            
            if failed_projects:
                # æœ‰å¤±æ•—é …ç›®ï¼Œå¿…é ˆæ•‘æ´
                self.logger.warning(f"{db_info.db_info}: ğŸš¨ æª¢æ¸¬åˆ° {len(failed_projects)} å€‹å¤±æ•—é …ç›®ï¼Œå•Ÿå‹•é›¶å®¹å¿æ•‘æ´")
                
                if self._enhanced_repair_failed_projects_zero_tolerance(db_info, failed_projects):
                    db_info.status = DBStatus.SUCCESS
                    db_info.end_time = datetime.now()
                    
                    elapsed = db_info.end_time - db_info.start_time
                    self.logger.info(f"{db_info.db_info}: âœ… é›¶å®¹å¿æ•‘æ´æˆåŠŸï¼Œé”åˆ°100% (è€—æ™‚: {elapsed})")
                else:
                    db_info.status = DBStatus.FAILED
                    db_info.error_message = f"é›¶å®¹å¿æ•‘æ´å¤±æ•—ï¼Œç„¡æ³•è™•ç† {len(failed_projects)} å€‹å¤±æ•—é …ç›®"
                    db_info.end_time = datetime.now()
                    
                    elapsed = db_info.end_time - db_info.start_time
                    self.logger.error(f"{db_info.db_info}: âŒ é›¶å®¹å¿æ•‘æ´å¤±æ•— (è€—æ™‚: {elapsed})")
            else:
                # æ²’æœ‰å¤±æ•—é …ç›®ï¼ŒçœŸæ­£çš„æˆåŠŸ
                db_info.status = DBStatus.SUCCESS
                db_info.end_time = datetime.now()
                
                elapsed = db_info.end_time - db_info.start_time
                self.logger.info(f"{db_info.db_info}: âœ… å®Œç¾å®Œæˆï¼Œç„¡å¤±æ•—é …ç›® (è€—æ™‚: {elapsed})")
            
            # å˜—è©¦å°å‡º manifest
            if db_info.status == DBStatus.SUCCESS:
                self.logger.info(f"{db_info.db_info}: å°å‡ºç‰ˆæœ¬è³‡è¨Š...")
                export_success = self.repo_manager.export_manifest(db_info.local_path)
                
                if not export_success:
                    self.logger.warning(f"{db_info.db_info}: âš ï¸ manifest å°å‡ºå¤±æ•—ï¼Œä½† sync å·²æˆåŠŸ")
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            db_info.end_time = datetime.now()
            self.logger.error(f"{db_info.db_info}: Phase 2 ç•°å¸¸ - {str(e)}")
        
        return db_info

    def _analyze_sync_result(self, db_info: DBInfo) -> tuple:
        """åˆ†æ sync çµæœï¼Œç²¾ç¢ºæå–å¤±æ•—é …ç›® - æ¸…ç†ç‰ˆæœ¬"""
        try:
            log_file = db_info.sync_log_path
            if not log_file:
                log_file = self._get_sync_log_file(db_info)
            
            if not log_file or not os.path.exists(log_file):
                return 0.0, []
            
            total_projects = 0
            failed_projects = set()
            
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # ğŸ”¥ å…ˆæ¸…ç† ANSI è½‰ç¾©å­—ç¬¦
                import re
                clean_content = re.sub(r'\x1b\[[0-9;]*[mK]', '', content)
                
                # æå–åŒæ­¥çµ±è¨ˆ
                sync_pattern = r'Syncing:\s*\d+%\s*\((\d+)/(\d+)\)'
                matches = re.findall(sync_pattern, clean_content)
                
                if matches:
                    last_match = matches[-1]
                    completed = int(last_match[0])
                    total_projects = int(last_match[1])
                    self.logger.debug(f"{db_info.db_info}: åŒæ­¥çµ±è¨ˆ - å®Œæˆ {completed}/{total_projects}")
                
                # ğŸ”¥ ç²¾ç¢ºæå–å¤±æ•—é …ç›® - åªå¾ç‰¹å®šéŒ¯èª¤éƒ¨åˆ†æå–
                
                # æ–¹å¼1ï¼šå¾ "The following projects failed" åˆ°ä¸‹ä¸€å€‹éŒ¯èª¤éƒ¨åˆ†
                failed_section_match = re.search(
                    r'The following projects failed[^:]*:\s*(.*?)(?=\n\s*error:|$)', 
                    clean_content, 
                    re.DOTALL | re.IGNORECASE
                )
                
                if failed_section_match:
                    failed_text = failed_section_match.group(1)
                    # æå–æœ‰æ•ˆçš„é …ç›®è·¯å¾‘
                    project_lines = [line.strip() for line in failed_text.split('\n') if line.strip()]
                    for line in project_lines:
                        # åªæ¥å—ä»¥ kernel/ é–‹é ­çš„æœ‰æ•ˆè·¯å¾‘
                        if line.startswith('kernel/') and '/' in line and len(line) > 10:
                            # æ¸…ç†å¯èƒ½çš„å°¾éƒ¨å­—ç¬¦
                            clean_project = re.sub(r'[^\w/\-_.].*$', '', line)
                            if clean_project and clean_project.count('/') >= 2:  # è‡³å°‘åŒ…å« kernel/xxx/yyy
                                failed_projects.add(clean_project)
                
                # æ–¹å¼2ï¼šå¾å…·é«”çš„ GitCommandError æ¶ˆæ¯ä¸­æå–
                git_error_pattern = r"GitCommandError:.*?on\s+(kernel/[^\s'\"]+)\s+failed"
                git_errors = re.findall(git_error_pattern, clean_content, re.IGNORECASE)
                for project in git_errors:
                    # æ¸…ç†é …ç›®è·¯å¾‘
                    clean_project = re.sub(r'[^\w/\-_.].*$', '', project)
                    if clean_project and clean_project.count('/') >= 2:
                        failed_projects.add(clean_project)
                
                # æ–¹å¼3ï¼šå¾ "fatal: not a git repository" éŒ¯èª¤æ¨æ–·é …ç›®
                repo_error_pattern = r"fatal: not a git repository.*?/([^/\s'\"]+)\.git"
                repo_errors = re.findall(repo_error_pattern, clean_content)
                
                # æ ¹æ“šé …ç›®åç¨±æ¨æ–·å®Œæ•´è·¯å¾‘
                known_project_mappings = {
                    'vts': 'kernel/android/U/test/vts',
                    'hal': 'kernel/android/U/test/vts-testcase/hal',
                    'hal-trace': 'kernel/android/U/test/vts-testcase/hal-trace'
                }
                
                for project_name in repo_errors:
                    if project_name in known_project_mappings:
                        failed_projects.add(known_project_mappings[project_name])
            
            # ğŸ”¥ æ¸…ç†å’Œé©—è­‰å¤±æ•—é …ç›®åˆ—è¡¨
            valid_failed_projects = []
            for project in failed_projects:
                # é©—è­‰é …ç›®è·¯å¾‘æ ¼å¼
                if (project.startswith('kernel/') and 
                    project.count('/') >= 2 and 
                    len(project) > 10 and
                    not any(char in project for char in ['\x1b', '"', "'", ':', ' ']) and
                    re.match(r'^kernel/[a-zA-Z0-9_\-/]+$', project)):
                    valid_failed_projects.append(project)
            
            # å»é‡ä¸¦æ’åº
            valid_failed_projects = sorted(list(set(valid_failed_projects)))
            
            # è¨ˆç®—æˆåŠŸç‡
            if total_projects > 0:
                success_count = total_projects - len(valid_failed_projects)
                success_rate = (success_count / total_projects) * 100
                
                self.logger.info(f"{db_info.db_info}: æ¸…ç†å¾Œåˆ†æ - ç¸½è¨ˆ:{total_projects}, æˆåŠŸ:{success_count}, å¤±æ•—:{len(valid_failed_projects)}, æˆåŠŸç‡:{success_rate:.1f}%")
                self.logger.info(f"{db_info.db_info}: æœ‰æ•ˆå¤±æ•—é …ç›®: {valid_failed_projects}")
                
                return success_rate, valid_failed_projects
            
            return 0.0, []
            
        except Exception as e:
            self.logger.debug(f"åˆ†æåŒæ­¥çµæœå¤±æ•—: {e}")
            return 0.0, []

    def _get_sync_log_file(self, db_info: DBInfo) -> str:
        """ç²å– sync æ—¥èªŒæ–‡ä»¶è·¯å¾‘ - å„ªå…ˆä½¿ç”¨ unbuffer ç‰ˆæœ¬"""
        try:
            log_dir = os.path.join(db_info.local_path, 'logs')
            if os.path.exists(log_dir):
                # å°‹æ‰¾æœ€æ–°çš„æ—¥èªŒæ–‡ä»¶ï¼Œå„ªå…ˆé¸æ“‡ unbuffer ç‰ˆæœ¬
                log_files = []
                for f in os.listdir(log_dir):
                    if f.startswith('repo_sync_') and f.endswith('.log'):
                        file_path = os.path.join(log_dir, f)
                        mtime = os.path.getmtime(file_path)
                        
                        # ğŸ”¥ çµ¦ä¸åŒé¡å‹çš„æ—¥èªŒä¸åŒå„ªå…ˆç´š
                        priority = 0
                        if 'unbuffer' in f:
                            priority = 100  # æœ€é«˜å„ªå…ˆç´š
                        elif 'script' in f:
                            priority = 50
                        elif 'hotfix' in f or 'fixed' in f:
                            priority = 25
                        # èˆŠç‰ˆæœ¬çš„æ—¥èªŒå„ªå…ˆç´šç‚º 0
                        
                        log_files.append((priority, mtime, file_path))
                
                if log_files:
                    # æŒ‰å„ªå…ˆç´šå’Œä¿®æ”¹æ™‚é–“æ’åº
                    log_files.sort(key=lambda x: (x[0], x[1]), reverse=True)
                    latest_log = log_files[0][2]
                    self.logger.debug(f"{db_info.db_info}: ä½¿ç”¨æ—¥èªŒæ–‡ä»¶: {os.path.basename(latest_log)}")
                    return latest_log
            
            self.logger.debug(f"{db_info.db_info}: æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨: {log_dir}")
        except Exception as e:
            self.logger.debug(f"{db_info.db_info}: ç²å–æ—¥èªŒæ–‡ä»¶å¤±æ•—: {e}")
        
        return ""

    # ========================================
    # ğŸ”¥ é›¶å¤±æ•—æ•‘æ´æ©Ÿåˆ¶
    # ========================================

    def _enhanced_repair_failed_projects_zero_tolerance(self, db_info: DBInfo, failed_projects: list) -> bool:
        """é›¶å®¹å¿ä¿®å¾©å¤±æ•—é …ç›® - å¿…é ˆé”åˆ°100%æˆåŠŸ"""
        if not failed_projects:
            return True
        
        self.logger.warning(f"{db_info.db_info}: ğŸš¨ æª¢æ¸¬åˆ° {len(failed_projects)} å€‹å¤±æ•—é …ç›®ï¼Œå•Ÿå‹•é›¶å®¹å¿æ•‘æ´")
        
        # ğŸ”¥ å¤šå±¤æ•‘æ´ç­–ç•¥
        rescue_strategies = [
            ("åŸºç¤ä¿®å¾©", self._basic_repair_strategy),
            ("ç¶²è·¯é‡ç½®ä¿®å¾©", self._network_reset_strategy), 
            ("å®Œå…¨é‡å»ºä¿®å¾©", self._complete_rebuild_strategy),
            ("çµ‚æ¥µæ•‘æ´", self._ultimate_rescue_strategy)
        ]
        
        for strategy_name, strategy_func in rescue_strategies:
            self.logger.info(f"{db_info.db_info}: ğŸ”§ åŸ·è¡Œ {strategy_name}...")
            
            if strategy_func(db_info, failed_projects):
                # é©—è­‰ä¿®å¾©çµæœ
                remaining_failures = self._verify_repair_result(db_info)
                if not remaining_failures:
                    self.logger.info(f"{db_info.db_info}: âœ… {strategy_name} æˆåŠŸï¼Œé”åˆ°100%å®Œæˆ")
                    return True
                else:
                    self.logger.warning(f"{db_info.db_info}: âš ï¸ {strategy_name} å¾Œä»æœ‰ {len(remaining_failures)} å€‹å¤±æ•—")
                    failed_projects = remaining_failures  # æ›´æ–°å¤±æ•—åˆ—è¡¨
            else:
                self.logger.warning(f"{db_info.db_info}: âŒ {strategy_name} å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹ç­–ç•¥")
        
        # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±æ•—ï¼Œè¨˜éŒ„è©³ç´°éŒ¯èª¤ä¸¦å¼·åˆ¶é‡ä¾†
        self.logger.error(f"{db_info.db_info}: ğŸ’¥ æ‰€æœ‰æ•‘æ´ç­–ç•¥å¤±æ•—ï¼ŒåŸ·è¡Œæœ€å¾Œçš„æ ¸æ­¦ç´šé‡å»º")
        return self._nuclear_rebuild(db_info)

    def _basic_repair_strategy(self, db_info: DBInfo, failed_projects: list) -> bool:
        """åŸºç¤ä¿®å¾©ç­–ç•¥ï¼šæ¸…ç†ä¸¦é‡æ–°åŒæ­¥"""
        try:
            self.logger.info(f"{db_info.db_info}: ğŸ”§ åŸºç¤ä¿®å¾©ï¼šæ¸…ç† {len(failed_projects)} å€‹å¤±æ•—é …ç›®")
            
            # é€å€‹æ¸…ç†å¤±æ•—é …ç›®
            for project in failed_projects:
                self._thorough_cleanup_project(db_info, project)
            
            # å–®ç·šç¨‹é‡æ–°åŒæ­¥
            projects_str = ' '.join([f'"{project}"' for project in failed_projects])
            sync_cmd = f"{config_manager.repo_config['repo_command']} sync -j1 --force-sync --no-clone-bundle {projects_str}"
            
            success, output = self.repo_manager.run_command(
                sync_cmd,
                cwd=db_info.local_path,
                timeout=1800  # 30åˆ†é˜
            )
            
            if success:
                self.logger.info(f"{db_info.db_info}: âœ… åŸºç¤ä¿®å¾©å®Œæˆ")
                return True
            else:
                self.logger.warning(f"{db_info.db_info}: âš ï¸ åŸºç¤ä¿®å¾©å¤±æ•—: {output[-500:]}")
                return False
                
        except Exception as e:
            self.logger.error(f"{db_info.db_info}: âŒ åŸºç¤ä¿®å¾©ç•°å¸¸: {e}")
            return False

    def _network_reset_strategy(self, db_info: DBInfo, failed_projects: list) -> bool:
        """ç¶²è·¯é‡ç½®ç­–ç•¥ï¼šé‡ç½®ç¶²è·¯ä¸¦æ›æºé‡è©¦"""
        try:
            self.logger.info(f"{db_info.db_info}: ğŸŒ ç¶²è·¯é‡ç½®ä¿®å¾©")
            
            # ğŸ”¥ æ¸…ç†ç¶²è·¯ç›¸é—œå¿«å–
            cache_cleanup_commands = [
                "git config --global --unset http.proxy 2>/dev/null || true",
                "git config --global --unset https.proxy 2>/dev/null || true", 
                "git config --global http.postBuffer 524288000",
                "git config --global http.maxRequestBuffer 100M",
                "git config --global core.compression 0",
            ]
            
            for cmd in cache_cleanup_commands:
                os.system(cmd)
            
            # ğŸ”¥ é€å€‹é …ç›®æ·±åº¦ä¿®å¾©
            for project in failed_projects:
                self.logger.info(f"{db_info.db_info}: ğŸ”„ æ·±åº¦ä¿®å¾©é …ç›®: {project}")
                
                # å®Œå…¨æ¸…ç†é …ç›®
                self._thorough_cleanup_project(db_info, project)
                
                # å–®ç¨åŒæ­¥é€™å€‹é …ç›®ï¼ˆæ›´æ¿€é€²çš„åƒæ•¸ï¼‰
                repair_commands = [
                    f"repo sync -j1 --force-sync --no-clone-bundle --current-branch {project}",
                    f"repo sync -j1 --force-sync --no-tags {project}",
                    f"repo sync -j1 --force-broken {project}",
                ]
                
                for cmd in repair_commands:
                    full_cmd = f"{config_manager.repo_config['repo_command']} " + cmd.replace("repo ", "")
                    
                    success, output = self.repo_manager.run_command(
                        full_cmd,
                        cwd=db_info.local_path,
                        timeout=900  # 15åˆ†é˜æ¯å€‹é …ç›®
                    )
                    
                    if success:
                        self.logger.info(f"{db_info.db_info}: âœ… é …ç›® {project} ä¿®å¾©æˆåŠŸ")
                        break
                    else:
                        self.logger.debug(f"{db_info.db_info}: å˜—è©¦ä¸‹ä¸€å€‹å‘½ä»¤: {output[-200:]}")
                else:
                    self.logger.warning(f"{db_info.db_info}: âš ï¸ é …ç›® {project} æ‰€æœ‰å‘½ä»¤éƒ½å¤±æ•—")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"{db_info.db_info}: âŒ ç¶²è·¯é‡ç½®ç­–ç•¥ç•°å¸¸: {e}")
            return False

    def _complete_rebuild_strategy(self, db_info: DBInfo, failed_projects: list) -> bool:
        """å®Œå…¨é‡å»ºç­–ç•¥ï¼šé‡å»ºå¤±æ•—é …ç›®çš„ git åº«"""
        try:
            self.logger.info(f"{db_info.db_info}: ğŸ—ï¸ å®Œå…¨é‡å»ºç­–ç•¥")
            
            # ğŸ”¥ æ ¸æ­¦ç´šæ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ç›¸é—œçš„ .repo æ•¸æ“š
            for project in failed_projects:
                self.logger.info(f"{db_info.db_info}: ğŸ’£ æ ¸æ­¦ç´šæ¸…ç†é …ç›®: {project}")
                
                # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„é …ç›®ç›¸é—œç›®éŒ„
                cleanup_patterns = [
                    os.path.join(db_info.local_path, '.repo', 'projects', f"{project}.git"),
                    os.path.join(db_info.local_path, '.repo', 'project-objects', f"{project}.git"),
                    os.path.join(db_info.local_path, '.repo', 'projects', project),
                    os.path.join(db_info.local_path, '.repo', 'project-objects', project),
                    os.path.join(db_info.local_path, project),
                ]
                
                # é‚„è¦æ¸…ç†å¯èƒ½çš„ç¬¦è™Ÿé€£çµå’Œå¼•ç”¨
                for pattern in cleanup_patterns:
                    if os.path.exists(pattern):
                        try:
                            import shutil
                            if os.path.islink(pattern):
                                os.unlink(pattern)
                            elif os.path.isdir(pattern):
                                shutil.rmtree(pattern)
                            else:
                                os.remove(pattern)
                            self.logger.debug(f"æ¸…ç†: {pattern}")
                        except Exception as e:
                            # å¼·åˆ¶æ¸…ç†
                            self.logger.debug(f"å¼·åˆ¶æ¸…ç†: {pattern}")
                            os.system(f'rm -rf "{pattern}" 2>/dev/null || true')
            
            # ğŸ”¥ é‡å»º project æ˜ å°„
            self.logger.info(f"{db_info.db_info}: ğŸ”„ é‡å»ºé …ç›®æ˜ å°„")
            
            # å¼·åˆ¶é‡æ–°åˆå§‹åŒ–é€™äº›é …ç›®
            init_cmd = f"{config_manager.repo_config['repo_command']} sync --force-sync -j1 " + " ".join([f'"{project}"' for project in failed_projects])
            
            success, output = self.repo_manager.run_command(
                init_cmd,
                cwd=db_info.local_path,
                timeout=2400  # 40åˆ†é˜
            )
            
            if success:
                self.logger.info(f"{db_info.db_info}: âœ… å®Œå…¨é‡å»ºæˆåŠŸ")
                return True
            else:
                self.logger.warning(f"{db_info.db_info}: âš ï¸ å®Œå…¨é‡å»ºå¤±æ•—: {output[-500:]}")
                return False
                
        except Exception as e:
            self.logger.error(f"{db_info.db_info}: âŒ å®Œå…¨é‡å»ºç­–ç•¥ç•°å¸¸: {e}")
            return False

    def _ultimate_rescue_strategy(self, db_info: DBInfo, failed_projects: list) -> bool:
        """çµ‚æ¥µæ•‘æ´ç­–ç•¥ï¼šæ‰‹å‹• git clone å¤±æ•—çš„é …ç›®"""
        try:
            self.logger.warning(f"{db_info.db_info}: ğŸ†˜ çµ‚æ¥µæ•‘æ´ï¼šæ‰‹å‹•å…‹éš†å¤±æ•—é …ç›®")
            
            # ğŸ”¥ è§£æ manifest ç²å–é …ç›®çš„çœŸå¯¦ git URL
            manifest_path = os.path.join(db_info.local_path, db_info.manifest_file)
            if not os.path.exists(manifest_path):
                self.logger.error(f"{db_info.db_info}: âŒ Manifest æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            project_urls = self._extract_project_urls_from_manifest(manifest_path, failed_projects)
            
            for project in failed_projects:
                if project not in project_urls:
                    self.logger.warning(f"{db_info.db_info}: âš ï¸ ç„¡æ³•æ‰¾åˆ°é …ç›® {project} çš„ URL")
                    continue
                
                git_url = project_urls[project]
                project_dir = os.path.join(db_info.local_path, project)
                
                self.logger.info(f"{db_info.db_info}: ğŸ”„ æ‰‹å‹•å…‹éš†: {project}")
                
                # ç¢ºä¿ç›®éŒ„ä¸å­˜åœ¨
                if os.path.exists(project_dir):
                    import shutil
                    shutil.rmtree(project_dir)
                
                # æ‰‹å‹• git clone
                clone_cmd = f"git clone --depth 1 {git_url} {project_dir}"
                
                success, output = self.repo_manager.run_command(
                    clone_cmd,
                    cwd=db_info.local_path,
                    timeout=600  # 10åˆ†é˜æ¯å€‹é …ç›®
                )
                
                if not success:
                    self.logger.error(f"{db_info.db_info}: âŒ æ‰‹å‹•å…‹éš† {project} å¤±æ•—: {output}")
                    return False
                
                self.logger.info(f"{db_info.db_info}: âœ… æ‰‹å‹•å…‹éš† {project} æˆåŠŸ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"{db_info.db_info}: âŒ çµ‚æ¥µæ•‘æ´ç•°å¸¸: {e}")
            return False

    def _nuclear_rebuild(self, db_info: DBInfo) -> bool:
        """æ ¸æ­¦ç´šé‡å»ºï¼šå®Œå…¨å¾é ­é–‹å§‹"""
        try:
            self.logger.warning(f"{db_info.db_info}: â˜¢ï¸ åŸ·è¡Œæ ¸æ­¦ç´šé‡å»º - å®Œå…¨å¾é ­é–‹å§‹")
            
            # ğŸ”¥ ä¿å­˜é‡è¦æ–‡ä»¶
            manifest_backup = None
            if db_info.manifest_file:
                src = os.path.join(db_info.local_path, db_info.manifest_file)
                if os.path.exists(src):
                    import tempfile
                    manifest_backup = tempfile.mktemp(suffix='.xml')
                    import shutil
                    shutil.copy2(src, manifest_backup)
            
            # ğŸ”¥ å¾¹åº•æ‘§æ¯€ä¸¦é‡å»º
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_path = f"{db_info.local_path}_NUCLEAR_BACKUP_{timestamp}"
            
            try:
                os.rename(db_info.local_path, backup_path)
                self.logger.info(f"{db_info.db_info}: ğŸ—‚ï¸ å‚™ä»½èˆŠç›®éŒ„: {backup_path}")
            except:
                import shutil
                shutil.rmtree(db_info.local_path)
            
            # é‡å»ºç›®éŒ„
            os.makedirs(db_info.local_path, exist_ok=True)
            
            # æ¢å¾© manifest
            if manifest_backup and os.path.exists(manifest_backup):
                dest = os.path.join(db_info.local_path, db_info.manifest_file)
                shutil.copy2(manifest_backup, dest)
                os.remove(manifest_backup)
            
            # ğŸ”¥ å®Œå…¨é‡æ–°åŸ·è¡Œåˆå§‹åŒ–æµç¨‹
            if not db_info.actual_source_cmd:
                self.logger.error(f"{db_info.db_info}: âŒ ç¼ºå°‘ source commandï¼Œç„¡æ³•é‡å»º")
                return False
            
            # é‡æ–° repo init
            if not self.repo_manager.repo_init(db_info.local_path, db_info.actual_source_cmd):
                self.logger.error(f"{db_info.db_info}: âŒ æ ¸æ­¦ç´š repo init å¤±æ•—")
                return False
            
            # é‡æ–°æ‡‰ç”¨ manifest
            manifest_path = os.path.join(db_info.local_path, db_info.manifest_file)
            if not self.repo_manager.apply_manifest(db_info.local_path, manifest_path):
                self.logger.error(f"{db_info.db_info}: âŒ æ ¸æ­¦ç´š apply manifest å¤±æ•—")
                return False
            
            # ğŸ”¥ ä½¿ç”¨æœ€ä¿å®ˆçš„åŒæ­¥ç­–ç•¥
            nuclear_sync_cmd = f"{config_manager.repo_config['repo_command']} sync -j1 --force-sync --force-broken --no-clone-bundle"
            
            success, output = self.repo_manager.run_command(
                nuclear_sync_cmd,
                cwd=db_info.local_path,
                timeout=7200  # 2å°æ™‚
            )
            
            if success:
                self.logger.info(f"{db_info.db_info}: âœ… æ ¸æ­¦ç´šé‡å»ºæˆåŠŸï¼")
                
                # èƒŒæ™¯åˆªé™¤å‚™ä»½
                delete_cmd = f"nohup rm -rf '{backup_path}' >/dev/null 2>&1 &"
                os.system(delete_cmd)
                
                return True
            else:
                self.logger.error(f"{db_info.db_info}: âŒ æ ¸æ­¦ç´šé‡å»ºä¹Ÿå¤±æ•—äº†: {output}")
                return False
            
        except Exception as e:
            self.logger.error(f"{db_info.db_info}: ğŸ’¥ æ ¸æ­¦ç´šé‡å»ºç•°å¸¸: {e}")
            return False

    def _thorough_cleanup_project(self, db_info: DBInfo, project: str):
        """å¾¹åº•æ¸…ç†å–®å€‹é …ç›®çš„æ‰€æœ‰ç›¸é—œæ–‡ä»¶"""
        cleanup_paths = [
            os.path.join(db_info.local_path, '.repo', 'projects', f"{project}.git"),
            os.path.join(db_info.local_path, '.repo', 'project-objects', f"{project}.git"),
            os.path.join(db_info.local_path, '.repo', 'projects', project),
            os.path.join(db_info.local_path, '.repo', 'project-objects', project),
            os.path.join(db_info.local_path, project),
        ]
        
        for path in cleanup_paths:
            if os.path.exists(path):
                try:
                    import shutil
                    if os.path.islink(path):
                        os.unlink(path)
                    elif os.path.isdir(path):
                        shutil.rmtree(path)
                    else:
                        os.remove(path)
                except:
                    # å¼·åˆ¶æ¸…ç†
                    os.system(f'chmod -R 777 "{path}" 2>/dev/null || true')
                    os.system(f'rm -rf "{path}" 2>/dev/null || true')

    def _verify_repair_result(self, db_info: DBInfo) -> list:
        """é©—è­‰ä¿®å¾©çµæœï¼Œè¿”å›ä»ç„¶å¤±æ•—çš„é …ç›®åˆ—è¡¨"""
        try:
            # åŸ·è¡Œ repo status æª¢æŸ¥
            status_cmd = f"{config_manager.repo_config['repo_command']} status"
            success, output = self.repo_manager.run_command(
                status_cmd,
                cwd=db_info.local_path,
                timeout=300
            )
            
            if not success:
                self.logger.warning(f"{db_info.db_info}: repo status æª¢æŸ¥å¤±æ•—")
                return []
            
            # æª¢æŸ¥æ˜¯å¦é‚„æœ‰éŒ¯èª¤
            if 'fatal:' in output or 'error:' in output:
                # è§£æä»ç„¶å¤±æ•—çš„é …ç›®
                remaining_failures = []
                for line in output.split('\n'):
                    if 'fatal:' in line or 'error:' in line:
                        # å˜—è©¦æå–é …ç›®åç¨±
                        project_match = re.search(r'(kernel/[^\s]+)', line)
                        if project_match:
                            remaining_failures.append(project_match.group(1))
                
                return list(set(remaining_failures))
            
            return []
            
        except Exception as e:
            self.logger.debug(f"é©—è­‰ä¿®å¾©çµæœæ™‚ç•°å¸¸: {e}")
            return []

    def _extract_project_urls_from_manifest(self, manifest_path: str, projects: list) -> dict:
        """å¾ manifest æ–‡ä»¶æå–é …ç›®çš„ git URL"""
        project_urls = {}
        
        try:
            import xml.etree.ElementTree as ET
            tree = ET.parse(manifest_path)
            root = tree.getroot()
            
            # æ‰¾åˆ° remote å®šç¾©
            remotes = {}
            for remote in root.findall('remote'):
                name = remote.get('name')
                fetch = remote.get('fetch')
                if name and fetch:
                    remotes[name] = fetch
            
            # æ‰¾åˆ°é …ç›®å®šç¾©
            for project_elem in root.findall('project'):
                path = project_elem.get('path')
                name = project_elem.get('name')
                remote = project_elem.get('remote', 'origin')
                
                if path in projects and remote in remotes:
                    base_url = remotes[remote].rstrip('/')
                    project_url = f"{base_url}/{name}"
                    project_urls[path] = project_url
                    
        except Exception as e:
            self.logger.debug(f"è§£æ manifest å¤±æ•—: {e}")
        
        return project_urls

    # ========================================
    # ä¸»è¦è™•ç†å‡½æ•¸
    # ========================================

    def process_dbs_async(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """ç•°æ­¥è™•ç†å¤šå€‹ DB - æ ¹æ“šæ¨¡å¼é¸æ“‡è™•ç†æ–¹å¼"""
        if self.zero_fail_mode:
            return self.process_dbs_async_zero_fail(db_list, db_versions)
        else:
            return self.process_dbs_async_standard(db_list, db_versions)

    def process_dbs_async_standard(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """æ¨™æº–ç•°æ­¥è™•ç†å¤šå€‹ DB - å¾¹åº•é¿å… SFTP è¡çª"""
        db_versions = db_versions or {}
        db_infos = []
        
        # æº–å‚™ DB è³‡è¨Š
        all_db_infos = self.get_all_dbs('all')
        
        for db_name in db_list:
            if '#' in db_name:
                db_name, version = db_name.split('#', 1)
            else:
                version = db_versions.get(db_name)
            
            for db_info in all_db_infos:
                if db_info.db_info == db_name:
                    db_info.version = version
                    db_infos.append(db_info)
                    break
        
        if not db_infos:
            self.logger.error("æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
            return

        self.logger.info(f"é–‹å§‹è™•ç† {len(db_infos)} å€‹ DB")
        
        try:
            # Phase 1: æº–å‚™å’Œå•Ÿå‹• sync
            self.logger.info("åŸ·è¡Œ Phase 1: æº–å‚™å·¥ä½œå’Œå•Ÿå‹•åŒæ­¥")
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase1, db_info): db_info for db_info in db_infos}
                
                phase1_results = []
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=300)
                        phase1_results.append(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 1 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        db_info.end_time = datetime.now()
                        phase1_results.append(db_info)
            
            # ç­‰å¾…æ‰€æœ‰ sync å®Œæˆï¼ˆå¢å¼·ç‰ˆç›£æ§ï¼‰
            if not self.dry_run:
                self.logger.info("ç­‰å¾…æ‰€æœ‰ repo sync å®Œæˆ...ï¼ˆå¢å¼·ç‰ˆé€²åº¦ç›£æ§ï¼‰")
                self._wait_for_all_syncs_enhanced(phase1_results)
                # ğŸ”¥ ç‹€æ…‹å·²åœ¨ _wait_for_all_syncs_enhanced ä¸­æ›´æ–°ï¼Œä¸éœ€é‡è¤‡æ›´æ–°
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase2, db_info): db_info for db_info in phase1_results}
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=60)
                        self.report.add_db(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 2 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        db_info.end_time = datetime.now()
                        self.report.add_db(db_info)
            
            self.logger.info("æ‰€æœ‰ DB è™•ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

    def process_dbs_async_zero_fail(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """é›¶å¤±æ•—å®¹å¿çš„ç•°æ­¥è™•ç†å¤šå€‹ DB"""
        db_versions = db_versions or {}
        db_infos = []
        
        # æº–å‚™ DB è³‡è¨Š
        all_db_infos = self.get_all_dbs('all')
        
        for db_name in db_list:
            if '#' in db_name:
                db_name, version = db_name.split('#', 1)
            else:
                version = db_versions.get(db_name)
            
            for db_info in all_db_infos:
                if db_info.db_info == db_name:
                    db_info.version = version
                    db_infos.append(db_info)
                    break
        
        if not db_infos:
            self.logger.error("æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
            return

        self.logger.info(f"ğŸ¯ é›¶å¤±æ•—æ¨¡å¼è™•ç† {len(db_infos)} å€‹ DB")
        
        try:
            # Phase 1: æº–å‚™å’Œå•Ÿå‹• sync
            self.logger.info("åŸ·è¡Œ Phase 1: æº–å‚™å·¥ä½œå’Œå•Ÿå‹•åŒæ­¥")
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase1, db_info): db_info for db_info in db_infos}
                
                phase1_results = []
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=300)
                        phase1_results.append(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 1 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        db_info.end_time = datetime.now()
                        phase1_results.append(db_info)
            
            # ğŸ”¥ ä½¿ç”¨é›¶å¤±æ•—ç›£æ§ç­‰å¾…æ‰€æœ‰ sync å®Œæˆ
            if not self.dry_run:
                self.logger.info("ğŸ¯ å•Ÿå‹•é›¶å¤±æ•—ç›£æ§ï¼Œç­‰å¾…æ‰€æœ‰ repo sync é”åˆ°100%...")
                self._wait_for_all_syncs_enhanced_zero_fail(phase1_results)
            
            # Phase 2: æœ€çµ‚æª¢æŸ¥å’Œæ¸…ç†ï¼ˆä½¿ç”¨é›¶å¤±æ•—ç‰ˆæœ¬ï¼‰
            self.logger.info("åŸ·è¡Œ Phase 2: é›¶å¤±æ•—æœ€çµ‚æª¢æŸ¥")
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase2_zero_fail, db_info): db_info for db_info in phase1_results}
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=60)
                        self.report.add_db(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 2 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        db_info.end_time = datetime.now()
                        self.report.add_db(db_info)
            
            # ğŸ”¥ é›¶å¤±æ•—æœ€çµ‚é©—è­‰
            self._final_zero_fail_verification()
            
            self.logger.info("æ‰€æœ‰ DB è™•ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

    def _wait_for_all_syncs_enhanced(self, db_results: List[DBInfo]):
        """å®Œæ•´ç‰ˆé€²åº¦ç›£æ§ - åŒ…å«å‹•æ…‹é›¶å¤±æ•—æ¨¡å¼å•Ÿç”¨"""
        max_wait_time = config_manager.repo_config['sync_timeout']
        start_wait = time.time()
        
        active_syncs = [db for db in db_results if db.sync_process and db.status != DBStatus.FAILED]
        self.logger.info(f"ç›£æ§ {len(active_syncs)} å€‹æ´»èºçš„ repo sync é€²ç¨‹")
        
        # ğŸ”¥ é›¶å¤±æ•—æ¨¡å¼å‹•æ…‹æ§åˆ¶è®Šæ•¸
        auto_zero_fail_triggered = False
        failure_threshold = 2  # å¤±æ•—æ•¸é‡é–¾å€¼
        failure_rate_threshold = 30.0  # å¤±æ•—ç‡é–¾å€¼ (%)
        last_failure_check = time.time()
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
        progress_tracker = {}
        for db_info in active_syncs:
            progress_tracker[db_info.db_info] = {
                'start_time': db_info.start_time or datetime.now(),
                'last_log_size': 0,
                'estimated_progress': 0,
                'current_activity': 'åˆå§‹åŒ–ä¸­...',
                'log_file': self._get_sync_log_file(db_info),
                'last_check_time': datetime.now(),
                'error_count': 0,
                'critical_errors': []  # ğŸ”¥ è¨˜éŒ„åš´é‡éŒ¯èª¤
            }
        
        check_interval = 30  # 30ç§’æª¢æŸ¥ä¸€æ¬¡
        
        while True:
            all_complete = True
            elapsed = int(time.time() - start_wait)
            current_time = time.time()
            
            # ğŸ”¥ æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡å¤±æ•—ç‡ï¼Œæ±ºå®šæ˜¯å¦å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
            if current_time - last_failure_check > 60:  # æ¯åˆ†é˜æª¢æŸ¥
                if self._check_and_enable_zero_fail_mode(active_syncs, auto_zero_fail_triggered):
                    auto_zero_fail_triggered = True
                    # åˆ‡æ›åˆ°é›¶å¤±æ•—ç›£æ§æ¨¡å¼
                    self.logger.info("ğŸ¯ åˆ‡æ›åˆ°é›¶å¤±æ•—ç›£æ§æ¨¡å¼")
                    self._wait_for_all_syncs_enhanced_zero_fail(active_syncs)
                    return
                last_failure_check = current_time
            
            print("\n" + "="*100)
            print(f"ğŸ“Š Repo Sync é€²åº¦ç›£æ§ - å·²ç­‰å¾… {elapsed}s")
            if not self.zero_fail_mode and not auto_zero_fail_triggered:
                print("ğŸ” æ™ºèƒ½å¤±æ•—æª¢æ¸¬æ¨¡å¼ (è‡ªå‹•åˆ‡æ›é›¶å¤±æ•—)")
            print("="*100)
            
            current_failed_count = 0
            current_running_count = 0
            current_completed_count = 0
            
            for db_info in active_syncs:
                if db_info.status == DBStatus.FAILED:
                    current_failed_count += 1
                    continue
                
                db_name = db_info.db_info
                tracker = progress_tracker[db_name]
                
                # æ§‹å»ºåŒ…å«ç‰ˆæœ¬ä¿¡æ¯çš„é¡¯ç¤ºåç¨±
                manifest_info = ""
                if db_info.manifest_file:
                    manifest_info = f" ({db_info.manifest_file})"
                elif db_info.version:
                    manifest_info = f" (v{db_info.version})"
                
                display_name = f"{db_name}{manifest_info}"
                
                if db_info.sync_process:
                    try:
                        poll = db_info.sync_process.poll()
                        
                        if poll is None:  # ä»åœ¨é‹è¡Œ
                            all_complete = False
                            current_running_count += 1
                            
                            # ğŸ”¥ æª¢æŸ¥åš´é‡éŒ¯èª¤ï¼Œå¯èƒ½è§¸ç™¼é›¶å¤±æ•—æ¨¡å¼
                            critical_error = self._check_for_critical_errors(db_info, tracker)
                            if critical_error and not auto_zero_fail_triggered:
                                self.logger.warning(f"ğŸš¨ {db_name}: æª¢æ¸¬åˆ°åš´é‡éŒ¯èª¤: {critical_error}")
                                tracker['critical_errors'].append(critical_error)
                                
                                # å¦‚æœåš´é‡éŒ¯èª¤ç´¯ç©æˆ–è€…æ˜¯è‡´å‘½éŒ¯èª¤ï¼Œç«‹å³å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
                                if len(tracker['critical_errors']) >= 2 or "FATAL" in critical_error:
                                    self.logger.warning(f"ğŸ¯ å› åš´é‡éŒ¯èª¤ç«‹å³å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
                                    self._enable_zero_fail_mode_dynamically()
                                    auto_zero_fail_triggered = True
                                    
                                    # ç«‹å³åˆ‡æ›åˆ°é›¶å¤±æ•—ç›£æ§
                                    self._wait_for_all_syncs_enhanced_zero_fail(active_syncs)
                                    return
                            
                            # æ›´æ–°é€²åº¦ä¿¡æ¯
                            self._update_progress_info(db_info, tracker)
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            progress = tracker['estimated_progress']
                            bar_length = 20
                            filled = int(bar_length * progress / 100)
                            bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
                            
                            activity = tracker.get('current_project', '').split('/')[-1] or tracker.get('current_activity', 'åŒæ­¥ä¸­')
                            if len(activity) > 15:
                                activity = activity[:12] + "..."
                            
                            # ğŸ”¥ é¡¯ç¤ºéŒ¯èª¤ç‹€æ…‹
                            status_info = ""
                            if tracker['critical_errors']:
                                status_info = f" âš ï¸{len(tracker['critical_errors'])}"
                            
                            print(f"ğŸ”„ {display_name:30s} â”‚{bar}â”‚ {progress:3d}% â”‚ {runtime_str} â”‚ {activity}{status_info}")
                            
                            # æª¢æŸ¥è¶…æ™‚
                            if time.time() - start_wait > max_wait_time:
                                self.logger.warning(f"{db_name}: repo sync è¶…æ™‚ï¼Œå¼·åˆ¶çµ‚æ­¢")
                                try:
                                    db_info.sync_process.terminate()
                                    db_info.sync_process.wait(timeout=5)
                                except:
                                    try:
                                        db_info.sync_process.kill()
                                    except:
                                        pass
                                db_info.status = DBStatus.FAILED
                                db_info.error_message = "Sync è¶…æ™‚"
                                current_failed_count += 1
                                print(f"â° {display_name:30s} â”‚ è¶…æ™‚çµ‚æ­¢")
                                
                        elif poll == 0:  # æˆåŠŸå®Œæˆ
                            current_completed_count += 1
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            bar = "â–ˆ" * 20
                            print(f"âœ… {display_name:30s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ å®Œæˆ")
                            
                        else:  # å¤±æ•— (poll != 0)
                            current_failed_count += 1
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            
                            # ğŸ”¥ åˆ†æå¤±æ•—åŸå› 
                            error_msg = f"Sync å¤±æ•— (è¿”å›ç¢¼: {poll})"
                            
                            # æª¢æŸ¥æ˜¯å¦éœ€è¦ç«‹å³å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
                            if poll == 1:  # éƒ¨åˆ†å¤±æ•—ï¼Œå¯èƒ½å¯ä»¥æ•‘æ´
                                success_rate, failed_projects = self._analyze_sync_result(db_info)
                                if failed_projects:
                                    error_msg += f" - {len(failed_projects)} å€‹é …ç›®å¤±æ•—"
                                    
                                    # ğŸ”¥ å¦‚æœå¤±æ•—é …ç›®å¾ˆå¤šï¼Œè€ƒæ…®å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
                                    if len(failed_projects) > 10 and not auto_zero_fail_triggered:
                                        self.logger.warning(f"ğŸš¨ {db_name}: å¤§é‡é …ç›®å¤±æ•— ({len(failed_projects)} å€‹)ï¼Œå»ºè­°å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
                            
                            db_info.status = DBStatus.FAILED
                            db_info.error_message = error_msg
                            print(f"âŒ {display_name:30s} â”‚{'':20s}â”‚   0% â”‚ {runtime_str} â”‚ {error_msg[:30]}")
                            
                    except Exception as e:
                        self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹å¤±æ•—: {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = f"é€²ç¨‹ç›£æ§å¤±æ•—: {e}"
                        current_failed_count += 1
                        print(f"âš ï¸  {display_name:30s} â”‚ ç›£æ§éŒ¯èª¤ â”‚   0% â”‚ {str(e)[:30]}")
            
            # ğŸ”¥ å¯¦æ™‚å¤±æ•—ç‡æª¢æŸ¥
            total_processed = current_failed_count + current_completed_count
            if total_processed > 0:
                failure_rate = (current_failed_count / total_processed) * 100
                
                # å¦‚æœå¤±æ•—ç‡éé«˜ä¸”é‚„æœ‰é‹è¡Œä¸­çš„é€²ç¨‹ï¼Œè€ƒæ…®å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
                if (failure_rate >= failure_rate_threshold and 
                    current_running_count > 0 and 
                    not auto_zero_fail_triggered and
                    current_failed_count >= failure_threshold):
                    
                    self.logger.warning(f"ğŸš¨ å¤±æ•—ç‡é”åˆ° {failure_rate:.1f}% ({current_failed_count}/{total_processed})ï¼Œè‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
                    self._enable_zero_fail_mode_dynamically()
                    auto_zero_fail_triggered = True
                    
                    # ç«‹å³æ•‘æ´å·²å¤±æ•—çš„ DB
                    self._rescue_failed_dbs_immediately(active_syncs)
                    
                    # åˆ‡æ›åˆ°é›¶å¤±æ•—ç›£æ§
                    self._wait_for_all_syncs_enhanced_zero_fail(active_syncs)
                    return
            
            # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
            print("-"*100)
            print(f"ğŸ“ˆ ç¸½è¨ˆ: é‹è¡Œä¸­ {current_running_count} | å®Œæˆ {current_completed_count} | å¤±æ•— {current_failed_count}")
            
            if current_failed_count > 0:
                total_dbs = len(active_syncs)
                failure_rate = (current_failed_count / total_dbs) * 100
                print(f"ğŸ“Š å¤±æ•—ç‡: {failure_rate:.1f}% ({current_failed_count}/{total_dbs})")
                
                if failure_rate >= failure_rate_threshold * 0.7:  # 70% of threshold
                    print(f"âš ï¸  æ¥è¿‘é›¶å¤±æ•—æ¨¡å¼è§¸ç™¼é–¾å€¼ ({failure_rate_threshold}%)")
            
            # ğŸ”¥ é›¶å¤±æ•—æ¨¡å¼æç¤º
            if not self.zero_fail_mode and not auto_zero_fail_triggered:
                if current_failed_count >= failure_threshold - 1:
                    print(f"ğŸ¯ æ™ºèƒ½æç¤º: å†æœ‰ {failure_threshold - current_failed_count} å€‹å¤±æ•—å°‡è‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
            
            if all_complete or (time.time() - start_wait) > max_wait_time:
                break
            
            # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
            time.sleep(check_interval)
        
        # æœ€çµ‚çµ±è¨ˆ
        completed = sum(1 for db in active_syncs if db.sync_process and db.sync_process.poll() == 0)
        failed = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        
        print(f"\nğŸ“‹ Repo sync æœ€çµ‚çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸ: {completed}")
        print(f"   âŒ å¤±æ•—: {failed}")
        
        # ğŸ”¥ å¦‚æœæœ€çµ‚é‚„æœ‰å¤±æ•—ä¸”æœªå•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼ï¼Œè©¢å•æ˜¯å¦å•Ÿç”¨
        if failed > 0 and not self.zero_fail_mode and not auto_zero_fail_triggered:
            print(f"\nğŸ¤” æª¢æ¸¬åˆ° {failed} å€‹å¤±æ•—çš„ DB")
            if hasattr(sys, 'stdin') and sys.stdin.isatty():
                response = input("æ˜¯å¦è¦å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼é€²è¡Œæ•‘æ´? (y/N): ").strip().lower()
                if response == 'y':
                    self.logger.info("ğŸ¯ ç”¨æˆ¶æ‰‹å‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
                    self._enable_zero_fail_mode_dynamically()
                    self._rescue_failed_dbs_immediately(active_syncs)
        
        self.logger.info(f"ğŸ“‹ Repo sync å®Œæˆçµ±è¨ˆ: æˆåŠŸ {completed}, å¤±æ•— {failed}")

    def _check_for_critical_errors(self, db_info: DBInfo, tracker: dict) -> str:
        """æª¢æŸ¥åš´é‡éŒ¯èª¤ï¼Œå¯èƒ½è§¸ç™¼é›¶å¤±æ•—æ¨¡å¼"""
        try:
            log_file = tracker.get('log_file', '')
            
            if not log_file or not os.path.exists(log_file):
                return ""
            
            current_size = os.path.getsize(log_file)
            
            # åªè®€å–æœ€å¾Œ 2KB çš„å…§å®¹
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    read_size = min(current_size, 2048)
                    f.seek(max(0, current_size - read_size))
                    recent_content = f.read()
                    
                    # ğŸ”¥ åˆ†ç´šéŒ¯èª¤æª¢æ¸¬
                    fatal_errors = [
                        'fatal: not a git repository',
                        'fatal: unable to access',
                        'fatal: repository',
                    ]
                    
                    critical_errors = [
                        'error: Unable to fully sync the tree',
                        'error: Downloading network changes failed',
                        'GitCommandError',
                        'Repo command failed',
                    ]
                    
                    warning_errors = [
                        'error: Checking out local projects failed',
                        'Cannot checkout',
                        'timeout',
                    ]
                    
                    # æª¢æŸ¥è‡´å‘½éŒ¯èª¤
                    for error in fatal_errors:
                        if error in recent_content:
                            return f"FATAL:{error}"
                    
                    # æª¢æŸ¥åš´é‡éŒ¯èª¤
                    for error in critical_errors:
                        if error in recent_content:
                            return f"CRITICAL:{error}"
                    
                    # æª¢æŸ¥è­¦å‘Šç´šéŒ¯èª¤
                    for error in warning_errors:
                        if error in recent_content:
                            return f"WARNING:{error}"
                            
            except Exception as e:
                self.logger.debug(f"æª¢æŸ¥åš´é‡éŒ¯èª¤æ™‚è®€å–æ—¥èªŒå¤±æ•—: {e}")
            
            return ""
            
        except Exception as e:
            self.logger.debug(f"åš´é‡éŒ¯èª¤æª¢æŸ¥ç•°å¸¸: {e}")
            return ""
            
    def _check_and_enable_zero_fail_mode(self, active_syncs: List[DBInfo], auto_triggered: bool) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼"""
        if self.zero_fail_mode or auto_triggered:
            return False
        
        failed_count = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        total_count = len(active_syncs)
        
        if total_count == 0:
            return False
        
        failure_rate = (failed_count / total_count) * 100
        
        # è§¸ç™¼æ¢ä»¶ï¼š
        # 1. å¤±æ•—æ•¸é‡ >= 2 å€‹
        # 2. å¤±æ•—ç‡ >= 30%
        # 3. æˆ–è€…å¤±æ•—æ•¸é‡ >= ç¸½æ•¸çš„ä¸€åŠ
        should_trigger = (
            failed_count >= 2 or
            failure_rate >= 30.0 or
            failed_count >= total_count // 2
        )
        
        if should_trigger:
            self.logger.warning(f"ğŸš¨ è§¸ç™¼é›¶å¤±æ•—æ¨¡å¼æ¢ä»¶: å¤±æ•— {failed_count}/{total_count} ({failure_rate:.1f}%)")
            self._enable_zero_fail_mode_dynamically()
            return True
        
        return False
        
    def _wait_for_all_syncs_enhanced_zero_fail(self, db_results: List[DBInfo]):
        """é›¶å¤±æ•—å®¹å¿çš„é€²åº¦ç›£æ§"""
        max_wait_time = config_manager.repo_config['sync_timeout']
        start_wait = time.time()
        
        active_syncs = [db for db in db_results if db.sync_process and db.status != DBStatus.FAILED]
        self.logger.info(f"ğŸ” é›¶å¤±æ•—ç›£æ§ {len(active_syncs)} å€‹æ´»èºçš„ repo sync é€²ç¨‹")
        
        if not active_syncs:
            return
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
        progress_tracker = {}
        for db_info in active_syncs:
            progress_tracker[db_info.db_info] = {
                'start_time': db_info.start_time or datetime.now(),
                'log_file': self._get_sync_log_file(db_info),
                'error_count': 0,
                'estimated_progress': 0,
                'current_activity': 'åˆå§‹åŒ–ä¸­...',
                'rescue_attempts': 0,  # ğŸ”¥ æ–°å¢ï¼šæ•‘æ´å˜—è©¦æ¬¡æ•¸
                'last_rescue_time': None,  # ğŸ”¥ æ–°å¢ï¼šæœ€å¾Œæ•‘æ´æ™‚é–“
            }
        
        check_interval = 3
        rescue_interval = 30  # ğŸ”¥ æ¯30ç§’æª¢æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦æ•‘æ´
        
        while True:
            all_complete = True
            elapsed = int(time.time() - start_wait)
            current_time = datetime.now()
            
            print("\033[2J\033[H")
            print(f"ğŸ”„ é›¶å¤±æ•— Repo Sync ç›£æ§ - {elapsed//60:02d}:{elapsed%60:02d}")
            print("="*80)
            
            completed_count = 0
            failed_count = 0
            
            for db_info in active_syncs:
                if db_info.status == DBStatus.FAILED:
                    failed_count += 1
                    continue
                
                db_name = db_info.db_info
                tracker = progress_tracker[db_name]
                
                display_name = f"{db_name} v{db_info.version}" if db_info.version else db_name
                
                if db_info.sync_process:
                    poll = db_info.sync_process.poll()
                    
                    if poll is None:  # ä»åœ¨é‹è¡Œ
                        all_complete = False
                        
                        # ğŸ”¥ å®šæœŸæª¢æŸ¥æ˜¯å¦éœ€è¦æ•‘æ´
                        should_rescue = (
                            (current_time - tracker.get('last_rescue_time', tracker['start_time'])).total_seconds() > rescue_interval and
                            tracker['rescue_attempts'] < 5  # æœ€å¤š5æ¬¡æ•‘æ´å˜—è©¦
                        )
                        
                        if should_rescue:
                            tracker['log_file'] = self._get_sync_log_file(db_info)
                            error_detected = self._check_for_sync_errors(db_info, tracker)
                            
                            if error_detected or self._is_sync_stuck(db_info, tracker):
                                tracker['rescue_attempts'] += 1
                                tracker['last_rescue_time'] = current_time
                                
                                self.logger.warning(f"{db_name}: ğŸš¨ ç¬¬ {tracker['rescue_attempts']} æ¬¡æ•‘æ´ - {error_detected or 'é€²åº¦åœæ»¯'}")
                                
                                if self._immediate_rescue(db_info, error_detected or 'é€²åº¦åœæ»¯'):
                                    tracker['current_activity'] = f"æ•‘æ´ #{tracker['rescue_attempts']} æˆåŠŸï¼Œé‡æ–°é–‹å§‹..."
                                    tracker['log_file'] = self._get_sync_log_file(db_info)
                                else:
                                    self.logger.error(f"{db_name}: âŒ ç¬¬ {tracker['rescue_attempts']} æ¬¡æ•‘æ´å¤±æ•—")
                                    if tracker['rescue_attempts'] >= 5:
                                        db_info.status = DBStatus.FAILED
                                        db_info.error_message = f"ç¶“é {tracker['rescue_attempts']} æ¬¡æ•‘æ´ä»ç„¶å¤±æ•—"
                                        failed_count += 1
                                        continue
                        
                        # æ›´æ–°é€²åº¦
                        self._update_progress_info(db_info, tracker)
                        
                        runtime = datetime.now() - tracker['start_time']
                        runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                        
                        progress = tracker['estimated_progress']
                        bar_length = 20
                        filled = int(bar_length * progress / 100)
                        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
                        
                        activity = tracker.get('current_activity', 'åŒæ­¥ä¸­')
                        
                        # é¡¯ç¤ºæ•‘æ´ç‹€æ…‹
                        status_info = ""
                        if tracker['rescue_attempts'] > 0:
                            status_info += f" R{tracker['rescue_attempts']}"
                        
                        print(f"ğŸ”„ {display_name:20s} â”‚{bar}â”‚ {progress:3d}% â”‚ {runtime_str} â”‚ {activity}{status_info}")
                        
                    elif poll == 0:  # æˆåŠŸå®Œæˆ
                        # ğŸ”¥ é›¶å®¹å¿ï¼šå³ä½¿è¿”å›ç¢¼æ˜¯0ï¼Œä¹Ÿè¦æª¢æŸ¥æ˜¯å¦çœŸçš„100%æˆåŠŸ
                        success_rate, failed_projects = self._analyze_sync_result(db_info)
                        
                        if failed_projects:
                            self.logger.warning(f"{db_name}: ğŸš¨ å³ä½¿è¿”å›ç¢¼0ï¼Œä»æœ‰å¤±æ•—é …ç›®ï¼Œå•Ÿå‹•æ•‘æ´")
                            
                            if self._enhanced_repair_failed_projects_zero_tolerance(db_info, failed_projects):
                                completed_count += 1
                                db_info.status = DBStatus.SUCCESS
                                db_info.end_time = datetime.now()
                                
                                runtime = datetime.now() - tracker['start_time']
                                runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                                bar = "â–ˆ" * 20
                                print(f"âœ… {display_name:20s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ æ•‘æ´æˆåŠŸé”åˆ°100%")
                            else:
                                # æ•‘æ´å¤±æ•—ï¼Œæ¨™è¨˜ç‚ºå¤±æ•—
                                failed_count += 1
                                db_info.status = DBStatus.FAILED
                                db_info.error_message = f"å®Œæˆå¾Œæª¢æ¸¬åˆ°å¤±æ•—é …ç›®ä¸”æ•‘æ´å¤±æ•—: {len(failed_projects)} å€‹"
                                
                                runtime = datetime.now() - tracker['start_time']
                                runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                                print(f"âŒ {display_name:20s} â”‚{'':20s}â”‚   0% â”‚ {runtime_str} â”‚ æ•‘æ´å¤±æ•—")
                                continue
                        else:
                            # çœŸæ­£çš„100%æˆåŠŸ
                            completed_count += 1
                            db_info.status = DBStatus.SUCCESS
                            db_info.end_time = datetime.now()
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            bar = "â–ˆ" * 20
                            print(f"âœ… {display_name:20s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ å®Œç¾å®Œæˆ")
                    
                    elif poll == 1:  # ğŸ”¥ è¿”å›ç¢¼1 - ç«‹å³å•Ÿå‹•æ•‘æ´
                        success_rate, failed_projects = self._analyze_sync_result(db_info)
                        
                        self.logger.warning(f"{db_name}: ğŸš¨ è¿”å›ç¢¼1ï¼ŒæˆåŠŸç‡ {success_rate:.1f}%ï¼Œ{len(failed_projects)} å€‹å¤±æ•—é …ç›®")
                        
                        if self._enhanced_repair_failed_projects_zero_tolerance(db_info, failed_projects):
                            completed_count += 1
                            db_info.status = DBStatus.SUCCESS
                            db_info.end_time = datetime.now()
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            bar = "â–ˆ" * 20
                            print(f"âœ… {display_name:20s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ é›¶å®¹å¿æ•‘æ´æˆåŠŸ")
                        else:
                            failed_count += 1
                            db_info.status = DBStatus.FAILED
                            db_info.error_message = f"é›¶å®¹å¿æ•‘æ´å¤±æ•—ï¼Œç„¡æ³•é”åˆ°100%"
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            print(f"âŒ {display_name:20s} â”‚{'':20s}â”‚   0% â”‚ {runtime_str} â”‚ é›¶å®¹å¿æ•‘æ´å¤±æ•—")
                    
                    else:  # å…¶ä»–éŒ¯èª¤ç¢¼
                        self.logger.error(f"{db_name}: ğŸš¨ åš´é‡éŒ¯èª¤ (è¿”å›ç¢¼: {poll})ï¼Œå•Ÿå‹•æ ¸æ­¦ç´šæ•‘æ´")
                        
                        if self._nuclear_rebuild(db_info):
                            # æ ¸æ­¦ç´šæ•‘æ´æˆåŠŸï¼Œé‡æ–°é–‹å§‹ç›£æ§
                            all_complete = False
                            tracker['rescue_attempts'] += 1
                            tracker['current_activity'] = "æ ¸æ­¦ç´šæ•‘æ´é‡æ–°é–‹å§‹..."
                            tracker['log_file'] = self._get_sync_log_file(db_info)
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            print(f"â˜¢ï¸ {display_name:20s} â”‚{'â–‘'*20}â”‚   0% â”‚ {runtime_str} â”‚ æ ¸æ­¦ç´šæ•‘æ´ä¸­...")
                        else:
                            failed_count += 1
                            db_info.status = DBStatus.FAILED
                            db_info.error_message = f"æ ¸æ­¦ç´šæ•‘æ´ä¹Ÿå¤±æ•— (åŸè¿”å›ç¢¼: {poll})"
                            
                            runtime = datetime.now() - tracker['start_time']
                            runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                            print(f"ğŸ’€ {display_name:20s} â”‚{'':20s}â”‚   0% â”‚ {runtime_str} â”‚ ç„¡æ³•æ•‘æ´")
            
            # çµ±è¨ˆä¿¡æ¯
            running_count = len(active_syncs) - completed_count - failed_count
            
            print("-" * 80)
            print(f"ğŸ“Š é‹è¡Œ:{running_count} â”‚ å®Œæˆ:{completed_count} â”‚ å¤±æ•—:{failed_count}")
            
            if all_complete or elapsed > max_wait_time:
                break
            
            time.sleep(check_interval)
        
        # æœ€çµ‚çµ±è¨ˆ
        self._display_final_summary_zero_fail(active_syncs, elapsed, progress_tracker)

    def _check_for_sync_errors(self, db_info: DBInfo, tracker: dict) -> str:
        """ç°¡åŒ–çš„éŒ¯èª¤æª¢æ¸¬ - å°ˆé–€é‡å° fatal éŒ¯èª¤"""
        try:
            log_file = tracker.get('log_file', '')
            
            if not log_file or not os.path.exists(log_file):
                return ""
            
            # è®€å–æœ€å¾Œ 4KB çš„å…§å®¹
            current_size = os.path.getsize(log_file)
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    read_size = min(current_size, 4096)
                    f.seek(max(0, current_size - read_size))
                    recent_content = f.read()
                    
                    # ğŸ”¥ æª¢æ¸¬åš´é‡éŒ¯èª¤ï¼Œè‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
                    critical_errors = [
                        'fatal: not a git repository',
                        'error: Unable to fully sync the tree',
                        'GitCommandError'
                    ]
                    
                    for keyword in critical_errors:
                        if keyword in recent_content:
                            self.logger.error(f"{db_info.db_info}: ğŸš¨ æª¢æ¸¬åˆ°é—œéµéŒ¯èª¤: {keyword}")
                            
                            # ğŸ”¥ è‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼
                            if not self.zero_fail_mode:
                                self.logger.warning(f"ğŸ¯ å› åš´é‡éŒ¯èª¤è‡ªå‹•å•Ÿç”¨é›¶å¤±æ•—æ¨¡å¼")
                                self._enable_zero_fail_mode_dynamically()
                            
                            return f"CRITICAL:Git éŒ¯èª¤ - {keyword}"
                            
            except Exception as e:
                self.logger.debug(f"æª¢æŸ¥éŒ¯èª¤æ™‚è®€å–æ—¥èªŒå¤±æ•—: {e}")
            
            return ""
            
        except Exception as e:
            self.logger.debug(f"éŒ¯èª¤æª¢æŸ¥ç•°å¸¸: {e}")
            return ""

    def _is_sync_stuck(self, db_info: DBInfo, tracker: dict) -> bool:
        """æª¢æŸ¥åŒæ­¥æ˜¯å¦å¡ä½ï¼ˆé€²åº¦é•·æ™‚é–“æ²’è®ŠåŒ–ï¼‰"""
        try:
            current_progress = tracker.get('estimated_progress', 0)
            last_check_time = tracker.get('last_progress_check_time')
            last_progress = tracker.get('last_progress_value', 0)
            
            current_time = datetime.now()
            
            if last_check_time:
                time_diff = (current_time - last_check_time).total_seconds()
                progress_diff = current_progress - last_progress
                
                # å¦‚æœ5åˆ†é˜å…§é€²åº¦è®ŠåŒ–å°‘æ–¼1%ï¼Œèªç‚ºå¡ä½äº†
                if time_diff > 300 and progress_diff < 1:
                    return True
            
            # æ›´æ–°æª¢æŸ¥é»
            tracker['last_progress_check_time'] = current_time
            tracker['last_progress_value'] = current_progress
            
            return False
            
        except Exception:
            return False

    def _immediate_rescue(self, db_info: DBInfo, error_msg: str) -> bool:
        """ç«‹å³æ•‘æ´ï¼ˆä¸ç­‰å¾…é€²ç¨‹çµæŸï¼‰"""
        try:
            self.logger.warning(f"{db_info.db_info}: ğŸš¨ ç«‹å³æ•‘æ´: {error_msg}")
            
            # å¼·åˆ¶çµ‚æ­¢ç•¶å‰é€²ç¨‹
            if db_info.sync_process:
                try:
                    if db_info.sync_process.poll() is None:
                        db_info.sync_process.kill()
                        time.sleep(3)
                except:
                    pass
                finally:
                    resource_manager.unregister_process(db_info.db_info)
            
            # å¿«é€Ÿé‡å•Ÿ
            process = self.repo_manager.start_repo_sync_async(
                db_info.local_path, 
                db_info.db_info
            )
            
            if process:
                db_info.sync_process = process
                self.logger.info(f"{db_info.db_info}: âœ… ç«‹å³æ•‘æ´é‡å•ŸæˆåŠŸ (PID: {process.pid})")
                return True
            else:
                self.logger.error(f"{db_info.db_info}: âŒ ç«‹å³æ•‘æ´é‡å•Ÿå¤±æ•—")
                return False
                
        except Exception as e:
            self.logger.error(f"{db_info.db_info}: ğŸ’¥ ç«‹å³æ•‘æ´ç•°å¸¸: {e}")
            return False

    def _display_final_summary_zero_fail(self, active_syncs: list, elapsed: int, progress_tracker: dict):
        """é¡¯ç¤ºé›¶å¤±æ•—æœ€çµ‚æ‘˜è¦"""
        completed = sum(1 for db in active_syncs if db.status == DBStatus.SUCCESS)
        failed = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        total_rescues = sum(progress_tracker.get(db.db_info, {}).get('rescue_attempts', 0) for db in active_syncs)
        
        print(f"\nğŸ é›¶å¤±æ•— Repo Sync æœ€çµ‚å ±å‘Š")
        print("=" * 60)
        print(f"â±ï¸  ç¸½ç”¨æ™‚: {elapsed//60:02d}:{elapsed%60:02d}")
        print(f"âœ… æˆåŠŸ: {completed}")
        print(f"âŒ å¤±æ•—: {failed}")
        print(f"ğŸš¨ ç¸½æ•‘æ´æ¬¡æ•¸: {total_rescues}")
        
        if failed > 0:
            print(f"\nğŸ’€ é›¶å®¹å¿æ•‘æ´ä¹Ÿç„¡æ³•æŒ½æ•‘çš„DB:")
            for db in active_syncs:
                if db.status == DBStatus.FAILED:
                    rescues = progress_tracker.get(db.db_info, {}).get('rescue_attempts', 0)
                    print(f"  - {db.db_info}: {db.error_message} (æ•‘æ´å˜—è©¦: {rescues})")
        
        success_rate = (completed / (completed + failed) * 100) if (completed + failed) > 0 else 0
        print(f"ğŸ¯ é›¶å¤±æ•—é”æˆç‡: {success_rate:.1f}%")
        
        if success_rate == 100.0:
            print("ğŸ‰ æ­å–œï¼é”åˆ°é›¶å¤±æ•—ç›®æ¨™ï¼")
        else:
            print("ğŸ˜ æœªèƒ½é”åˆ°é›¶å¤±æ•—ç›®æ¨™ï¼Œéœ€è¦æª¢æŸ¥å¤±æ•—åŸå› ")
        
        print("=" * 60)

    def _final_zero_fail_verification(self):
        """é›¶å¤±æ•—æœ€çµ‚é©—è­‰"""
        self.logger.info("ğŸ” åŸ·è¡Œé›¶å¤±æ•—æœ€çµ‚é©—è­‰...")
        
        failed_dbs = [db for db in self.report.db_details if db.status == DBStatus.FAILED]
        
        if failed_dbs:
            self.logger.error(f"ğŸ’€ é›¶å¤±æ•—é©—è­‰å¤±æ•—ï¼ä»æœ‰ {len(failed_dbs)} å€‹ DB å¤±æ•—:")
            for db in failed_dbs:
                self.logger.error(f"  - {db.db_info}: {db.error_message}")
        
        # é‡æ–°çµ±è¨ˆ
        final_failed = sum(1 for db in self.report.db_details if db.status == DBStatus.FAILED)
        final_success = sum(1 for db in self.report.db_details if db.status == DBStatus.SUCCESS)
        
        if final_failed == 0:
            self.logger.info("ğŸ‰ é›¶å¤±æ•—é©—è­‰é€šéï¼æ‰€æœ‰ DB éƒ½æˆåŠŸäº†ï¼")
        else:
            self.logger.error(f"ğŸ’€ é›¶å¤±æ•—ç›®æ¨™æœªé”æˆï¼Œæœ€çµ‚ä»æœ‰ {final_failed} å€‹å¤±æ•—")

    def _update_progress_info(self, db_info: DBInfo, tracker: dict):
        """æ›´æ–°é€²åº¦ä¿¡æ¯ - å°ˆé–€å„ªåŒ– unbuffer è¼¸å‡ºè§£æ"""
        try:
            log_file = tracker.get('log_file')
            
            # ğŸ”¥ æ¯æ¬¡éƒ½é‡æ–°ç²å–æ—¥èªŒæ–‡ä»¶ï¼Œç¢ºä¿ä½¿ç”¨æœ€æ–°çš„ unbuffer æ—¥èªŒ
            current_log_file = self._get_sync_log_file(db_info)
            if current_log_file and current_log_file != log_file:
                tracker['log_file'] = current_log_file
                log_file = current_log_file
                self.logger.debug(f"{db_info.db_info}: åˆ‡æ›åˆ°æ–°æ—¥èªŒæ–‡ä»¶: {os.path.basename(log_file)}")
            
            if not log_file or not os.path.exists(log_file):
                tracker['current_activity'] = 'ç­‰å¾…æ—¥èªŒ...'
                tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                return
            
            # ğŸ”¥ å„ªåŒ–çš„æ—¥èªŒè§£æ - å°ˆé–€è™•ç† unbuffer æ ¼å¼
            try:
                file_size = os.path.getsize(log_file)
                
                # åªè®€å–æœ€å¾Œ 2KB é¿å…è™•ç†å¤§æ–‡ä»¶
                read_size = min(file_size, 2048)
                
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(max(0, file_size - read_size))
                    content = f.read()
                    lines = content.split('\n')[-15:]  # æœ€å¾Œ15è¡Œ
                
                # è§£ææœ€æ–°çš„åŒæ­¥ç‹€æ…‹
                latest_progress = 0
                latest_activity = "åŒæ­¥ä¸­..."
                current_project = ""
                total_projects = 0
                current_count = 0
                
                # ğŸ”¥ é‡é»ï¼šè§£æ unbuffer è¼¸å‡ºçš„ç‰¹å®šæ ¼å¼
                for line in reversed(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£æ "Syncing: X% (current/total) time | jobs | project" æ ¼å¼
                    if "Syncing:" in line and "%" in line:
                        import re
                        
                        # åŒ¹é…é€²åº¦ç™¾åˆ†æ¯”å’Œè¨ˆæ•¸
                        progress_match = re.search(r'Syncing:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                        if progress_match:
                            latest_progress = int(progress_match.group(1))
                            current_count = int(progress_match.group(2))
                            total_projects = int(progress_match.group(3))
                            
                            # æå–ç•¶å‰è™•ç†çš„é …ç›®
                            # æ ¼å¼é€šå¸¸æ˜¯: "... | X jobs | time project_path @ ..."
                            if '|' in line:
                                parts = line.split('|')
                                for part in parts:
                                    part = part.strip()
                                    # å°‹æ‰¾åŒ…å«é …ç›®è·¯å¾‘çš„éƒ¨åˆ†
                                    if '@' in part:
                                        project_info = part.split('@')[-1].strip()
                                    elif '/' in part and 'job' not in part and ':' not in part:
                                        project_info = part
                                    else:
                                        continue
                                    
                                    # æå–é …ç›®åç¨±
                                    if '/' in project_info:
                                        current_project = project_info.split('/')[-1]
                                    else:
                                        current_project = project_info
                                    break
                            
                            # æ§‹å»ºæ´»å‹•æè¿°
                            latest_activity = f"åŒæ­¥: {current_count}/{total_projects}"
                            if current_project:
                                # é™åˆ¶é …ç›®åç¨±é•·åº¦
                                project_name = current_project[:20] + "..." if len(current_project) > 20 else current_project
                                latest_activity += f" - {project_name}"
                            
                            break
                    
                    # ğŸ”¥ è§£æå…¶ä»–ç‹€æ…‹ä¿¡æ¯
                    elif "Fetching project" in line:
                        project_match = re.search(r'Fetching project\s+([^\s]+)', line)
                        if project_match:
                            project_path = project_match.group(1)
                            current_project = project_path.split('/')[-1]
                            latest_activity = f"ç²å–: {current_project}"
                    
                    elif "Skipped fetching project" in line:
                        project_match = re.search(r'Skipped fetching project\s+([^\s]+)', line)
                        if project_match:
                            project_path = project_match.group(1)
                            current_project = project_path.split('/')[-1]
                            latest_activity = f"è·³é: {current_project}"
                
                # ğŸ”¥ æ›´æ–°è¿½è¸ªä¿¡æ¯
                if latest_progress > 0:
                    tracker['estimated_progress'] = latest_progress
                else:
                    # å¦‚æœæ²’æœ‰è§£æåˆ°é€²åº¦ï¼Œä½¿ç”¨æ™‚é–“ä¼°ç®—
                    tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                
                tracker['current_activity'] = latest_activity
                tracker['current_project'] = current_project
                tracker['total_projects'] = total_projects
                tracker['current_count'] = current_count
                tracker['last_update'] = datetime.now()
                
                # ğŸ”¥ èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
                if latest_progress > 0:
                    self.logger.debug(f"{db_info.db_info}: è§£ææˆåŠŸ - {latest_progress}% ({current_count}/{total_projects}) {current_project}")
                    
            except Exception as e:
                self.logger.debug(f"{db_info.db_info}: è§£ææ—¥èªŒå¤±æ•—: {e}")
                tracker['current_activity'] = 'è§£æå¤±æ•—'
                tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                
        except Exception as e:
            self.logger.debug(f"{db_info.db_info}: é€²åº¦æ›´æ–°å¤±æ•—: {e}")
            tracker['current_activity'] = 'æ›´æ–°å¤±æ•—'
            tracker['estimated_progress'] = self._get_time_based_progress(tracker)

    def _get_time_based_progress(self, tracker: dict) -> int:
        """åŸºæ–¼æ™‚é–“çš„é€²åº¦ä¼°ç®—"""
        runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
        # æ¯åˆ†é˜ç´„1.5%çš„é€²åº¦ï¼Œæœ€å¤š95%
        return min(int(runtime_minutes * 1.5), 95)

    def process_selected_dbs(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """è™•ç†é¸å®šçš„ DB"""
        if config_manager.repo_config['async_sync'] and not self.dry_run:
            self.process_dbs_async(db_list, db_versions)
        else:
            # åŒæ­¥è™•ç†é‚è¼¯
            for db_name in db_list:
                if '#' in db_name:
                    db_name, version = db_name.split('#', 1)
                else:
                    version = db_versions.get(db_name) if db_versions else None
                
                for db_info in self.get_all_dbs('all'):
                    if db_info.db_info == db_name:
                        db_info.version = version
                        
                        db_info = self.process_db_phase1(db_info)
                        
                        if not self.dry_run and db_info.sync_process:
                            self.logger.info(f"ç­‰å¾… {db_name} sync å®Œæˆ...")
                            db_info.sync_process.wait()
                        
                        db_info = self.process_db_phase2(db_info)
                        self.report.add_db(db_info)
                        break

    def generate_report(self, output_file: str = None):
        """ç”¢ç”Ÿæ”¹é€²ç‰ˆå ±å‘Š - ä¿®æ­£çµ±è¨ˆé‚è¼¯"""
        self.report.finalize()
        
        if not output_file:
            output_file = os.path.join(
                self.output_dir, 
                config_manager.path_config['report_filename']
            )
        
        try:
            self.logger.info("é–‹å§‹ç”¢ç”Ÿ Excel å ±å‘Š")
            
            report_data = []
            for i, db in enumerate(self.report.db_details, 1):
                # æ‰‹å‹•æ§‹å»ºå­—å…¸ï¼Œç¢ºä¿æ‰€æœ‰æ¬„ä½æ­£ç¢º
                db_dict = {
                    'sn': i,
                    'module': db.module,
                    'db_type': db.db_type,
                    'db_info': db.db_info,
                    'status': db.status.value if hasattr(db.status, 'value') else str(db.status),
                    'version': db.version or 'æœªæŒ‡å®š',
                    'start_time': db.start_time.strftime('%Y-%m-%d %H:%M:%S') if db.start_time else '',
                    'end_time': db.end_time.strftime('%Y-%m-%d %H:%M:%S') if db.end_time else '',
                    'error_message': db.error_message or '',
                    'sync_log_path': db.sync_log_path or '',
                    'sftp_path': db.sftp_path,
                    'local_path': db.local_path or '',
                    'has_existing_repo': 'æ˜¯' if db.has_existing_repo else 'å¦',
                    'jira_link': db.jira_link or 'æœªæ‰¾åˆ°',
                }
                
                # é‡æ–°å‘½åæ¬„ä½
                db_dict['å®Œæ•´_JIRA_é€£çµ'] = db.jira_link or 'æœªæ‰¾åˆ°'
                db_dict['å®Œæ•´_repo_init_æŒ‡ä»¤'] = db.actual_source_cmd or 'æœªè¨˜éŒ„'
                db_dict['manifest_ç‰ˆæœ¬'] = db.version or 'æœªæŒ‡å®š'
                
                report_data.append(db_dict)
            
            df = pd.DataFrame(report_data)
            
            # é‡æ–°æ’åˆ—æ¬„ä½é †åº
            important_columns = [
                'sn', 'module', 'db_type', 'db_info', 'status',
                'manifest_ç‰ˆæœ¬', 'å®Œæ•´_JIRA_é€£çµ', 'å®Œæ•´_repo_init_æŒ‡ä»¤',
                'start_time', 'end_time', 'sync_log_path', 'error_message'
            ]
            
            existing_columns = [col for col in important_columns if col in df.columns]
            other_columns = [col for col in df.columns if col not in important_columns]
            df = df[existing_columns + other_columns]
            
            # ğŸ”¥ ä¿®æ­£ï¼šé‡æ–°è¨ˆç®—çµ±è¨ˆï¼ŒåŸºæ–¼å¯¦éš›çš„ status å€¼
            status_counts = df['status'].value_counts()
            successful_count = status_counts.get('âœ… å®Œæˆ', 0)
            failed_count = status_counts.get('âŒ å¤±æ•—', 0)
            skipped_count = status_counts.get('â­ï¸ è·³é', 0)
            
            self.logger.info(f"å ±å‘Šçµ±è¨ˆ: æˆåŠŸ {successful_count}, å¤±æ•— {failed_count}, è·³é {skipped_count}")
            
            # å»ºç«‹æ‘˜è¦
            summary = {
                'é …ç›®': ['ç¸½ DB æ•¸', 'æˆåŠŸ', 'å¤±æ•—', 'è·³é', 'åŸ·è¡Œæ™‚é–“'],
                'æ•¸å€¼': [
                    len(self.report.db_details),
                    successful_count,
                    failed_count,
                    skipped_count,
                    str(self.report.end_time - self.report.start_time) if self.report.end_time else 'N/A'
                ]
            }
            summary_df = pd.DataFrame(summary)
            
            # å¯«å…¥æ”¹é€²ç‰ˆ Excel
            self._write_enhanced_excel(df, summary_df, output_file)
            
            self.logger.info(f"Excel å ±å‘Šå·²ç”¢ç”Ÿ: {output_file}")
            print(f"\nğŸ“Š Excel å ±å‘Šå·²ç”¢ç”Ÿ: {output_file}")
            
        except Exception as e:
            self.logger.error(f"ç”¢ç”Ÿå ±å‘Šå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

    def _write_enhanced_excel(self, main_df: pd.DataFrame, summary_df: pd.DataFrame, output_file: str):
        """å¯«å…¥æ”¹é€²ç‰ˆ Excelï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰"""
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # å¯«å…¥ä¸»è¦è³‡æ–™
                main_df.to_excel(writer, sheet_name='è©³ç´°è³‡è¨Š', index=False)
                summary_df.to_excel(writer, sheet_name='æ‘˜è¦', index=False)
                
                # æ ¼å¼åŒ–å·¥ä½œè¡¨
                for sheet_name in ['è©³ç´°è³‡è¨Š', 'æ‘˜è¦']:
                    worksheet = writer.sheets[sheet_name]
                    self._format_worksheet_enhanced(worksheet)
            
            self.logger.info(f"æˆåŠŸå¯«å…¥å¢å¼·ç‰ˆ Excel æª”æ¡ˆ: {output_file}")
            
        except Exception as e:
            self.logger.error(f"å¯«å…¥ Excel æª”æ¡ˆå¤±æ•—: {str(e)}")
            raise

    def _format_worksheet_enhanced(self, worksheet):
        """æ ¼å¼åŒ–å·¥ä½œè¡¨ï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰"""
        try:
            from openpyxl.styles import Font, PatternFill, Alignment
            from openpyxl.utils import get_column_letter
            
            # è¨­å®šè¡¨é ­æ ¼å¼ï¼ˆè—åº•ç™½å­—ï¼‰
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_font = Font(color="FFFFFF", bold=True)
            header_alignment = Alignment(horizontal="center", vertical="center")
            
            # æ‡‰ç”¨åˆ°ç¬¬ä¸€è¡Œï¼ˆè¡¨é ­ï¼‰
            for cell in worksheet[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = header_alignment
            
            # è‡ªå‹•èª¿æ•´æ¬„å¯¬
            for column in worksheet.columns:
                max_length = 0
                column_letter = get_column_letter(column[0].column)
                
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                
                # è¨­å®šæœ€å°å¯¬åº¦å’Œæœ€å¤§å¯¬åº¦
                adjusted_width = min(max(max_length + 2, 10), 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
            
            # è¨­å®šè¡Œé«˜
            for row in worksheet.iter_rows():
                worksheet.row_dimensions[row[0].row].height = 20
            
            # å‡çµç¬¬ä¸€è¡Œ
            worksheet.freeze_panes = 'A2'
            
        except Exception as e:
            self.logger.warning(f"æ ¼å¼åŒ–å·¥ä½œè¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# =====================================
# ===== äº’å‹•å¼ä»‹é¢ =====
# =====================================

class InteractiveUI:
    """äº’å‹•å¼ä½¿ç”¨è€…ä»‹é¢"""
    
    def __init__(self):
        self.tool = ManifestPinningTool()
        self.logger = setup_logger(self.__class__.__name__)
        self.selected_dbs = []
        self.db_versions = {}
        self.selected_db_type = 'all'
        
        self._load_default_config()
    
    def _load_default_config(self):
        """è¼‰å…¥é è¨­é…ç½®"""
        if config_manager.default_execution_config.get('sftp_override'):
            config_manager.apply_overrides(
                config_manager.default_execution_config['sftp_override'],
                source='default_config'
            )
        
        if config_manager.default_execution_config.get('output_dir'):
            self.tool.output_dir = config_manager.default_execution_config['output_dir']
    
    def setup_sftp(self):
        """è¨­å®š SFTP é€£ç·šè³‡è¨Š"""
        print("\nç›®å‰ SFTP è¨­å®š:")
        print(f"  Host: {config_manager.sftp_config['host']}")
        print(f"  Port: {config_manager.sftp_config['port']}")
        print(f"  Username: {config_manager.sftp_config['username']}")
        
        if input("\næ˜¯å¦è¦ä¿®æ”¹è¨­å®š? (y/N): ").strip().lower() == 'y':
            host = input(f"Host [{config_manager.sftp_config['host']}]: ").strip()
            if host:
                config_manager.sftp_config['host'] = host
                
            port = input(f"Port [{config_manager.sftp_config['port']}]: ").strip()
            if port:
                config_manager.sftp_config['port'] = int(port)
                
            username = input(f"Username [{config_manager.sftp_config['username']}]: ").strip()
            if username:
                config_manager.sftp_config['username'] = username
                
            password = input("Password (ç•™ç©ºä¿æŒåŸå€¼): ").strip()
            if password:
                config_manager.sftp_config['password'] = password
            
            print("âœ… SFTP è¨­å®šå·²æ›´æ–°")
    
    def display_current_settings(self):
        """é¡¯ç¤ºç›®å‰è¨­å®š"""
        print("\nç›®å‰è¨­å®š:")
        print(f"  Mapping table: {'å·²è¼‰å…¥' if self.tool.mapping_reader.df is not None else 'æœªè¼‰å…¥'}")
        print(f"  DB é¡å‹: {self.selected_db_type}")
        print(f"  é¸æ“‡çš„ DB: {len(self.selected_dbs)} å€‹")
        print(f"  è¨­å®šç‰ˆæœ¬çš„ DB: {len(self.db_versions)} å€‹")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        print(f"  å¹³è¡Œè™•ç†: {'å•Ÿç”¨' if config_manager.parallel_config['enable_parallel'] else 'é—œé–‰'}")
        print(f"  Worker æ•¸é‡: {config_manager.parallel_config['max_workers']}")
    
    def load_mapping_table(self):
        """è¼‰å…¥ mapping table"""
        default_mapping = config_manager.default_execution_config.get('mapping_table')
        
        if default_mapping and os.path.exists(default_mapping):
            print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ mapping table: {default_mapping}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­æª”æ¡ˆ? (Y/n): ").strip().lower() != 'n':
                file_path = default_mapping
            else:
                file_path = input(
                    f"è«‹è¼¸å…¥ mapping table è·¯å¾‘ [{config_manager.path_config['default_mapping_table']}]: "
                ).strip()
                if not file_path:
                    file_path = config_manager.path_config['default_mapping_table']
        else:
            default_path = config_manager.path_config['default_mapping_table']
            file_path = input(f"è«‹è¼¸å…¥ mapping table è·¯å¾‘ [{default_path}]: ").strip()
            if not file_path:
                file_path = default_path
        
        if not os.path.exists(file_path):
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            return
        
        if self.tool.load_mapping_table(file_path):
            print(f"âœ… æˆåŠŸè¼‰å…¥ mapping tableï¼Œå…± {len(self.tool.mapping_reader.df)} ç­†è³‡æ–™")
            all_dbs = self.tool.get_all_dbs()
            unique_db_names = list(set([db.db_info for db in all_dbs]))
            print(f"   æ‰¾åˆ° {len(unique_db_names)} å€‹ä¸é‡è¤‡çš„ DB")
            
            if config_manager.default_execution_config.get('db_type'):
                self.selected_db_type = config_manager.default_execution_config['db_type']
                print(f"   ğŸ“Œ ä½¿ç”¨é è¨­ DB é¡å‹: {self.selected_db_type}")
        else:
            print("âŒ è¼‰å…¥å¤±æ•—")
    
    def select_db_type(self):
        """é¸æ“‡ DB é¡å‹"""
        default_type = config_manager.default_execution_config.get('db_type')
        
        if default_type and default_type in ['all', 'master', 'premp', 'mp', 'mpbackup']:
            print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ DB é¡å‹: {default_type}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­å€¼? (Y/n): ").strip().lower() != 'n':
                self.selected_db_type = default_type
                print(f"âœ… å·²é¸æ“‡ DB é¡å‹: {self.selected_db_type}")
                return self.selected_db_type
        
        print("\né¸æ“‡ DB é¡å‹:")
        print("1. All (æ‰€æœ‰é¡å‹)")
        print("2. Master")
        print("3. PreMP")
        print("4. MP")
        print("5. MP Backup")
        
        choice = input("è«‹é¸æ“‡: ").strip()
        
        type_map = {
            '1': 'all',
            '2': 'master',
            '3': 'premp',
            '4': 'mp',
            '5': 'mpbackup'
        }
        
        self.selected_db_type = type_map.get(choice, 'all')
        print(f"âœ… å·²é¸æ“‡ DB é¡å‹: {self.selected_db_type}")
        return self.selected_db_type
    
    def select_dbs(self):
        """é¸æ“‡è¦å®šç‰ˆçš„ DB"""
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return []
        
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        if not all_db_infos:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {self.selected_db_type} é¡å‹çš„ DB")
            return []
        
        # å–å¾—ä¸é‡è¤‡çš„ DB åˆ—è¡¨
        unique_dbs = {}
        for db_info in all_db_infos:
            if db_info.db_info not in unique_dbs:
                unique_dbs[db_info.db_info] = db_info
        
        db_list = list(unique_dbs.keys())
        db_list.sort()
        
        # æª¢æŸ¥é è¨­ DB åˆ—è¡¨
        default_dbs = config_manager.default_execution_config.get('selected_dbs')
        
        if default_dbs:
            if default_dbs in ['all', '*']:
                print(f"\nğŸ“Œ é è¨­é…ç½®ç‚ºé¸æ“‡æ‰€æœ‰ {self.selected_db_type} é¡å‹çš„ DB")
                if input(f"æ˜¯å¦é¸æ“‡å…¨éƒ¨ {len(db_list)} å€‹ DB? (Y/n): ").strip().lower() != 'n':
                    self.selected_dbs = db_list
                    print(f"âœ… å·²é¸æ“‡æ‰€æœ‰ {len(db_list)} å€‹ DB")
                    return db_list
            
            elif isinstance(default_dbs, list) and len(default_dbs) > 0:
                parsed_dbs = []
                for db_spec in default_dbs:
                    if '#' in db_spec:
                        db_name, version = db_spec.split('#', 1)
                        if db_name in db_list:
                            parsed_dbs.append(db_name)
                            self.db_versions[db_name] = version
                    else:
                        if db_spec in db_list:
                            parsed_dbs.append(db_spec)
                
                if parsed_dbs:
                    print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ DB åˆ—è¡¨: {', '.join(default_dbs)}")
                    print(f"   å…¶ä¸­ {len(parsed_dbs)} å€‹ DB å­˜åœ¨æ–¼ç•¶å‰ mapping table")
                    if input("æ˜¯å¦ä½¿ç”¨é è¨­ DB åˆ—è¡¨? (Y/n): ").strip().lower() != 'n':
                        self.selected_dbs = parsed_dbs
                        print(f"âœ… å·²é¸æ“‡ {len(parsed_dbs)} å€‹ DB")
                        return parsed_dbs
        
        # é¡¯ç¤º DB åˆ—è¡¨å’Œé¸æ“‡é‚è¼¯
        print(f"\næ‰¾åˆ° {len(db_list)} å€‹ä¸é‡è¤‡çš„ DB (é¡å‹: {self.selected_db_type})")
        
        print("\nDB åˆ—è¡¨:")
        for i, db in enumerate(db_list, 1):
            db_info = unique_dbs[db]
            print(f"{i:3d}. {db:10s} - {db_info.module:10s} ({db_info.db_type})")
        
        print("\né¸æ“‡æ–¹å¼:")
        print("1. å…¨é¸")
        print("2. è¼¸å…¥ç·¨è™Ÿç¯„åœ (å¦‚: 1-5,7,9-12)")
        print("3. è¼¸å…¥ DB åç¨±åˆ—è¡¨ (é€—è™Ÿåˆ†éš”)")
        
        choice = input("è«‹é¸æ“‡: ").strip()
        
        selected = []
        
        if choice == '1':
            selected = db_list
        elif choice == '2':
            indices_input = input("è«‹è¼¸å…¥ç·¨è™Ÿç¯„åœ: ").strip()
            try:
                indices = self._parse_number_range(indices_input, len(db_list))
                selected = [db_list[i-1] for i in indices if 1 <= i <= len(db_list)]
            except Exception as e:
                print(f"âŒ ç„¡æ•ˆçš„ç·¨è™Ÿç¯„åœ: {e}")
                return []
        elif choice == '3':
            db_names = input("è«‹è¼¸å…¥ DB åç¨± (å¦‚: DB2302,DB2575): ").strip()
            input_dbs = [db.strip() for db in db_names.split(',')]
            for db in input_dbs:
                if db in db_list:
                    selected.append(db)
                else:
                    print(f"âš ï¸ DB {db} ä¸å­˜åœ¨ï¼Œå·²è·³é")
        
        self.selected_dbs = selected
        print(f"âœ… å·²é¸æ“‡ {len(selected)} å€‹ DB")
        return selected
    
    def _parse_number_range(self, range_str: str, max_num: int) -> List[int]:
        """è§£ææ•¸å­—ç¯„åœå­—ä¸²"""
        result = []
        parts = range_str.split(',')
        
        for part in parts:
            part = part.strip()
            if '-' in part:
                start, end = part.split('-', 1)
                start = int(start.strip())
                end = int(end.strip())
                
                if start > end:
                    start, end = end, start
                
                for i in range(start, min(end + 1, max_num + 1)):
                    if i > 0:
                        result.append(i)
            else:
                num = int(part)
                if 1 <= num <= max_num:
                    result.append(num)
        
        return sorted(list(set(result)))
    
    def setup_db_versions(self):
        """è¨­å®š DB ç‰ˆæœ¬"""
        if not self.selected_dbs:
            print("âŒ è«‹å…ˆé¸æ“‡ DB")
            return
        
        default_versions = config_manager.default_execution_config.get('db_versions', {})
        
        if default_versions:
            applicable_versions = {
                db: ver for db, ver in default_versions.items() 
                if db in self.selected_dbs
            }
            
            if applicable_versions:
                print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ç‰ˆæœ¬è¨­å®š:")
                for db, ver in applicable_versions.items():
                    print(f"   {db}: ç‰ˆæœ¬ {ver}")
                
                if input("æ˜¯å¦ä½¿ç”¨é è¨­ç‰ˆæœ¬è¨­å®š? (Y/n): ").strip().lower() != 'n':
                    self.db_versions.update(applicable_versions)
                    print(f"âœ… å·²å¥—ç”¨ {len(applicable_versions)} å€‹ DB çš„é è¨­ç‰ˆæœ¬")
                    
                    unset_dbs = [db for db in self.selected_dbs if db not in self.db_versions]
                    if unset_dbs:
                        print(f"\né‚„æœ‰ {len(unset_dbs)} å€‹ DB æœªè¨­å®šç‰ˆæœ¬:")
                        for db in unset_dbs:
                            version = input(f"{db} çš„ç‰ˆæœ¬ [æœ€æ–°]: ").strip()
                            if version:
                                self.db_versions[db] = version
                    return
        
        print("\nè¨­å®š DB ç‰ˆæœ¬ (ç•™ç©ºä½¿ç”¨æœ€æ–°ç‰ˆæœ¬)")
        for db in self.selected_dbs:
            if db in self.db_versions:
                print(f"{db}: å·²è¨­å®šç‰ˆæœ¬ {self.db_versions[db]}")
                continue
            
            version = input(f"{db} çš„ç‰ˆæœ¬ [æœ€æ–°]: ").strip()
            if version:
                self.db_versions[db] = version
        
        if self.db_versions:
            print(f"âœ… å·²è¨­å®š {len(self.db_versions)} å€‹ DB çš„ç‰ˆæœ¬")
    
    def execute_pinning(self):
        """åŸ·è¡Œå®šç‰ˆ"""
        if not self.selected_dbs:
            print("âŒ è«‹å…ˆé¸æ“‡è¦å®šç‰ˆçš„ DB")
            return
        
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return
        
        print("\n" + "="*60)
        print("æº–å‚™åŸ·è¡Œå®šç‰ˆ")
        print("="*60)
        print(f"ğŸ“Œ DB æ•¸é‡: {len(self.selected_dbs)}")
        print(f"ğŸ” è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # è©¢å•è¼¸å‡ºç›®éŒ„
        default_output = config_manager.default_execution_config.get('output_dir')
        if default_output:
            print(f"ğŸ“Œ æ‰¾åˆ°é è¨­è¼¸å‡ºç›®éŒ„: {default_output}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­è¼¸å‡ºç›®éŒ„? (Y/n): ").strip().lower() != 'n':
                self.tool.output_dir = default_output
            else:
                output_dir = input(f"è¼¸å‡ºç›®éŒ„ [{self.tool.output_dir}]: ").strip()
                if output_dir:
                    self.tool.output_dir = output_dir
        else:
            output_dir = input(f"è¼¸å‡ºç›®éŒ„ [{self.tool.output_dir}]: ").strip()
            if output_dir:
                self.tool.output_dir = output_dir
        
        # ç¢ºèªåŸ·è¡Œ
        if config_manager.default_execution_config.get('auto_confirm'):
            print("ğŸ“Œ è‡ªå‹•ç¢ºèªåŸ·è¡Œï¼ˆæ ¹æ“šé è¨­é…ç½®ï¼‰")
        else:
            if input("\nç¢ºèªé–‹å§‹åŸ·è¡Œ? (Y/n): ").strip().lower() == 'n':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                return
        
        print("\nğŸš€ é–‹å§‹åŸ·è¡Œå®šç‰ˆ...")
        
        print("ğŸŒ æº–å‚™ SFTP é€£ç·šï¼ˆæ¯å€‹ DB ä½¿ç”¨ç¨ç«‹é€£ç·šï¼‰...")
        
        try:
            # åŸ·è¡Œå®šç‰ˆ
            self.tool.process_selected_dbs(self.selected_dbs, self.db_versions)
            
            # ç”¢ç”Ÿå ±å‘Š
            print("\nğŸ“Š ç”¢ç”Ÿå ±å‘Š...")
            report_path = os.path.join(
                self.tool.output_dir, 
                config_manager.path_config['report_filename']
            )
            self.tool.generate_report(report_path)
            
            print("\nâœ¨ å®šç‰ˆå®Œæˆï¼")
            
        finally:
            resource_manager.cleanup_all()
    
    def quick_execute_with_defaults(self):
        """ä½¿ç”¨é è¨­é…ç½®å¿«é€ŸåŸ·è¡Œ"""
        print("\n" + "="*60)
        print("å¿«é€ŸåŸ·è¡Œæ¨¡å¼ - ä½¿ç”¨é è¨­é…ç½®")
        print("="*60)
        
        # è‡ªå‹•è¼‰å…¥ mapping table
        if config_manager.default_execution_config.get('mapping_table'):
            file_path = config_manager.default_execution_config['mapping_table']
            if os.path.exists(file_path):
                print(f"ğŸ“Œ è¼‰å…¥é è¨­ mapping table: {file_path}")
                if self.tool.load_mapping_table(file_path):
                    print(f"âœ… æˆåŠŸè¼‰å…¥")
                else:
                    print("âŒ è¼‰å…¥å¤±æ•—")
                    return
            else:
                print(f"âŒ é è¨­ mapping table ä¸å­˜åœ¨: {file_path}")
                return
        else:
            print("âŒ æœªè¨­å®šé è¨­ mapping table")
            return
        
        # è¨­å®š DB é¡å‹
        if config_manager.default_execution_config.get('db_type'):
            self.selected_db_type = config_manager.default_execution_config['db_type']
            print(f"ğŸ“Œ ä½¿ç”¨é è¨­ DB é¡å‹: {self.selected_db_type}")
        
        # é¸æ“‡ DB
        default_dbs = config_manager.default_execution_config.get('selected_dbs')
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        
        if default_dbs:
            if default_dbs in ['all', '*']:
                unique_dbs = list(set([db.db_info for db in all_db_infos]))
                self.selected_dbs = unique_dbs
                print(f"ğŸ“Œ é¸æ“‡æ‰€æœ‰ {self.selected_db_type} é¡å‹çš„ DB: {len(unique_dbs)} å€‹")
            
            elif isinstance(default_dbs, list) and len(default_dbs) > 0:
                parsed_dbs = []
                for db_spec in default_dbs:
                    if '#' in db_spec:
                        db_name, version = db_spec.split('#', 1)
                        parsed_dbs.append(db_name)
                        self.db_versions[db_name] = version
                    else:
                        parsed_dbs.append(db_spec)
                self.selected_dbs = parsed_dbs
                print(f"ğŸ“Œ ä½¿ç”¨é è¨­ DB åˆ—è¡¨: {len(parsed_dbs)} å€‹")
        else:
            print("âš ï¸ é è¨­é…ç½®æœªæŒ‡å®š DB åˆ—è¡¨ï¼Œç„¡æ³•è‡ªå‹•åŸ·è¡Œ")
            return
        
        # è¨­å®šç‰ˆæœ¬
        if config_manager.default_execution_config.get('db_versions'):
            self.db_versions.update(config_manager.default_execution_config['db_versions'])
            print(f"ğŸ“Œ å¥—ç”¨é è¨­ç‰ˆæœ¬è¨­å®š: {len(self.db_versions)} å€‹")
        
        # è¨­å®šè¼¸å‡ºç›®éŒ„
        if config_manager.default_execution_config.get('output_dir'):
            self.tool.output_dir = config_manager.default_execution_config['output_dir']
            print(f"ğŸ“Œ ä½¿ç”¨é è¨­è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # é¡¯ç¤ºæ‘˜è¦
        print("\nåŸ·è¡Œæ‘˜è¦:")
        print(f"  DB é¡å‹: {self.selected_db_type}")
        print(f"  DB æ•¸é‡: {len(self.selected_dbs)}")
        print(f"  è¨­å®šç‰ˆæœ¬: {len(self.db_versions)} å€‹")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # ç¢ºèªåŸ·è¡Œ
        if not config_manager.default_execution_config.get('auto_confirm'):
            if input("\nç¢ºèªåŸ·è¡Œ? (Y/n): ").strip().lower() == 'n':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆ")
                return
        
        # åŸ·è¡Œå®šç‰ˆ
        self.execute_pinning()
    
    def test_jira_connection(self):
        """æ¸¬è©¦ JIRA é€£ç·š"""
        print("\næ¸¬è©¦ JIRA é€£ç·š...")
        
        if not hasattr(self.tool, 'source_cmd_manager'):
            self.tool.source_cmd_manager = SourceCommandManager()
        
        if self.tool.source_cmd_manager.jira_client.connect():
            print("âœ… JIRA é€£ç·šæˆåŠŸï¼")
            
            if input("\næ˜¯å¦è¦æ¸¬è©¦æŸ¥è©¢ DB çš„ source command? (y/N): ").strip().lower() == 'y':
                db_name = input("è«‹è¼¸å…¥ DB åç¨± (ä¾‹å¦‚: DB2302): ").strip()
                if db_name:
                    print(f"\næŸ¥è©¢ {db_name} çš„ source command...")
                    
                    test_db_info = DBInfo(
                        sn=0,
                        module="Test",
                        db_type="master",
                        db_info=db_name,
                        db_folder="",
                        sftp_path=""
                    )
                    
                    cmd = self.tool.source_cmd_manager.get_source_command(
                        test_db_info,
                        self.tool.mapping_reader.df if self.tool.mapping_reader else None
                    )
                    
                    if cmd:
                        print(f"\næ‰¾åˆ°çš„ source command:")
                        print(f"  {cmd}")
                    else:
                        print(f"\næœªæ‰¾åˆ° {db_name} çš„ source command")
        else:
            print("âŒ JIRA é€£ç·šå¤±æ•—ï¼")
    
    def test_sftp_connection(self):
        """æ¸¬è©¦ SFTP é€£ç·š"""
        print("\næ¸¬è©¦ SFTP é€£ç·š...")
        print(f"ä¼ºæœå™¨: {config_manager.sftp_config['host']}:{config_manager.sftp_config['port']}")
        print(f"ç”¨æˆ¶: {config_manager.sftp_config['username']}")
        
        # å‰µå»ºè‡¨æ™‚ SFTP ç®¡ç†å™¨é€²è¡Œæ¸¬è©¦
        test_sftp_manager = SFTPManager()
        
        if test_sftp_manager.connect():
            print("âœ… SFTP é€£ç·šæˆåŠŸï¼")
            
            if input("\næ˜¯å¦è¦æ¸¬è©¦è·¯å¾‘å­˜å–? (y/N): ").strip().lower() == 'y':
                path = input("è«‹è¼¸å…¥è¦æ¸¬è©¦çš„è·¯å¾‘: ").strip()
                if path:
                    try:
                        result = test_sftp_manager.find_latest_manifest(path)
                        if result:
                            print(f"âœ… æ‰¾åˆ° manifest: {result[1]}")
                        else:
                            print("âŒ æ²’æœ‰æ‰¾åˆ° manifest")
                    except Exception as e:
                        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
            
            test_sftp_manager.disconnect()
        else:
            print("âŒ SFTP é€£ç·šå¤±æ•—ï¼")
    
    def display_menu(self) -> str:
        """é¡¯ç¤ºä¸»é¸å–®"""
        print("\n" + "="*60)
        print("Manifest å®šç‰ˆå·¥å…· - ä¸»é¸å–® (æ”¹é€²ç‰ˆ)")
        
        has_defaults = any([
            config_manager.default_execution_config.get('mapping_table'),
            config_manager.default_execution_config.get('db_type'),
            config_manager.default_execution_config.get('selected_dbs'),
            config_manager.default_execution_config.get('db_versions'),
            config_manager.default_execution_config.get('output_dir')
        ])
        
        if has_defaults:
            print("(ğŸ“Œ å·²è¼‰å…¥é è¨­é…ç½®)")
        
        print("="*60)
        print("1. è¼‰å…¥ mapping table")
        print("2. è¨­å®š SFTP é€£ç·šè³‡è¨Š")
        print("3. é¸æ“‡ DB é¡å‹ (master/premp/mp/mpbackup/all)")
        print("4. é¸æ“‡è¦å®šç‰ˆçš„ DB")
        print("5. è¨­å®š DB ç‰ˆæœ¬")
        print("6. é–‹å§‹åŸ·è¡Œå®šç‰ˆ")
        print("7. é¡¯ç¤ºç›®å‰è¨­å®š")
        print("8. å¿«é€ŸåŸ·è¡Œï¼ˆä½¿ç”¨æ‰€æœ‰é è¨­å€¼ï¼‰")
        print("9. æ¸¬è©¦ JIRA é€£ç·š")
        print("10. æ¸¬è©¦ SFTP é€£ç·š")
        print("0. çµæŸç¨‹å¼")
        print("="*60)
        
        return input("è«‹é¸æ“‡åŠŸèƒ½: ").strip()
    
    def run_interactive(self):
        """åŸ·è¡Œäº’å‹•å¼ä»‹é¢"""
        print("\næ­¡è¿ä½¿ç”¨ Manifest å®šç‰ˆå·¥å…·ï¼")
        print(f"ç‰ˆæœ¬: {__version__} (æ”¹é€²ç‰ˆ)")
        print("æ”¹é€²å…§å®¹: ä¿®å¾© SFTP Garbage packet å•é¡Œã€æ”¹å–„æ—¥èªŒè¼¸å‡º")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é è¨­é…ç½®
        has_complete_defaults = all([
            config_manager.default_execution_config.get('mapping_table'),
            os.path.exists(config_manager.default_execution_config.get('mapping_table', ''))
        ])
        
        if has_complete_defaults:
            print("\nğŸ“Œ åµæ¸¬åˆ°å®Œæ•´çš„é è¨­é…ç½®")
            if input("æ˜¯å¦è¦ä½¿ç”¨é è¨­é…ç½®å¿«é€ŸåŸ·è¡Œ? (y/N): ").strip().lower() == 'y':
                self.quick_execute_with_defaults()
                return
        
        while True:
            try:
                choice = self.display_menu()
                
                if choice == '0':
                    print("\nğŸ‘‹ å†è¦‹ï¼")
                    break
                elif choice == '1':
                    self.load_mapping_table()
                elif choice == '2':
                    self.setup_sftp()
                elif choice == '3':
                    self.select_db_type()
                elif choice == '4':
                    self.select_dbs()
                elif choice == '5':
                    self.setup_db_versions()
                elif choice == '6':
                    self.execute_pinning()
                elif choice == '7':
                    self.display_current_settings()
                elif choice == '8':
                    self.quick_execute_with_defaults()
                elif choice == '9':
                    self.test_jira_connection()
                elif choice == '10':
                    self.test_sftp_connection()
                else:
                    print("âŒ ç„¡æ•ˆçš„é¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·")
                if input("ç¢ºå®šè¦çµæŸç¨‹å¼å—? (Y/n): ").strip().lower() != 'n':
                    break
            except Exception as e:
                self.logger.error(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                if input("æ˜¯å¦ç¹¼çºŒ? (Y/n): ").strip().lower() == 'n':
                    break
        
        resource_manager.cleanup_all()

# =====================================
# ===== ä¸»ç¨‹å¼ =====
# =====================================

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(
        description='Manifest å®šç‰ˆå·¥å…· - è‡ªå‹•åŒ– repo å®šç‰ˆè™•ç† (æ”¹é€²ç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
ç‰ˆæœ¬: {__version__}
ä½œè€…: {__author__}
æ—¥æœŸ: {__date__}

æ”¹é€²å…§å®¹:
- ä¿®å¾© SFTP "Garbage packet received" éŒ¯èª¤
- æ”¹ç‚ºæ­£å¸¸é †åºæ—¥èªŒè¼¸å‡ºï¼Œä¾¿æ–¼ debug
- æ”¹é€² Excel å ±å‘Šæ ¼å¼ï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰
- è¨˜éŒ„å®Œæ•´ repo init æŒ‡ä»¤å’Œ JIRA é€£çµ

ç¯„ä¾‹:
  # ä½¿ç”¨äº’å‹•å¼ä»‹é¢
  python {sys.argv[0]}
  
  # è™•ç†æŒ‡å®šçš„ DB
  python {sys.argv[0]} -m mapping.xlsx -d DB2302,DB2575 -o ./output
  
  # æ¸¬è©¦æ¨¡å¼
  python {sys.argv[0]} -m mapping.xlsx --dry-run
        """
    )
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('-m', '--mapping', 
                        type=str,
                        help='Mapping table Excel æª”æ¡ˆè·¯å¾‘')
    
    parser.add_argument('-o', '--output', 
                        type=str,
                        help=f'è¼¸å‡ºç›®éŒ„ (é è¨­: {config_manager.path_config["default_output_dir"]})')
    
    parser.add_argument('-t', '--type',
                        choices=['all', 'master', 'premp', 'mp', 'mpbackup'],
                        default='all',
                        help='è¦è™•ç†çš„ DB é¡å‹ (é è¨­: all)')
    
    parser.add_argument('-d', '--dbs', 
                        type=str,
                        help='è¦è™•ç†çš„ DB åˆ—è¡¨ï¼Œé€—è™Ÿåˆ†éš” (å¯åŒ…å«ç‰ˆæœ¬ï¼Œå¦‚: DB2302#3,DB2575)')
    
    parser.add_argument('-v', '--versions', 
                        type=str,
                        help='DB ç‰ˆæœ¬è¨­å®šï¼Œæ ¼å¼: DB2302#3,DB2575#186')
    
    parser.add_argument('--dry-run', 
                        action='store_true',
                        help='æ¸¬è©¦æ¨¡å¼ï¼Œåªé¡¯ç¤ºå°‡è¦åŸ·è¡Œçš„å‹•ä½œï¼Œä¸å¯¦éš›åŸ·è¡Œ')
    
    parser.add_argument('--debug', 
                        action='store_true',
                        help='å•Ÿç”¨ debug æ¨¡å¼ï¼Œé¡¯ç¤ºè©³ç´°æ—¥èªŒ')
    
    parser.add_argument('--version', 
                        action='version',
                        version=f'%(prog)s {__version__}')
    
    # è§£æåƒæ•¸
    args = parser.parse_args()
    
    # è¨­å®šæ—¥èªŒç­‰ç´š
    if args.debug:
        config_manager.log_config['level'] = logging.DEBUG
        logging.getLogger().setLevel(logging.DEBUG)
        for handler in logging.getLogger().handlers:
            handler.setLevel(logging.DEBUG)
        print("ğŸ” Debug æ¨¡å¼å·²å•Ÿç”¨")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¸¬è©¦æ¨¡å¼
    if args.dry_run:
        print("\n" + "="*60)
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ (Dry Run) - ä¸æœƒå¯¦éš›åŸ·è¡Œä»»ä½•æ“ä½œ")
        print("="*60)
    
    # æ±ºå®šåŸ·è¡Œæ¨¡å¼
    if args.mapping:
        # å‘½ä»¤åˆ—æ¨¡å¼
        print("\n" + "="*60)
        print(f"ğŸ“‹ Manifest å®šç‰ˆå·¥å…· v{__version__} - å‘½ä»¤åˆ—æ¨¡å¼ (æ”¹é€²ç‰ˆ)")
        print("="*60)
        
        tool = ManifestPinningTool()
        
        if args.dry_run:
            tool.dry_run = True
        
        try:
            # è¼‰å…¥ mapping table
            print(f"\nğŸ“‚ è¼‰å…¥ mapping table: {args.mapping}")
            if not os.path.exists(args.mapping):
                print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.mapping}")
                sys.exit(1)
            
            if not tool.load_mapping_table(args.mapping):
                print("âŒ ç„¡æ³•è¼‰å…¥ mapping table")
                sys.exit(1)
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ mapping table")
            
            # è¨­å®šè¼¸å‡ºç›®éŒ„
            tool.output_dir = args.output or config_manager.path_config['default_output_dir']
            os.makedirs(tool.output_dir, exist_ok=True)
            print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {tool.output_dir}")

            print(f"\nğŸŒ æº–å‚™ SFTP é€£ç·š: {config_manager.sftp_config['host']}")
            print("â„¹ï¸  æ¯å€‹ DB å°‡ä½¿ç”¨ç¨ç«‹çš„ SFTP é€£ç·š")

            try:
                # æ±ºå®šè¦è™•ç†çš„ DB åˆ—è¡¨
                db_list = []
                db_versions = {}
                
                if args.dbs:
                    db_specs = [db.strip() for db in args.dbs.split(',')]
                    
                    for db_spec in db_specs:
                        if '#' in db_spec:
                            db_name, version = db_spec.split('#', 1)
                            db_list.append(db_name)
                            db_versions[db_name] = version
                        else:
                            db_list.append(db_spec)
                    
                    print(f"\nğŸ“Œ ä½¿ç”¨æŒ‡å®šçš„ DB åˆ—è¡¨: {', '.join(db_list)}")
                else:
                    all_db_infos = tool.get_all_dbs(args.type)
                    db_list = list(set([db.db_info for db in all_db_infos]))
                    
                    if args.type == 'all':
                        print(f"\nğŸ“Œ ä½¿ç”¨æ‰€æœ‰ DBï¼Œå…± {len(db_list)} å€‹")
                    else:
                        print(f"\nğŸ“Œ ä½¿ç”¨æ‰€æœ‰ {args.type} é¡å‹çš„ DBï¼Œå…± {len(db_list)} å€‹")
                
                # è™•ç†é¡å¤–çš„ç‰ˆæœ¬è¨­å®š
                if args.versions:
                    version_specs = [v.strip() for v in args.versions.split(',')]
                    for version_spec in version_specs:
                        if '#' in version_spec:
                            db_name, version = version_spec.split('#', 1)
                            db_versions[db_name] = version
                    
                    print(f"ğŸ“Œ è¨­å®šäº† {len(db_versions)} å€‹ DB çš„ç‰ˆæœ¬")
                
                # ç¢ºèªè™•ç†è³‡è¨Š
                print("\n" + "-"*40)
                print("ğŸ“‹ æº–å‚™è™•ç†ä»¥ä¸‹ DB:")
                for i, db in enumerate(db_list, 1):
                    version_info = f" (ç‰ˆæœ¬: {db_versions[db]})" if db in db_versions else " (æœ€æ–°ç‰ˆæœ¬)"
                    print(f"  {i:3d}. {db}{version_info}")
                print("-"*40)
                
                if not db_list:
                    print("âŒ æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
                    sys.exit(1)
                
                # è©¢å•ç¢ºèªï¼ˆé™¤éæ˜¯æ¸¬è©¦æ¨¡å¼ï¼‰
                if not args.dry_run:
                    if sys.stdin.isatty():
                        confirm = input(f"\nç¢ºèªè¦è™•ç† {len(db_list)} å€‹ DB? (Y/n): ").strip().lower()
                        if confirm == 'n':
                            print("âŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                            sys.exit(0)
                
                # é–‹å§‹è™•ç†
                print("\n" + "="*60)
                if args.dry_run:
                    print("ğŸ§ª é–‹å§‹æ¸¬è©¦åŸ·è¡Œï¼ˆä¸æœƒå¯¦éš›åŸ·è¡Œæ“ä½œï¼‰")
                else:
                    print("ğŸš€ é–‹å§‹åŸ·è¡Œå®šç‰ˆè™•ç†")
                print("="*60)
                
                start_time = datetime.now()
                
                # åŸ·è¡Œè™•ç†
                tool.process_selected_dbs(db_list, db_versions)
                
                end_time = datetime.now()
                elapsed_time = end_time - start_time
                
                # ç”¢ç”Ÿå ±å‘Š
                if not args.dry_run:
                    print("\nğŸ“Š ç”¢ç”Ÿè™•ç†å ±å‘Š...")
                    report_path = os.path.join(
                        tool.output_dir, 
                        config_manager.path_config['report_filename']
                    )
                    tool.generate_report(report_path)
                
                # é¡¯ç¤ºçµæœæ‘˜è¦
                print("\n" + "="*60)
                print("âœ¨ è™•ç†å®Œæˆï¼")
                print("="*60)
                print(f"ğŸ“Š ç¸½ DB æ•¸: {tool.report.total_dbs}")
                print(f"âœ… æˆåŠŸ: {tool.report.successful_dbs}")
                print(f"âŒ å¤±æ•—: {tool.report.failed_dbs}")
                print(f"â­ï¸ è·³é: {tool.report.skipped_dbs}")
                print(f"â±ï¸ ç¸½è€—æ™‚: {elapsed_time}")
                print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {tool.output_dir}")
                if not args.dry_run:
                    print(f"ğŸ“Š å ±å‘Šæª”æ¡ˆ: {report_path}")
                print("="*60)
                
                # å¦‚æœæœ‰å¤±æ•—çš„é …ç›®ï¼Œé¡¯ç¤ºè©³ç´°è³‡è¨Š
                if tool.report.failed_dbs > 0:
                    print("\nâŒ å¤±æ•—çš„ DB:")
                    for db in tool.report.db_details:
                        if db.status == DBStatus.FAILED:
                            print(f"  - {db.module}/{db.db_info}: {db.error_message}")
                
            finally:
                print("\nğŸ“Œ æ¸…ç†è³‡æº...")
                resource_manager.cleanup_all()
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ æ”¶åˆ° Ctrl+Cï¼Œæ¸…ç†æ‰€æœ‰é€²ç¨‹...")
            resource_manager.cleanup_all()
            sys.exit(0)
            
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            if args.debug:
                import traceback
                traceback.print_exc()
            sys.exit(1)
        
        finally:
            resource_manager.cleanup_all()
    
    else:
        # äº’å‹•å¼æ¨¡å¼
        print("\n" + "="*60)
        print(f"ğŸ® Manifest å®šç‰ˆå·¥å…· v{__version__} - äº’å‹•å¼ä»‹é¢ (æ”¹é€²ç‰ˆ)")
        print("="*60)
        print("æ”¹é€²å…§å®¹: ä¿®å¾© SFTP Garbage packet å•é¡Œã€æ”¹å–„æ—¥èªŒè¼¸å‡º")
        print("æç¤º: ä½¿ç”¨ -h åƒæ•¸æŸ¥çœ‹å‘½ä»¤åˆ—é¸é …")
        print("="*60)
        
        try:
            ui = InteractiveUI()
            
            if args.output:
                ui.tool.output_dir = args.output
            
            if args.dry_run:
                ui.tool.dry_run = True
                print("ğŸ§ª æ¸¬è©¦æ¨¡å¼å·²å•Ÿç”¨")
            
            ui.run_interactive()
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è¦‹ï¼")
            sys.exit(0)
            
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            if args.debug:
                import traceback
                traceback.print_exc()
            sys.exit(1)
        
        finally:
            resource_manager.cleanup_all()

# åœ¨ä½ çš„ç¨‹å¼ä¸­æ·»åŠ é€™å€‹æ¸¬è©¦å‡½æ•¸
def test_sftp_connection():
    """æ¸¬è©¦ SFTP é€£ç·šå’Œä¼ºæœå™¨èƒ½åŠ›"""
    print("="*50)
    print("SFTP é€£ç·šå’Œèƒ½åŠ›æ¸¬è©¦")
    print("="*50)
    
    sftp_mgr = SFTPManager()
    
    try:
        # æ¸¬è©¦é€£ç·š
        print("1. æ¸¬è©¦ SFTP é€£ç·š...")
        if sftp_mgr.connect():
            print("âœ… SFTP é€£ç·šæˆåŠŸ")
            
            # é¡¯ç¤ºä¼ºæœå™¨èƒ½åŠ›
            print("\n2. ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬çµæœ:")
            capabilities = sftp_mgr._server_capabilities
            print(f"   listdir_attr æ”¯æ´: {'âœ…' if capabilities['supports_listdir_attr'] else 'âŒ'}")
            print(f"   stat æ”¯æ´: {'âœ…' if capabilities['supports_stat'] else 'âŒ'}")
            
            # æ¸¬è©¦åŸºæœ¬æ“ä½œ
            print("\n3. æ¸¬è©¦åŸºæœ¬æ“ä½œ...")
            try:
                items = sftp_mgr._safe_listdir('.')
                print(f"âœ… åŸºæœ¬ listdir æˆåŠŸï¼Œæ‰¾åˆ° {len(items)} å€‹é …ç›®")
            except Exception as e:
                print(f"âŒ åŸºæœ¬ listdir å¤±æ•—: {e}")
            
            # æ¸¬è©¦è©³ç´°åˆ—è¡¨
            print("\n4. æ¸¬è©¦è©³ç´°åˆ—è¡¨...")
            try:
                items = sftp_mgr._safe_listdir_with_details('.')
                print(f"âœ… è©³ç´°åˆ—è¡¨æˆåŠŸï¼Œæ‰¾åˆ° {len(items)} å€‹é …ç›®")
                for item in items[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    print(f"   - {item['name']} ({'ç›®éŒ„' if item['is_dir'] else 'æª”æ¡ˆ'})")
            except Exception as e:
                print(f"âŒ è©³ç´°åˆ—è¡¨å¤±æ•—: {e}")
            
            sftp_mgr.disconnect()
            print("\nâœ… æ¸¬è©¦å®Œæˆ")
            
        else:
            print("âŒ SFTP é€£ç·šå¤±æ•—")
    
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

def test_manifest_search():
    """æ¸¬è©¦å¯¦éš›çš„ manifest æœå°‹åŠŸèƒ½"""
    print("="*60)
    print("é€²éš Manifest æœå°‹æ¸¬è©¦")
    print("="*60)
    
    sftp_mgr = SFTPManager()
    
    try:
        # é€£ç·š
        print("1. å»ºç«‹ SFTP é€£ç·š...")
        if not sftp_mgr.connect():
            print("âŒ SFTP é€£ç·šå¤±æ•—")
            return
        print("âœ… SFTP é€£ç·šæˆåŠŸ")
        
        # æ¸¬è©¦è·¯å¾‘ï¼ˆä½¿ç”¨ DB2145 çš„å¯¦éš›è·¯å¾‘ï¼‰
        test_paths = [
            "/DailyBuild/Merlin8/DB2145_Merlin8_FW_Android14_Ref_Plus_GoogleGMS",
            "/DailyBuild/Merlin8",  # ä¸Šå±¤ç›®éŒ„æ¸¬è©¦
        ]
        
        for test_path in test_paths:
            print(f"\n2. æ¸¬è©¦è·¯å¾‘: {test_path}")
            
            # æ¸¬è©¦åŸºæœ¬åˆ—ç›®éŒ„
            try:
                print(f"   2.1 æ¸¬è©¦åŸºæœ¬ listdir...")
                items = sftp_mgr._safe_listdir(test_path)
                print(f"   âœ… æ‰¾åˆ° {len(items)} å€‹é …ç›®")
                
                # é¡¯ç¤ºå‰å¹¾å€‹é …ç›®
                for i, item in enumerate(items[:5]):
                    print(f"      - {item}")
                if len(items) > 5:
                    print(f"      ... é‚„æœ‰ {len(items) - 5} å€‹é …ç›®")
                    
            except Exception as e:
                print(f"   âŒ åŸºæœ¬ listdir å¤±æ•—: {e}")
                continue
            
            # æ¸¬è©¦è©³ç´°åˆ—ç›®éŒ„
            try:
                print(f"   2.2 æ¸¬è©¦è©³ç´° listdir...")
                items = sftp_mgr._safe_listdir_with_details(test_path)
                print(f"   âœ… æ‰¾åˆ° {len(items)} å€‹è©³ç´°é …ç›®")
                
                # åˆ†æç›®éŒ„çµæ§‹
                dirs = [item for item in items if item['is_dir']]
                files = [item for item in items if not item['is_dir']]
                
                print(f"      ç›®éŒ„: {len(dirs)} å€‹")
                print(f"      æª”æ¡ˆ: {len(files)} å€‹")
                
                # é¡¯ç¤ºç‰ˆæœ¬ç›®éŒ„
                version_dirs = []
                for item in dirs:
                    version_num = sftp_mgr._extract_version_number(item['name'])
                    if version_num:
                        version_dirs.append((int(version_num), item['name']))
                
                if version_dirs:
                    version_dirs.sort(reverse=True)
                    print(f"      ç‰ˆæœ¬ç›®éŒ„: {len(version_dirs)} å€‹")
                    for ver_num, ver_name in version_dirs[:3]:
                        print(f"         ç‰ˆæœ¬ {ver_num}: {ver_name}")
                    
            except Exception as e:
                print(f"   âŒ è©³ç´° listdir å¤±æ•—: {e}")
                continue
            
            # æ¸¬è©¦ manifest æœå°‹
            try:
                print(f"   2.3 æ¸¬è©¦ manifest æœå°‹...")
                result = sftp_mgr.find_latest_manifest(test_path)
                
                if result:
                    manifest_path, manifest_name = result
                    print(f"   âœ… æ‰¾åˆ° manifest: {manifest_name}")
                    print(f"      å®Œæ•´è·¯å¾‘: {manifest_path}")
                    
                    # æ¸¬è©¦æª”æ¡ˆå­˜åœ¨æ€§
                    print(f"   2.4 é©—è­‰ manifest æª”æ¡ˆ...")
                    if sftp_mgr._file_exists_and_valid(manifest_path):
                        print(f"   âœ… Manifest æª”æ¡ˆæœ‰æ•ˆ")
                    else:
                        print(f"   âŒ Manifest æª”æ¡ˆç„¡æ•ˆ")
                else:
                    print(f"   âŒ æœªæ‰¾åˆ° manifest")
                    
            except Exception as e:
                print(f"   âŒ Manifest æœå°‹å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
        
        # æ¸¬è©¦ç‰¹å®šç‰ˆæœ¬æœå°‹ï¼ˆå¦‚æœç”¨æˆ¶æœ‰ç‰¹å®šç‰ˆæœ¬éœ€æ±‚ï¼‰
        print(f"\n3. æ¸¬è©¦ç‰¹å®šç‰ˆæœ¬æœå°‹...")
        test_version = "709"  # DB2145 çš„ç‰ˆæœ¬
        test_path = "/DailyBuild/Merlin8/DB2145_Merlin8_FW_Android14_Ref_Plus_GoogleGMS"
        
        try:
            result = sftp_mgr.find_latest_manifest(test_path, target_version=test_version)
            if result:
                manifest_path, manifest_name = result
                print(f"âœ… æ‰¾åˆ°ç‰ˆæœ¬ {test_version} çš„ manifest: {manifest_name}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°ç‰ˆæœ¬ {test_version} çš„ manifest")
        except Exception as e:
            print(f"âŒ ç‰¹å®šç‰ˆæœ¬æœå°‹å¤±æ•—: {e}")
        
        sftp_mgr.disconnect()
        print("\nâœ… é€²éšæ¸¬è©¦å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        
        if sftp_mgr.connected:
            sftp_mgr.disconnect()

if __name__ == "__main__":
    # test_sftp_connection()
    # test_manifest_search()
    main()