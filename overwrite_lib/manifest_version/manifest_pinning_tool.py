#!/usr/bin/env python3
"""
Manifest Pinning Tool - è‡ªå‹•åŒ–å®šç‰ˆå·¥å…· (æ”¹é€²ç‰ˆ)
ç”¨æ–¼å¾ SFTP ä¸‹è¼‰ manifest æª”æ¡ˆä¸¦åŸ·è¡Œ repo å®šç‰ˆæ“ä½œ
æ”¹é€²ç‰ˆæœ¬ï¼šç°¡åŒ– SFTPã€æ”¹é€²å ±å‘Šæ ¼å¼ã€æ­£å¸¸æ—¥èªŒè¼¸å‡º
"""

from enum import Enum
import os
import sys
import argparse
import pandas as pd
import paramiko
import subprocess
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import logging
from dataclasses import dataclass, field, asdict
import signal
import atexit
from contextlib import contextmanager
from collections import defaultdict
import random
import socket
from threading import Semaphore, Lock
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

# =====================================
# ===== ç‰ˆæœ¬è³‡è¨Š =====
# =====================================
__version__ = '2.1.1'
__author__ = 'Vince Lin'
__date__ = '2024-12-19'

# =====================================
# ===== é…ç½®ç®¡ç†å™¨ =====
# =====================================

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰é…ç½®çš„å„ªå…ˆç´šå’Œä¾†æº"""
    
    def __init__(self):
        # åŸºç¤é…ç½®
        self.sftp_config = {
            'host': 'mmsftpx.realtek.com',
            'port': 22,
            'username': 'lgwar_user',
            'password': 'Ab!123456',
            'timeout': 30,
            'retry_count': 3,
            'retry_delay': 2
        }
        
        self.jira_config = {
            'site': 'jira.realtek.com',
            'username': 'vince_lin',
            'password': 'Amon200!Amon200!',
            'api_url': 'https://jira.realtek.com/rest/api/2'
        }
        
        self.path_config = {
            'default_output_dir': './DB-source',
            'default_mapping_table': '../all_chip_mapping_table.xlsx',
            'manifest_pattern': r'manifest_(\d+)\.xml',
            'report_filename': 'pinning_report.xlsx'
        }
        
        self.repo_config = {
            'repo_command': 'repo',
            'sync_jobs': 8,
            'sync_retry': 2,
            'init_timeout': 60,
            'sync_timeout': 7200,
            'async_sync': True,
            'show_sync_output': False
        }
        
        self.parallel_config = {
            'max_workers': 4,
            'enable_parallel': True,
        }
        
        self.log_config = {
            'level': logging.INFO,
            'format': '%(asctime)s - [%(levelname)s] - %(name)s - %(message)s',
            'date_format': '%Y-%m-%d %H:%M:%S'
        }
        
        # é è¨­åŸ·è¡Œé…ç½®
        self.default_execution_config = {
            'mapping_table': os.path.join(os.path.dirname(__file__), '..', 'all_chip_mapping_table.xlsx'),
            'db_type': 'all',
            'selected_dbs': ['DB2145', 'DB2858', 'DB2575', 'DB2919'],
            'db_versions': {},
            'output_dir': './DB-source',
            'auto_confirm': False,
            'sftp_override': {}
        }
        
        # é…ç½®å„ªå…ˆç´šè¿½è¹¤
        self.config_sources = {}
        
    def apply_overrides(self, overrides: Dict[str, Any], source: str = 'user'):
        """æ‡‰ç”¨é…ç½®è¦†è“‹ä¸¦è¨˜éŒ„ä¾†æº"""
        for key, value in overrides.items():
            if value is not None:
                self.config_sources[key] = source
                # æ ¹æ“š key æ›´æ–°å°æ‡‰çš„é…ç½®
                if key.startswith('sftp_'):
                    sftp_key = key.replace('sftp_', '')
                    if sftp_key in self.sftp_config:
                        self.sftp_config[sftp_key] = value
                elif key.startswith('repo_'):
                    repo_key = key.replace('repo_', '')
                    if repo_key in self.repo_config:
                        self.repo_config[repo_key] = value
                elif key.startswith('parallel_'):
                    parallel_key = key.replace('parallel_', '')
                    if parallel_key in self.parallel_config:
                        self.parallel_config[parallel_key] = value
    
    def validate_config(self) -> Tuple[bool, List[str]]:
        """é©—è­‰é…ç½®çš„å®Œæ•´æ€§å’Œåˆç†æ€§"""
        errors = []
        
        # é©—è­‰ SFTP é…ç½®
        if not self.sftp_config.get('host'):
            errors.append("SFTP host ä¸èƒ½ç‚ºç©º")
        if not self.sftp_config.get('username'):
            errors.append("SFTP username ä¸èƒ½ç‚ºç©º")
        
        # é©—è­‰è·¯å¾‘é…ç½®
        if not self.path_config.get('default_output_dir'):
            errors.append("è¼¸å‡ºç›®éŒ„ä¸èƒ½ç‚ºç©º")
        
        # é©—è­‰ä¸¦è¡Œé…ç½®
        if self.parallel_config['max_workers'] < 1:
            errors.append("max_workers å¿…é ˆå¤§æ–¼ 0")
        
        return len(errors) == 0, errors

# å…¨åŸŸé…ç½®ç®¡ç†å™¨å¯¦ä¾‹
config_manager = ConfigManager()

# =====================================
# ===== JIRA API å®¢æˆ¶ç«¯ =====
# =====================================

from jira import JIRA
import requests
from urllib.parse import quote
import urllib3

# é—œé–‰ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class JiraAPIClient:
    """ç°¡åŒ–ç‰ˆ JIRA API å®¢æˆ¶ç«¯"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.session = None
        self._connected = False
        self.base_url = f"https://{config_manager.jira_config['site']}"
        
    def connect(self) -> bool:
        """é€£æ¥åˆ° JIRA"""
        try:
            username = config_manager.jira_config['username']
            password = config_manager.jira_config['password']
            
            self.session = requests.Session()
            self.session.auth = (username, password)
            self.session.headers.update({
                'Accept': 'application/json',
                'Content-Type': 'application/json'
            })
            self.session.verify = False
            
            # æ¸¬è©¦é€£æ¥
            test_url = f"{self.base_url}/rest/api/2/myself"
            response = self.session.get(test_url, timeout=30)
            
            if response.status_code == 200:
                self._connected = True
                return True
            else:
                self.logger.error(f"JIRA é€£æ¥å¤±æ•—: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"JIRA é€£æ¥å¤±æ•—: {e}")
            return False

    def search_db_ticket(self, db_name: str, module: str = None) -> Optional[str]:
        """æ ¹æ“š DB å‘½åæ…£ä¾‹æœå°‹å°æ‡‰çš„ JIRA ticket"""
        try:
            if not self._connected:
                if not self.connect():
                    return None
            
            # æ ¹æ“šå‘½åæ…£ä¾‹ç›´æ¥æ§‹å»º ticket key
            db_number = db_name.replace('DB', '')
            possible_tickets = [
                f"MMQCDB-{db_number}",
                f"LGSWRD-{db_number}",
                f"RTK-{db_number}",
                f"DB-{db_number}",
            ]
            
            for ticket_key in possible_tickets:
                if self._check_ticket_exists(ticket_key):
                    return ticket_key
            
            return None
            
        except Exception as e:
            self.logger.error(f"æœå°‹ JIRA ticket å¤±æ•—: {e}")
            return None

    def _check_ticket_exists(self, ticket_key: str) -> bool:
        """æª¢æŸ¥æŒ‡å®šçš„ ticket æ˜¯å¦å­˜åœ¨"""
        try:
            url = f"{self.base_url}/rest/api/2/issue/{ticket_key}"
            response = self.session.get(url, timeout=30)
            return response.status_code == 200
        except:
            return False

    def get_source_command_from_ticket(self, ticket_key: str) -> Optional[str]:
        """å¾ JIRA ticket ä¸­æå– source command"""
        try:
            if not self._connected:
                if not self.connect():
                    return None
            
            url = f"{self.base_url}/rest/api/2/issue/{ticket_key}"
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                fields = data.get('fields', {})
                
                # æª¢æŸ¥æè¿°æ¬„ä½
                description = fields.get('description', '')
                if description:
                    cmd = self._extract_repo_command(description)
                    if cmd:
                        return cmd
                
                # æª¢æŸ¥è©•è«–
                comments_data = fields.get('comment', {})
                comments = comments_data.get('comments', [])
                for comment in comments:
                    body = comment.get('body', '')
                    if body:
                        cmd = self._extract_repo_command(body)
                        if cmd:
                            return cmd
                
                return None
            
            return None
                
        except Exception as e:
            self.logger.error(f"å¾ ticket {ticket_key} ç²å– source command å¤±æ•—: {e}")
            return None

    def _extract_repo_command(self, text: str) -> Optional[str]:
        """æå– repo init å‘½ä»¤"""
        if not text or 'repo init' not in text:
            return None
        
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            
            # å°‹æ‰¾åŒ…å« repo init çš„è¡Œ
            if 'repo init' in line:
                repo_start = line.find('repo init')
                if repo_start != -1:
                    cmd = line[repo_start:].strip()
                    cmd = self._clean_command(cmd)
                    if self._is_valid_repo_command(cmd):
                        return cmd
        
        return None

    def _clean_command(self, cmd: str) -> str:
        """æ¸…ç†å‘½ä»¤å­—ç¬¦ä¸²"""
        # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´
        prefixes = ['$', '>', '#', '//', '*', '-', '=>']
        for prefix in prefixes:
            if cmd.startswith(prefix):
                cmd = cmd[len(prefix):].strip()
        
        cmd = cmd.rstrip('\n\r\\')
        return cmd.strip()

    def _is_valid_repo_command(self, cmd: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ repo init å‘½ä»¤"""
        if not cmd or not cmd.startswith('repo init'):
            return False
        
        required_parts = ['-u', '-b']
        return all(part in cmd for part in required_parts)

    def disconnect(self):
        """æ–·é–‹ JIRA é€£æ¥"""
        self._connected = False
        if self.session:
            self.session.close()
            self.session = None

# =====================================
# ===== æ—¥èªŒè¨­å®šå‡½å¼ =====
# =====================================

def setup_logger(name: str = __name__) -> logging.Logger:
    """è¨­å®šæ—¥èªŒè¨˜éŒ„å™¨"""
    logger = logging.getLogger(name)
    logger.setLevel(config_manager.log_config['level'])
    
    if not logger.handlers:
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(config_manager.log_config['level'])
        
        # Formatter
        formatter = logging.Formatter(
            config_manager.log_config['format'],
            datefmt=config_manager.log_config['date_format']
        )
        console_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
    
    return logger

logger = setup_logger(__name__)

# =====================================
# ===== é€²åº¦ç‹€æ…‹å®šç¾© =====
# =====================================

class DBStatus(Enum):
    """DB è™•ç†ç‹€æ…‹"""
    PENDING = "ç­‰å¾…ä¸­"
    DOWNLOADING_MANIFEST = "ä¸‹è¼‰ manifest"
    CHECKING_REPO = "æª¢æŸ¥ repo ç‹€æ…‹"
    REPO_INIT = "åŸ·è¡Œ repo init"
    REPO_SYNC = "åŸ·è¡Œ repo sync"
    EXPORTING = "å°å‡ºç‰ˆæœ¬"
    SUCCESS = "âœ… å®Œæˆ"
    FAILED = "âŒ å¤±æ•—"
    SKIPPED = "â­ï¸ è·³é"

# =====================================
# ===== è³‡æ–™çµæ§‹å®šç¾© =====
# =====================================

@dataclass
class DBInfo:
    """DB è³‡è¨Šè³‡æ–™çµæ§‹"""
    sn: int
    module: str
    db_type: str
    db_info: str
    db_folder: str
    sftp_path: str
    version: Optional[str] = None
    jira_link: Optional[str] = None
    source_command: Optional[str] = None
    manifest_file: Optional[str] = None
    manifest_full_path: Optional[str] = None
    local_path: Optional[str] = None
    has_existing_repo: bool = False
    status: DBStatus = DBStatus.PENDING
    error_message: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    sync_process: Optional[subprocess.Popen] = None
    actual_source_cmd: Optional[str] = None
    sync_log_path: Optional[str] = None

    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        
        # è™•ç† Enum å’Œ datetime
        if isinstance(result['status'], DBStatus):
            result['status'] = result['status'].value
        if result['start_time']:
            result['start_time'] = result['start_time'].strftime('%Y-%m-%d %H:%M:%S')
        if result['end_time']:
            result['end_time'] = result['end_time'].strftime('%Y-%m-%d %H:%M:%S')
        
        # ç§»é™¤ç„¡æ³•åºåˆ—åŒ–çš„ç‰©ä»¶
        result.pop('sync_process', None)
        
        return result

@dataclass
class PinningReport:
    """å®šç‰ˆå ±å‘Šè³‡æ–™çµæ§‹"""
    total_dbs: int = 0
    successful_dbs: int = 0
    failed_dbs: int = 0
    skipped_dbs: int = 0
    db_details: List[DBInfo] = field(default_factory=list)
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    
    def add_db(self, db_info: DBInfo):
        self.db_details.append(db_info)
        if db_info.status == DBStatus.SUCCESS:
            self.successful_dbs += 1
        elif db_info.status == DBStatus.FAILED:
            self.failed_dbs += 1
        elif db_info.status == DBStatus.SKIPPED:
            self.skipped_dbs += 1
    
    def finalize(self):
        """å®Œæˆå ±å‘Š"""
        self.end_time = datetime.now()
        self.total_dbs = len(self.db_details)

# =====================================
# ===== è³‡æºç®¡ç†å™¨ =====
# =====================================

class ResourceManager:
    """çµ±ä¸€ç®¡ç†æ‰€æœ‰ç³»çµ±è³‡æº"""
    
    def __init__(self):
        self.active_processes = {}
        self.sftp_connections = []
        self.lock = threading.Lock()
        self.logger = setup_logger(self.__class__.__name__)
        
        atexit.register(self.cleanup_all)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """è™•ç†ä¸­æ–·ä¿¡è™Ÿ"""
        print(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ¸…ç†æ‰€æœ‰é€²ç¨‹...")
        
        # åŸæœ‰çš„æ¸…ç†
        self.cleanup_all()
        
        # ğŸ”¥ æ–°å¢ï¼šç³»çµ±ç´šå¼·åˆ¶æ¸…ç†
        print("ğŸš¨ åŸ·è¡Œç³»çµ±ç´šæ¸…ç†...")
        os.system("pkill -TERM -f 'repo sync' 2>/dev/null || true")
        os.system("pkill -TERM -f 'unbuffer.*repo' 2>/dev/null || true")
        time.sleep(2)
        os.system("pkill -KILL -f 'repo sync' 2>/dev/null || true")
        os.system("pkill -KILL -f 'unbuffer.*repo' 2>/dev/null || true")
        
        print("âœ… æ¸…ç†å®Œæˆ")
        sys.exit(0)
    
    def register_process(self, name: str, process: subprocess.Popen):
        """è¨»å†Šæ–°çš„å­é€²ç¨‹"""
        with self.lock:
            self.active_processes[name] = process
    
    def unregister_process(self, name: str):
        """å–æ¶ˆè¨»å†Šå­é€²ç¨‹"""
        with self.lock:
            if name in self.active_processes:
                del self.active_processes[name]
    
    def register_sftp(self, connection):
        """è¨»å†Š SFTP é€£ç·š"""
        with self.lock:
            self.sftp_connections.append(connection)
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰è³‡æº"""
        with self.lock:
            # çµ‚æ­¢æ‰€æœ‰å­é€²ç¨‹
            for name, process in self.active_processes.items():
                try:
                    if process.poll() is None:
                        process.terminate()
                        process.wait(timeout=5)
                except:
                    try:
                        process.kill()
                    except:
                        pass
            
            # é—œé–‰æ‰€æœ‰ SFTP é€£ç·š
            for conn in self.sftp_connections:
                try:
                    if hasattr(conn, 'disconnect'):
                        conn.disconnect()
                    elif hasattr(conn, 'close'):
                        conn.close()
                except:
                    pass
            
            self.active_processes.clear()
            self.sftp_connections.clear()

# å…¨åŸŸè³‡æºç®¡ç†å™¨
resource_manager = ResourceManager()

# =====================================
# ===== æ”¹é€²ç‰ˆ SFTP ç®¡ç†å™¨ï¼ˆä¿®å¾© Garbage packet å•é¡Œï¼‰=====
# =====================================

class SFTPManager:
    """æ”¹é€²ç‰ˆ SFTP ç®¡ç†å™¨ - ä¿®å¾© type 3 unimplemented éŒ¯èª¤"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.config = config_manager.sftp_config
        self.sftp = None
        self.transport = None
        self.connected = False
        self._connection_lock = threading.Lock()
        # å¿«å–ä¼ºæœå™¨èƒ½åŠ›
        self._server_capabilities = {
            'supports_listdir_attr': None,
            'supports_stat': None,
            'checked': False
        }
        
    def connect(self) -> bool:
        """å»ºç«‹ SFTP é€£ç·š"""
        with self._connection_lock:
            try:
                # å¦‚æœå·²é€£ç·šï¼Œå…ˆæ¸…ç†
                if self.connected:
                    self.disconnect()
                
                self.logger.info(f"å»ºç«‹ SFTP é€£ç·š: {self.config['host']}:{self.config['port']}")
                
                # é‡è©¦é€£ç·š
                for attempt in range(self.config['retry_count']):
                    try:
                        # å»ºç«‹ Socket é€£ç·š
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(self.config['timeout'])
                        
                        # è¨­å®š socket é¸é …é¿å… Garbage packet
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                        
                        # é€£æ¥åˆ°ä¼ºæœå™¨
                        sock.connect((self.config['host'], self.config['port']))
                        
                        # å»ºç«‹ Transportï¼ˆä¿®å¾© Garbage packet çš„é—œéµï¼‰
                        self.transport = paramiko.Transport(sock)
                        
                        # è¨­å®šè¼ƒä¿å®ˆçš„åƒæ•¸é¿å…å”è­°å•é¡Œ
                        self.transport.set_keepalive(30)
                        self.transport.use_compression(False)  # é—œé–‰å£“ç¸®
                        
                        # é–‹å§‹ SSH æ¡æ‰‹
                        self.transport.start_client()
                        
                        # èªè­‰
                        self.transport.auth_password(
                            self.config['username'],
                            self.config['password']
                        )
                        
                        # å»ºç«‹ SFTP å®¢æˆ¶ç«¯
                        self.sftp = paramiko.SFTPClient.from_transport(self.transport)
                        
                        # æ¸¬è©¦é€£ç·šä¸¦æª¢æ¸¬ä¼ºæœå™¨èƒ½åŠ›
                        self._detect_server_capabilities()
                        
                        self.connected = True
                        resource_manager.register_sftp(self)
                        self.logger.info(f"SFTP é€£ç·šæˆåŠŸ (å˜—è©¦ {attempt + 1})")
                        return True
                        
                    except Exception as e:
                        self.logger.warning(f"SFTP é€£ç·šå˜—è©¦ {attempt + 1} å¤±æ•—: {e}")
                        self._cleanup_failed_connection()
                        
                        if attempt < self.config['retry_count'] - 1:
                            time.sleep(self.config['retry_delay'])
                
                self.logger.error("SFTP é€£ç·šå¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                return False
                
            except Exception as e:
                self.logger.error(f"SFTP é€£ç·šéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                self._cleanup_failed_connection()
                return False
    
    def _detect_server_capabilities(self):
        """æª¢æ¸¬ SFTP ä¼ºæœå™¨æ”¯æ´çš„åŠŸèƒ½"""
        try:
            self.logger.debug("æª¢æ¸¬ SFTP ä¼ºæœå™¨èƒ½åŠ›...")
            
            # æ¸¬è©¦åŸºæœ¬ listdir
            try:
                self.sftp.listdir('.')
                self.logger.debug("âœ… åŸºæœ¬ listdir æ”¯æ´")
            except Exception as e:
                self.logger.warning(f"âŒ åŸºæœ¬ listdir ä¸æ”¯æ´: {e}")
                raise Exception("ä¼ºæœå™¨ä¸æ”¯æ´åŸºæœ¬ SFTP æ“ä½œ")
            
            # æ¸¬è©¦ listdir_attr
            try:
                self.sftp.listdir_attr('.')
                self._server_capabilities['supports_listdir_attr'] = True
                self.logger.debug("âœ… listdir_attr æ”¯æ´")
            except Exception as e:
                self._server_capabilities['supports_listdir_attr'] = False
                self.logger.debug(f"âŒ listdir_attr ä¸æ”¯æ´: {e}")
            
            # æ¸¬è©¦ stat
            try:
                self.sftp.stat('.')
                self._server_capabilities['supports_stat'] = True
                self.logger.debug("âœ… stat æ”¯æ´")
            except Exception as e:
                self._server_capabilities['supports_stat'] = False
                self.logger.debug(f"âŒ stat ä¸æ”¯æ´: {e}")
            
            self._server_capabilities['checked'] = True
            self.logger.info(f"ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬å®Œæˆ: listdir_attr={self._server_capabilities['supports_listdir_attr']}, stat={self._server_capabilities['supports_stat']}")
            
        except Exception as e:
            self.logger.warning(f"ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬å¤±æ•—: {e}")
            # è¨­å®šç‚ºæœ€ä¿å®ˆçš„æ¨¡å¼
            self._server_capabilities = {
                'supports_listdir_attr': False,
                'supports_stat': False,
                'checked': True
            }
    
    def _cleanup_failed_connection(self):
        """æ¸…ç†å¤±æ•—çš„é€£ç·š"""
        try:
            if self.sftp:
                self.sftp.close()
                self.sftp = None
            if self.transport:
                self.transport.close()
                self.transport = None
            self.connected = False
            self._server_capabilities['checked'] = False
        except:
            pass
    
    def disconnect(self):
        """å®‰å…¨æ–·é–‹ SFTP é€£ç·š"""
        with self._connection_lock:
            try:
                if self.sftp:
                    self.sftp.close()
                    self.sftp = None
                if self.transport:
                    self.transport.close()
                    self.transport = None
                self.connected = False
                self._server_capabilities['checked'] = False
                self.logger.info("SFTP é€£ç·šå·²é—œé–‰")
            except Exception as e:
                self.logger.warning(f"é—œé–‰ SFTP é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _ensure_connected(self) -> bool:
        """ç¢ºä¿é€£ç·šæœ‰æ•ˆ"""
        if not self.connected or not self.sftp or not self.transport:
            return self.connect()
        
        # æ¸¬è©¦é€£ç·šæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        try:
            self.sftp.listdir('.')
            return True
        except:
            self.logger.info("SFTP é€£ç·šå·²æ–·é–‹ï¼Œé‡æ–°é€£ç·š...")
            return self.connect()
    
    def get_latest_version(self, sftp_path: str) -> Tuple[str, str, str]:
        """
        å–å¾—æœ€æ–°ç‰ˆæœ¬è³‡è¨Šï¼ˆç‰ˆè™Ÿæœ€å¤§çš„ï¼‰
        
        Args:
            sftp_path: SFTP è·¯å¾‘
            
        Returns:
            (db_folder, db_version, full_path)
        """
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            # å¾è·¯å¾‘ä¸­æå–åŸºç¤è³‡è¨Š
            path_parts = sftp_path.rstrip('/').split('/')
            db_folder = path_parts[-1] if path_parts else ''
            
            # åˆ—å‡ºç›®éŒ„å…§å®¹
            try:
                items = self._safe_listdir(sftp_path)
            except:
                self.logger.warning(f"ç„¡æ³•åˆ—å‡ºç›®éŒ„: {sftp_path}")
                return db_folder, '', sftp_path
            
            # éæ¿¾ç‰ˆæœ¬è³‡æ–™å¤¾ï¼ˆæ ¼å¼: æ•¸å­—é–‹é ­ï¼Œå¦‚ 536_all_202507312300ï¼‰
            version_folders = []
            for item in items:
                # æª¢æŸ¥æ˜¯å¦ç‚ºç‰ˆæœ¬è³‡æ–™å¤¾æ ¼å¼
                match = re.match(r'^(\d+)', item)
                if match:
                    version_num = int(match.group(1))
                    version_folders.append((version_num, item))
            
            if not version_folders:
                self.logger.warning(f"æ‰¾ä¸åˆ°ç‰ˆæœ¬è³‡æ–™å¤¾: {sftp_path}")
                return db_folder, '', sftp_path
            
            # æŒ‰ç‰ˆæœ¬è™Ÿæ’åºï¼Œå–æœ€å¤§çš„ï¼ˆæœ€æ–°çš„ï¼‰
            version_folders.sort(key=lambda x: x[0], reverse=True)
            latest_version = version_folders[0][1]
            full_path = f"{sftp_path}/{latest_version}"
            
            self.logger.info(f"æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬: {latest_version} (ç‰ˆè™Ÿ: {version_folders[0][0]}) in {sftp_path}")
            return db_folder, latest_version, full_path
            
        except Exception as e:
            self.logger.error(f"å–å¾—ç‰ˆæœ¬è³‡è¨Šå¤±æ•—: {str(e)}")
            return '', '', sftp_path
    
    def get_specific_version(self, sftp_path: str, db_info: str) -> Tuple[str, str, str]:
        """
        å–å¾—ç‰¹å®šç‰ˆæœ¬è³‡è¨Š
        
        Args:
            sftp_path: SFTP åŸºç¤è·¯å¾‘
            db_info: DBè³‡è¨Š (æ ¼å¼: DB2302#196)
            
        Returns:
            (db_folder, db_version, full_path)
        """
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            # è§£æ db_info
            if '#' not in db_info:
                self.logger.warning(f"DBè³‡è¨Šæ ¼å¼éŒ¯èª¤: {db_info}")
                return self.get_latest_version(sftp_path)
            
            db_number, version_prefix = db_info.split('#')
            
            # åˆ—å‡ºç›®éŒ„æ‰¾åˆ°å°æ‡‰çš„ DB è³‡æ–™å¤¾
            parent_path = '/'.join(sftp_path.rstrip('/').split('/')[:-1])
            items = self._safe_listdir(parent_path)
            
            db_folder = None
            for item in items:
                if item.startswith(f"{db_number}_"):
                    db_folder = item
                    break
            
            if not db_folder:
                self.logger.warning(f"æ‰¾ä¸åˆ° DB è³‡æ–™å¤¾: {db_number}")
                return '', '', sftp_path
            
            # å»ºæ§‹å®Œæ•´è·¯å¾‘
            db_path = f"{parent_path}/{db_folder}"
            
            # åˆ—å‡ºç‰ˆæœ¬è³‡æ–™å¤¾
            version_items = self._safe_listdir(db_path)
            
            # æ‰¾åˆ°å°æ‡‰ç‰ˆæœ¬
            for version_item in version_items:
                if version_item.startswith(f"{version_prefix}_"):
                    full_path = f"{db_path}/{version_item}"
                    self.logger.info(f"æ‰¾åˆ°æŒ‡å®šç‰ˆæœ¬: {version_item}")
                    return db_folder, version_item, full_path
            
            self.logger.warning(f"æ‰¾ä¸åˆ°æŒ‡å®šç‰ˆæœ¬: {version_prefix}")
            return db_folder, '', db_path
            
        except Exception as e:
            self.logger.error(f"å–å¾—ç‰¹å®šç‰ˆæœ¬å¤±æ•—: {str(e)}")
            return '', '', sftp_path
    
    def find_latest_manifest(self, path: str, db_name: str = None, target_version: str = None) -> Optional[Tuple[str, str]]:
        """æ”¹é€²ç‰ˆï¼šå¿«é€Ÿæœå°‹ manifest æª”æ¡ˆï¼ˆå¼·åŒ–éŒ¯èª¤è™•ç†ï¼‰"""
        try:
            if not self._ensure_connected():
                raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
            
            if target_version:
                self.logger.info(f"å¿«é€Ÿæœå°‹æŒ‡å®šç‰ˆæœ¬ {target_version}: {path}")
                return self._find_specific_version_manifest(path, target_version)
            else:
                self.logger.info(f"å¿«é€Ÿæœå°‹æœ€æ–°ç‰ˆæœ¬: {path}")
                return self._find_latest_version_manifest(path)
                
        except Exception as e:
            self.logger.error(f"æœç´¢ manifest å¤±æ•—: {e}")
            return None

    def _find_latest_version_manifest(self, base_path: str) -> Optional[Tuple[str, str]]:
        """æœå°‹æœ€æ–°ç‰ˆæœ¬çš„ manifest - å¼·åŒ– type 3 éŒ¯èª¤è™•ç†"""
        try:
            # åˆ—å‡ºç›®éŒ„
            version_dirs = []
            try:
                items = self._safe_listdir_with_details(base_path)
                
                for item_info in items:
                    try:
                        if item_info.get('is_dir', False):
                            dir_name = item_info['name']
                            version_num = self._extract_version_number(dir_name)
                            if version_num:
                                version_dirs.append({
                                    'name': dir_name,
                                    'version': int(version_num),
                                    'mtime': item_info.get('mtime', 0)
                                })
                    except Exception as e:
                        self.logger.debug(f"è·³éå•é¡Œç›®éŒ„: {item_info.get('name', 'unknown')}, éŒ¯èª¤: {e}")
                        continue
                        
            except Exception as e:
                self.logger.error(f"åˆ—å‡ºç›®éŒ„å¤±æ•—: {e}")
                return None
            
            if not version_dirs:
                raise Exception(f"åœ¨ {base_path} ä¸­æ²’æœ‰æ‰¾åˆ°ç‰ˆæœ¬ç›®éŒ„")
            
            # æŒ‰ç‰ˆæœ¬è™Ÿé™åºæ’åˆ—ï¼ˆæœ€æ–°ç‰ˆæœ¬åœ¨å‰ï¼‰
            version_dirs.sort(key=lambda x: (x['version'], x['mtime']), reverse=True)
            self.logger.info(f"æ‰¾åˆ° {len(version_dirs)} å€‹ç‰ˆæœ¬ç›®éŒ„ï¼Œæœ€æ–°ç‰ˆæœ¬: {version_dirs[0]['version']}")
            
            # åªæª¢æŸ¥å‰3å€‹æœ€æ–°ç‰ˆæœ¬
            for version_dir in version_dirs[:3]:
                self.logger.info(f"æª¢æŸ¥ç‰ˆæœ¬ {version_dir['version']} ({version_dir['name']})")
                manifest_path = self._try_get_manifest_from_dir(
                    base_path, 
                    version_dir['name'], 
                    str(version_dir['version'])
                )
                if manifest_path:
                    return manifest_path
            
            raise Exception("åœ¨æœ€æ–°ç‰ˆæœ¬ç›®éŒ„ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ manifest")
            
        except Exception as e:
            self.logger.error(f"æœå°‹æœ€æ–°ç‰ˆæœ¬å¤±æ•—: {e}")
            return None
        
    def _find_specific_version_manifest(self, base_path: str, version: str) -> Optional[Tuple[str, str]]:
        """æœå°‹æŒ‡å®šç‰ˆæœ¬çš„ manifest - å¼·åŒ– type 3 éŒ¯èª¤è™•ç†"""
        try:
            self.logger.info(f"æœå°‹æŒ‡å®šç‰ˆæœ¬: {version}")
            
            # å˜—è©¦åˆ—å‡ºç›®éŒ„ï¼Œå¦‚æœå¤±æ•—å°±ç”¨ç›´æ¥è·¯å¾‘æ–¹å¼
            matching_dirs = []
            try:
                items = self._safe_listdir_with_details(base_path)
                
                for item_info in items:
                    try:
                        # æª¢æŸ¥æ˜¯å¦ç‚ºç›®éŒ„
                        if item_info.get('is_dir', False):
                            dir_name = item_info['name']
                            if self._matches_version(dir_name, version):
                                matching_dirs.append({
                                    'name': dir_name,
                                    'mtime': item_info.get('mtime', 0)
                                })
                                self.logger.debug(f"æ‰¾åˆ°åŒ¹é…ç‰ˆæœ¬ç›®éŒ„: {dir_name}")
                    except Exception as e:
                        self.logger.debug(f"è·³éå•é¡Œé …ç›®: {item_info.get('name', 'unknown')}, éŒ¯èª¤: {e}")
                        continue
                        
            except Exception as e:
                self.logger.warning(f"åˆ—å‡ºç›®éŒ„å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è·¯å¾‘: {e}")
                return self._try_direct_version_paths(base_path, version)
            
            if not matching_dirs:
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦ç›´æ¥æ§‹å»ºè·¯å¾‘
                return self._try_direct_version_paths(base_path, version)
            
            # æŒ‰æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
            matching_dirs.sort(key=lambda x: x['mtime'], reverse=True)
            
            # å˜—è©¦æ¯å€‹åŒ¹é…çš„ç‰ˆæœ¬ç›®éŒ„
            for dir_info in matching_dirs:
                manifest_path = self._try_get_manifest_from_dir(base_path, dir_info['name'], version)
                if manifest_path:
                    return manifest_path
            
            # å¦‚æœåŒ¹é…ç›®éŒ„éƒ½å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è·¯å¾‘
            return self._try_direct_version_paths(base_path, version)
            
        except Exception as e:
            self.logger.error(f"æœå°‹æŒ‡å®šç‰ˆæœ¬å¤±æ•—: {e}")
            return None

    def _safe_listdir(self, path: str) -> list:
        """æœ€å®‰å…¨çš„åˆ—ç›®éŒ„æ–¹æ³• - åªä½¿ç”¨åŸºæœ¬ listdir"""
        try:
            return self.sftp.listdir(path)
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºç›®éŒ„å¤±æ•—: {path}, éŒ¯èª¤: {e}")
            raise
    
    def _safe_listdir_with_details(self, path: str) -> list:
        """å®‰å…¨çš„åˆ—å‡ºç›®éŒ„å…§å®¹ï¼Œæ ¹æ“šä¼ºæœå™¨èƒ½åŠ›é¸æ“‡æ–¹æ³•"""
        try:
            result = []
            
            # å¦‚æœæ”¯æ´ listdir_attrï¼Œå„ªå…ˆä½¿ç”¨
            if self._server_capabilities.get('supports_listdir_attr', False):
                try:
                    items = self.sftp.listdir_attr(path)
                    for item in items:
                        result.append({
                            'name': item.filename,
                            'is_dir': self._is_directory_from_stat(item),
                            'mtime': getattr(item, 'st_mtime', 0),
                            'size': getattr(item, 'st_size', 0)
                        })
                    return result
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self.logger.warning(f"listdir_attr ä¸æ”¯æ´ï¼Œåˆ‡æ›åˆ°åŸºæœ¬æ¨¡å¼: {e}")
                        self._server_capabilities['supports_listdir_attr'] = False
                    else:
                        raise
            
            # å›é€€åˆ°åŸºæœ¬ listdir + å€‹åˆ¥ stat
            self.logger.debug("ä½¿ç”¨åŸºæœ¬ listdir æ¨¡å¼")
            filenames = self.sftp.listdir(path)
            
            for filename in filenames:
                item_info = {
                    'name': filename,
                    'is_dir': False,  # é è¨­ç‚ºæª”æ¡ˆ
                    'mtime': 0,
                    'size': 0
                }
                
                # å˜—è©¦ç²å–è©³ç´°è³‡è¨Š
                if self._server_capabilities.get('supports_stat', True):
                    try:
                        full_path = f"{path}/{filename}"
                        stat_info = self.sftp.stat(full_path)
                        item_info.update({
                            'is_dir': self._is_directory_from_stat(stat_info),
                            'mtime': getattr(stat_info, 'st_mtime', 0),
                            'size': getattr(stat_info, 'st_size', 0)
                        })
                    except Exception as e:
                        error_msg = str(e).lower()
                        if 'unimplemented' in error_msg or 'type 3' in error_msg:
                            self.logger.debug(f"stat ä¸æ”¯æ´: {filename}")
                            self._server_capabilities['supports_stat'] = False
                        else:
                            self.logger.debug(f"stat å¤±æ•—: {filename}, {e}")
                        
                        # ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„
                        item_info['is_dir'] = self._guess_is_directory(filename)
                else:
                    # ä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•
                    item_info['is_dir'] = self._guess_is_directory(filename)
                
                result.append(item_info)
            
            return result
            
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºç›®éŒ„è©³ç´°è³‡è¨Šå¤±æ•—: {path}, éŒ¯èª¤: {e}")
            raise
    
    def _is_directory_from_stat(self, stat_obj) -> bool:
        """å¾ stat å°è±¡åˆ¤æ–·æ˜¯å¦ç‚ºç›®éŒ„"""
        try:
            return bool(stat_obj.st_mode & 0o40000)
        except:
            return False
    
    def _guess_is_directory(self, filename: str) -> bool:
        """å•Ÿç™¼å¼æ–¹æ³•çŒœæ¸¬æ˜¯å¦ç‚ºç›®éŒ„"""
        # å¦‚æœæª”ååŒ…å«æ˜é¡¯çš„å‰¯æª”åï¼Œå¯èƒ½æ˜¯æª”æ¡ˆ
        common_extensions = ['.xml', '.txt', '.log', '.zip', '.tar', '.gz', '.json', '.csv']
        filename_lower = filename.lower()
        
        for ext in common_extensions:
            if filename_lower.endswith(ext):
                return False
        
        # å¦‚æœçœ‹èµ·ä¾†åƒç‰ˆæœ¬ç›®éŒ„æ ¼å¼ï¼Œå‡è¨­æ˜¯ç›®éŒ„
        if re.match(r'^\d+(_all)?(_\d{12})?(_.*)?$', filename):
            return True
        
        # é è¨­å‡è¨­æ˜¯ç›®éŒ„ï¼ˆåœ¨æœå°‹ manifest çš„æƒ…å¢ƒä¸‹æ¯”è¼ƒå®‰å…¨ï¼‰
        return True

    def _try_direct_version_paths(self, base_path: str, version: str) -> Optional[Tuple[str, str]]:
        """ç•¶æ‰¾ä¸åˆ°ç‰ˆæœ¬ç›®éŒ„æ™‚ï¼Œå˜—è©¦ç›´æ¥æ§‹å»ºå¯èƒ½çš„è·¯å¾‘"""
        try:
            import datetime
            today = datetime.datetime.now()
            
            self.logger.info(f"å˜—è©¦ç›´æ¥è·¯å¾‘æœå°‹ç‰ˆæœ¬ {version}")
            
            # ç”Ÿæˆå¯èƒ½çš„æ™‚é–“æˆ³ï¼ˆæœ€è¿‘30å¤©ï¼Œæ¯3å¤©ä¸€å€‹ï¼‰
            for days_back in range(0, 30, 3):
                date = today - datetime.timedelta(days=days_back)
                
                # ç”Ÿæˆå¤šå€‹å¯èƒ½çš„æ™‚é–“æ ¼å¼
                timestamps = [
                    date.strftime("%Y%m%d0000"),  # 202508170000
                    date.strftime("%Y%m%d1200"),  # 202508171200
                    date.strftime("%Y%m%d1800"),  # 202508171800
                    date.strftime("%Y%m%d2300"),  # 202508172300
                ]
                
                for timestamp in timestamps:
                    # å˜—è©¦ä¸åŒçš„ç›®éŒ„æ ¼å¼
                    possible_dirs = [
                        f"{version}_{timestamp}",
                        f"{version}_all_{timestamp}",
                        f"{version}_all_{timestamp}_no-release",
                        f"{version}_all_{timestamp}_backup",
                        f"{version}_{timestamp}_backup",
                    ]
                    
                    for dir_name in possible_dirs:
                        try:
                            manifest_path = self._try_get_manifest_from_dir(base_path, dir_name, version)
                            if manifest_path:
                                self.logger.info(f"âœ… é€éç›´æ¥è·¯å¾‘æ‰¾åˆ°: {dir_name}")
                                return manifest_path
                        except Exception as e:
                            self.logger.debug(f"ç›´æ¥è·¯å¾‘å¤±æ•—: {dir_name}, {e}")
                            continue
            
            return None
            
        except Exception as e:
            self.logger.debug(f"ç›´æ¥è·¯å¾‘æœå°‹å¤±æ•—: {e}")
            return None
            
    def _try_get_manifest_from_dir(self, base_path: str, dir_name: str, version: str) -> Optional[Tuple[str, str]]:
        """å˜—è©¦å¾æŒ‡å®šç›®éŒ„ç²å– manifest - ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹æ³•"""
        try:
            version_path = f"{base_path}/{dir_name}"
            
            # ç›´æ¥æ§‹å»ºå¯èƒ½çš„ manifest æª”æ¡ˆå
            possible_manifests = [
                f"manifest_{version}.xml",
                "manifest.xml",
                f"default_{version}.xml", 
                "default.xml",
                f"{version}.xml"
            ]
            
            for manifest_name in possible_manifests:
                manifest_full_path = f"{version_path}/{manifest_name}"
                
                try:
                    # ä½¿ç”¨æœ€åŸºæœ¬çš„æª¢æŸ¥æ–¹æ³•
                    if self._file_exists_and_valid(manifest_full_path):
                        self.logger.info(f"âœ… æ‰¾åˆ°æœ‰æ•ˆ manifest: {manifest_name} (è·¯å¾‘: {dir_name})")
                        return manifest_full_path, manifest_name
                        
                except Exception as e:
                    self.logger.debug(f"æª¢æŸ¥æª”æ¡ˆå¤±æ•—: {manifest_name}, {e}")
                    continue
            
            return None
            
        except Exception as e:
            self.logger.debug(f"æª¢æŸ¥ç›®éŒ„å¤±æ•—: {dir_name}, {e}")
            return None

    def _file_exists_and_valid(self, file_path: str) -> bool:
        """æœ€åŸºæœ¬çš„æª”æ¡ˆå­˜åœ¨æ€§å’Œæœ‰æ•ˆæ€§æª¢æŸ¥"""
        try:
            # å¦‚æœ stat å¯ç”¨ï¼Œä½¿ç”¨ stat
            if self._server_capabilities.get('supports_stat', True):
                try:
                    stat_info = self.sftp.stat(file_path)
                    file_size = getattr(stat_info, 'st_size', 0)
                    return file_size > 1000  # manifest æª”æ¡ˆæ‡‰è©²å¤§æ–¼ 1KB
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self._server_capabilities['supports_stat'] = False
                    elif 'no such file' in error_msg or 'not found' in error_msg:
                        return False
                    else:
                        raise
            
            # å¦‚æœ stat ä¸å¯ç”¨ï¼Œå˜—è©¦æ‰“é–‹æª”æ¡ˆä¾†æª¢æŸ¥
            try:
                with self.sftp.open(file_path, 'r') as f:
                    # è®€å–å‰å¹¾å€‹å­—ç¯€æª¢æŸ¥æ˜¯å¦åƒ XML
                    header = f.read(100).decode('utf-8', errors='ignore')
                    return '<?xml' in header or '<manifest' in header
            except Exception:
                return False
                
        except Exception:
            return False
                    
    def _matches_version(self, dir_name: str, target_version: str) -> bool:
        """æª¢æŸ¥ç›®éŒ„åæ˜¯å¦åŒ¹é…æŒ‡å®šç‰ˆæœ¬"""
        patterns = [
            rf'^{target_version}_\d{{12}}$',                    # 5_202508170000
            rf'^{target_version}_all_\d{{12}}$',                # 702_all_202508092300  
            rf'^{target_version}_all_\d{{12}}_.*$',             # 669_all_202507142300_no-release
            rf'^{target_version}_\d{{12}}_.*$',                 # 5_202508170000_backup
        ]
        
        for pattern in patterns:
            if re.match(pattern, dir_name):
                return True
        return False
    
    def _extract_version_number(self, name: str) -> Optional[str]:
        """æå–ç‰ˆæœ¬è™Ÿ"""
        patterns = [
            r'^(\d+)_all_\d{12}',      # 702_all_202508092300
            r'^(\d+)_all_\d{12}_',     # 669_all_202507142300_no-release
            r'^(\d+)_\d{12}',          # 5_202508170000
            r'^(\d+)_\d{12}_',         # 5_202508170000_backup
            r'^(\d+)$',                # 5
            r'^v(\d+)',                # v5
        ]
        
        for pattern in patterns:
            match = re.search(pattern, name)
            if match:
                return match.group(1)
        
        return None
    
    def download_file(self, remote_path: str, local_path: str) -> bool:
        """ä¸‹è¼‰æª”æ¡ˆ"""
        try:
            if not self._ensure_connected():
                return False
            
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            filename = os.path.basename(remote_path)
            self.logger.info(f"é–‹å§‹ä¸‹è¼‰: {filename}")
            
            # ç²å–æª”æ¡ˆå¤§å°ï¼ˆå¦‚æœæ”¯æ´çš„è©±ï¼‰
            file_size = 0
            if self._server_capabilities.get('supports_stat', True):
                try:
                    file_stat = self.sftp.stat(remote_path)
                    file_size = file_stat.st_size
                    self.logger.debug(f"æª”æ¡ˆå¤§å°: {file_size} bytes")
                except Exception as e:
                    error_msg = str(e).lower()
                    if 'unimplemented' in error_msg or 'type 3' in error_msg:
                        self._server_capabilities['supports_stat'] = False
                        self.logger.debug("stat ä¸æ”¯æ´ï¼Œè·³éæª”æ¡ˆå¤§å°æª¢æŸ¥")
                    else:
                        self.logger.debug(f"ç„¡æ³•ç²å–æª”æ¡ˆå¤§å°: {e}")
            
            # ä¸‹è¼‰æª”æ¡ˆ
            self.sftp.get(remote_path, local_path)
            
            # é©—è­‰ä¸‹è¼‰
            if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                actual_size = os.path.getsize(local_path)
                self.logger.info(f"ä¸‹è¼‰å®Œæˆ: {filename} ({actual_size} bytes)")
                
                # é©—è­‰æª”æ¡ˆå¤§å°æ˜¯å¦ä¸€è‡´ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                if file_size > 0 and actual_size != file_size:
                    self.logger.warning(f"æª”æ¡ˆå¤§å°ä¸ä¸€è‡´: é æœŸ {file_size}, å¯¦éš› {actual_size}")
                
                return True
            else:
                self.logger.error(f"ä¸‹è¼‰çš„æª”æ¡ˆç„¡æ•ˆæˆ–ç‚ºç©º: {local_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {e}")
            return False

# =====================================
# ===== Source Command ç®¡ç†å™¨ =====
# =====================================

class SourceCommandManager:
    """ç®¡ç†ä¸åŒ DB çš„ source command"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.jira_client = JiraAPIClient()
        self.cache = {}
        
    def get_source_command(self, db_info: DBInfo, mapping_df: pd.DataFrame = None) -> Optional[str]:
        """æ ¹æ“š DB è³‡è¨Šç²å–å°æ‡‰çš„ source command"""
        db_name = db_info.db_info
        
        # æª¢æŸ¥å¿«å–
        if db_name in self.cache:
            self.logger.info(f"ä½¿ç”¨å¿«å–çš„ source command for {db_name}")
            return self.cache[db_name]
        
        # å¾ JIRA æœå°‹
        self.logger.info(f"å¾ JIRA æœå°‹ {db_name} çš„ source command")
        ticket_key = self.jira_client.search_db_ticket(db_name, db_info.module)
        if ticket_key:
            # æ›´æ–° jira_link
            db_info.jira_link = f"https://{config_manager.jira_config['site']}/browse/{ticket_key}"
            self.logger.info(f"æ‰¾åˆ° JIRA ticket: {ticket_key}")
            
            cmd = self.jira_client.get_source_command_from_ticket(ticket_key)
            if cmd:
                self.cache[db_name] = cmd
                self.logger.info(f"æˆåŠŸå¾ JIRA ç²å– source command")
                return cmd
        
        self.logger.warning(f"ç„¡æ³•å¾ JIRA ç²å– {db_name} çš„ source command")
        return None
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰å¿«å–"""
        self.cache.clear()

# =====================================
# ===== Mapping Table è®€å–å™¨ =====
# =====================================

class MappingTableReader:
    """è®€å–å’Œè§£æ mapping table çš„é¡åˆ¥"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.df = None
        
    def load_excel(self, file_path: str) -> bool:
        """è¼‰å…¥ Excel æª”æ¡ˆ"""
        try:
            self.df = pd.read_excel(file_path)
            self.logger.info(f"æˆåŠŸè¼‰å…¥ mapping table: {len(self.df)} ç­†è³‡æ–™")
            return True
        except Exception as e:
            self.logger.error(f"è¼‰å…¥ Excel å¤±æ•—: {str(e)}")
            return False
    
    def get_db_info_list(self, db_type: str = 'all') -> List[DBInfo]:
        """å–å¾— DB è³‡è¨Šåˆ—è¡¨"""
        db_list = []
        
        if self.df is None:
            return db_list
        
        type_columns = {
            'master': ('DB_Type', 'DB_Info', 'DB_Folder', 'SftpPath'),
            'premp': ('premp_DB_Type', 'premp_DB_Info', 'premp_DB_Folder', 'premp_SftpPath'),
            'mp': ('mp_DB_Type', 'mp_DB_Info', 'mp_DB_Folder', 'mp_SftpPath'),
            'mpbackup': ('mpbackup_DB_Type', 'mpbackup_DB_Info', 'mpbackup_DB_Folder', 'mpbackup_SftpPath')
        }
        
        types_to_process = type_columns.keys() if db_type == 'all' else [db_type]
        
        for idx, row in self.df.iterrows():
            for dtype in types_to_process:
                if dtype not in type_columns:
                    continue
                    
                cols = type_columns[dtype]
                db_info_col = cols[1]
                
                if db_info_col in row and pd.notna(row[db_info_col]):
                    db_info = DBInfo(
                        sn=row['SN'] if 'SN' in row else idx + 1,
                        module=row['Module'] if 'Module' in row else '',
                        db_type=dtype,
                        db_info=str(row[db_info_col]),
                        db_folder=str(row[cols[2]]) if cols[2] in row and pd.notna(row[cols[2]]) else '',
                        sftp_path=str(row[cols[3]]) if cols[3] in row and pd.notna(row[cols[3]]) else ''
                    )
                    db_list.append(db_info)
        
        return db_list

# =====================================
# ===== Repo ç®¡ç†å™¨ =====
# =====================================

class RepoManager:
    """Repo æŒ‡ä»¤ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.lock = threading.Lock()
    
    def check_repo_exists(self, work_dir: str) -> bool:
        """æª¢æŸ¥ .repo ç›®éŒ„æ˜¯å¦å­˜åœ¨"""
        repo_dir = os.path.join(work_dir, '.repo')
        exists = os.path.exists(repo_dir)
        self.logger.info(f"æª¢æŸ¥ .repo ç›®éŒ„: {work_dir} -> {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
        return exists
    
    def run_command(self, cmd: str, cwd: str = None, timeout: int = None) -> Tuple[bool, str]:
        """åŒæ­¥åŸ·è¡ŒæŒ‡ä»¤"""
        try:
            self.logger.debug(f"åŸ·è¡ŒæŒ‡ä»¤: {cmd}")
            result = subprocess.run(
                cmd,
                shell=True,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            if result.returncode == 0:
                self.logger.debug(f"æŒ‡ä»¤åŸ·è¡ŒæˆåŠŸ")
                return True, result.stdout
            else:
                self.logger.warning(f"æŒ‡ä»¤åŸ·è¡Œå¤±æ•— (è¿”å›ç¢¼: {result.returncode})")
                return False, result.stderr
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"æŒ‡ä»¤åŸ·è¡Œè¶…æ™‚")
            return False, "Command timeout"
        except Exception as e:
            self.logger.error(f"æŒ‡ä»¤åŸ·è¡Œç•°å¸¸: {e}")
            return False, str(e)
    
    def repo_init(self, work_dir: str, init_cmd: str) -> bool:
        """åŸ·è¡Œ repo init"""
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„èˆŠ .repo ç›®éŒ„
        repo_dir = os.path.join(work_dir, '.repo')
        if os.path.exists(repo_dir):
            self.logger.info(f"æ¸…ç†èˆŠçš„ .repo ç›®éŒ„: {repo_dir}")
            import shutil
            try:
                shutil.rmtree(repo_dir)
                self.logger.debug("èˆŠ .repo ç›®éŒ„æ¸…ç†æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"æ¸…ç† .repo å¤±æ•—: {e}")
        
        self.logger.info(f"åŸ·è¡Œ repo init: {init_cmd}")
        success, output = self.run_command(
            init_cmd,
            cwd=work_dir,
            timeout=config_manager.repo_config['init_timeout']
        )
        
        if success:
            self.logger.info("Repo init åŸ·è¡ŒæˆåŠŸ")
        else:
            self.logger.error(f"Repo init å¤±æ•—: {output}")
        
        return success
    
    def apply_manifest(self, work_dir: str, manifest_file: str) -> bool:
        """æ‡‰ç”¨ manifest æª”æ¡ˆ"""
        try:
            manifest_name = os.path.basename(manifest_file)
            self.logger.info(f"æ‡‰ç”¨ manifest: {manifest_name}")
            
            repo_dir = os.path.join(work_dir, '.repo')
            manifests_dir = os.path.join(repo_dir, 'manifests')
            
            if not os.path.exists(manifests_dir):
                self.logger.error(f"Manifests ç›®éŒ„ä¸å­˜åœ¨: {manifests_dir}")
                return False
            
            # è¤‡è£½ manifest æª”æ¡ˆ
            dest_file = os.path.join(manifests_dir, manifest_name)
            import shutil
            shutil.copy2(manifest_file, dest_file)
            self.logger.debug(f"è¤‡è£½ manifest: {manifest_file} -> {dest_file}")
            
            # åˆ‡æ›åˆ°æŒ‡å®šçš„ manifest
            switch_cmd = f"{config_manager.repo_config['repo_command']} init -m {manifest_name}"
            self.logger.info(f"åˆ‡æ› manifest: {switch_cmd}")
            
            success, output = self.run_command(
                switch_cmd,
                cwd=work_dir,
                timeout=config_manager.repo_config['init_timeout']
            )
            
            if success:
                self.logger.info(f"æˆåŠŸåˆ‡æ›åˆ° manifest: {manifest_name}")
                return True
            else:
                self.logger.error(f"åˆ‡æ› manifest å¤±æ•—: {output}")
                return False
                
        except Exception as e:
            self.logger.error(f"æ‡‰ç”¨ manifest å¤±æ•—: {str(e)}")
            return False
    
    def start_repo_sync_async(self, work_dir: str, db_name: str) -> subprocess.Popen:
        """ğŸ¯ ä¿®å¾©ç‰ˆæœ¬ - ä½¿ç”¨ unbuffer ç¢ºä¿å¯¦æ™‚è¼¸å‡º"""
        try:
            self.logger.info(f"{db_name}: å•Ÿå‹• unbuffer ç‰ˆæœ¬ repo sync")
            
            # æª¢æŸ¥å·¥ä½œç›®éŒ„
            if not os.path.exists(os.path.join(work_dir, '.repo')):
                raise Exception(f"å·¥ä½œç›®éŒ„æ²’æœ‰ .repo: {work_dir}")
            
            # æª¢æŸ¥ unbuffer æ˜¯å¦å¯ç”¨
            try:
                subprocess.run(['which', 'unbuffer'], check=True, capture_output=True, timeout=5)
                use_unbuffer = True
            except:
                use_unbuffer = False
                self.logger.warning(f"{db_name}: unbuffer ä¸å¯ç”¨ï¼Œä½¿ç”¨ script æ–¹æ³•")
            
            # å»ºç«‹æ—¥èªŒ
            log_dir = os.path.join(work_dir, 'logs')
            os.makedirs(log_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            method_name = "unbuffer" if use_unbuffer else "script"
            log_file = os.path.join(log_dir, f'repo_sync_{method_name}_{timestamp}.log')
            
            # å¯«å…¥åˆå§‹ä¿¡æ¯
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"=== Fixed Repo Sync Log for {db_name} ===\n")
                f.write(f"é–‹å§‹æ™‚é–“: {datetime.now()}\n")
                f.write(f"å·¥ä½œç›®éŒ„: {work_dir}\n")
                f.write(f"ä½¿ç”¨æ–¹æ³•: {method_name}\n\n")
            
            # æº–å‚™å‘½ä»¤
            repo_cmd = config_manager.repo_config['repo_command']
            jobs = min(config_manager.repo_config['sync_jobs'], 4)
            
            if use_unbuffer:
                # ğŸ¯ ä½¿ç”¨ unbuffer æ–¹æ³•ï¼ˆå·²é©—è­‰æœ‰æ•ˆï¼‰
                cmd_parts = [
                    'unbuffer',
                    repo_cmd, 'sync', 
                    f'-j{jobs}', 
                    '--verbose', 
                    '--force-sync'
                ]
                
                process = subprocess.Popen(
                    cmd_parts,
                    cwd=work_dir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=0,
                    preexec_fn=os.setsid
                )
            else:
                # å‚™ç”¨ï¼šä½¿ç”¨ script æ–¹æ³•
                shell_cmd = f"""
    cd "{work_dir}"
    echo "[SCRIPT] é€²ç¨‹å•Ÿå‹•ï¼ŒPID: $$" >> "{log_file}"
    script -fq /dev/null -c "{repo_cmd} sync -j{jobs} --verbose --force-sync" | tee -a "{log_file}"
    echo "[SCRIPT] é€²ç¨‹çµæŸï¼Œæ™‚é–“: $(date)" >> "{log_file}"
    """
                
                process = subprocess.Popen(
                    ['bash', '-c', shell_cmd],
                    cwd=work_dir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
            
            # ğŸ¯ å‰µå»ºå¯¦æ™‚æ—¥èªŒå¯«å…¥ç·šç¨‹ï¼ˆé—œéµæ”¹é€²ï¼‰
            def log_writer():
                try:
                    with open(log_file, 'a', encoding='utf-8', buffering=1) as f:
                        f.write(f"[UNBUFFER] é€²ç¨‹å•Ÿå‹•ï¼ŒPID: {process.pid}\n")
                        f.write(f"[UNBUFFER] é–‹å§‹æ™‚é–“: {datetime.now()}\n\n")
                        f.flush()
                        
                        # ğŸ”¥ é€²åº¦è¿½è¹¤è®Šæ•¸
                        last_reported_progress = -1
                        last_report_time = datetime.now()
                        message_count = 0
                        
                        if use_unbuffer:
                            while True:
                                line = process.stdout.readline()
                                if line:
                                    # ğŸ“ æ‰€æœ‰å…§å®¹éƒ½å¯«å…¥æ–‡ä»¶ï¼ˆä¸è®Šï¼‰
                                    f.write(line)
                                    f.flush()
                                    
                                    # ğŸ”¥ æ™ºèƒ½éæ¿¾ console è¼¸å‡º
                                    line_clean = line.strip()
                                    message_count += 1
                                    
                                    # âœ… åªå ±å‘Šé‡è¦çš„é€²åº¦è®ŠåŒ–
                                    if "Syncing:" in line_clean and "%" in line_clean:
                                        import re
                                        progress_match = re.search(r'Syncing:\s*(\d+)%\s*\((\d+)/(\d+)\)', line_clean)
                                        if progress_match:
                                            current_progress = int(progress_match.group(1))
                                            current_count = int(progress_match.group(2))
                                            total_count = int(progress_match.group(3))
                                            
                                            # ğŸ¯ åªåœ¨ä»¥ä¸‹æƒ…æ³å ±å‘Šé€²åº¦ï¼š
                                            should_report = (
                                                last_reported_progress == -1 or  # ç¬¬ä¸€æ¬¡
                                                current_progress - last_reported_progress >= 5 or  # é€²åº¦å¢åŠ 5%ä»¥ä¸Š
                                                current_progress % 10 == 0 or  # æ¯10%é‡Œç¨‹ç¢‘
                                                current_progress >= 95  # æ¥è¿‘å®Œæˆæ™‚
                                            )
                                            
                                            if should_report:
                                                # è¨ˆç®—é€Ÿåº¦
                                                current_time = datetime.now()
                                                elapsed = (current_time - last_report_time).total_seconds()
                                                
                                                speed_info = ""
                                                if last_reported_progress > 0 and elapsed > 0:
                                                    progress_diff = current_progress - last_reported_progress
                                                    speed = progress_diff / (elapsed / 60)  # %/åˆ†é˜
                                                    if speed > 0:
                                                        remaining_time = (100 - current_progress) / speed
                                                        speed_info = f" (é è¨ˆå‰©é¤˜: {remaining_time:.0f}åˆ†é˜)"
                                                
                                                # ğŸ“Š ç°¡æ½”çš„é€²åº¦å ±å‘Š
                                                # self.logger.info(
                                                #    f"{db_name}: {current_progress}% "
                                                #    f"({current_count}/{total_count}){speed_info}"
                                                #)
                                                
                                                last_reported_progress = current_progress
                                                last_report_time = current_time
                                    
                                    # âš ï¸ å ±å‘ŠéŒ¯èª¤å’Œè­¦å‘Š
                                    elif any(keyword in line_clean.lower() for keyword in 
                                        ['error:', 'fatal:', 'failed', 'timeout', 'exception', 'abort']):
                                        self.logger.warning(f"{db_name}: {line_clean}")
                                    
                                    # ğŸ‰ å ±å‘Šé‡è¦é‡Œç¨‹ç¢‘
                                    elif any(phrase in line_clean.lower() for phrase in
                                        ['sync has finished', 'completed successfully', 'repo sync complete']):
                                        self.logger.info(f"{db_name}: åŒæ­¥å®Œæˆï¼")
                                    
                                    # ğŸš« ä¸å ±å‘Šçš„å…§å®¹ï¼š
                                    # - "Skipped fetching project"
                                    # - "fetching project" 
                                    # - "..working.."
                                    # - é‡è¤‡çš„é€²åº¦ä¿¡æ¯
                                    
                                elif process.poll() is not None:
                                    break
                        
                        return_code = process.poll()
                        f.write(f"\n[UNBUFFER] é€²ç¨‹çµæŸï¼Œè¿”å›ç¢¼: {return_code}\n")
                        f.write(f"[UNBUFFER] çµæŸæ™‚é–“: {datetime.now()}\n")
                        f.write(f"[UNBUFFER] ç¸½è™•ç†æ¶ˆæ¯æ•¸: {message_count}\n")
                        f.flush()
                        
                        # ğŸ“ˆ æœ€çµ‚å ±å‘Š
                        if return_code == 0:
                            self.logger.info(f"{db_name}: âœ… åŒæ­¥æˆåŠŸå®Œæˆ")
                        else:
                            self.logger.error(f"{db_name}: âŒ åŒæ­¥å¤±æ•— (è¿”å›ç¢¼: {return_code})")
                        
                except Exception as e:
                    self.logger.error(f"{db_name}: æ—¥èªŒå¯«å…¥éŒ¯èª¤: {e}")
            
            # å•Ÿå‹•æ—¥èªŒç·šç¨‹
            if use_unbuffer:  # åªæœ‰ unbuffer éœ€è¦æ—¥èªŒç·šç¨‹
                log_thread = threading.Thread(target=log_writer, daemon=True)
                log_thread.start()
                process._log_thread = log_thread
            
            # é©—è­‰å•Ÿå‹•
            time.sleep(2)
            if process.poll() is not None:
                raise Exception(f"é€²ç¨‹ç«‹å³å¤±æ•—ï¼Œè¿”å›ç¢¼: {process.poll()}")
            
            # ä¿å­˜é€²ç¨‹ä¿¡æ¯
            process._log_file_path = log_file
            process._db_name = db_name
            process._start_time = datetime.now()
            
            resource_manager.register_process(db_name, process)
            self.logger.info(f"{db_name}: repo sync å•Ÿå‹•æˆåŠŸ (PID: {process.pid})")
            
            return process
            
        except Exception as e:
            self.logger.error(f"{db_name}: å•Ÿå‹•å¤±æ•—: {e}")
            return None
    
    def check_process_status(self, db_name: str, process: subprocess.Popen) -> Optional[int]:
        """æ”¹é€²çš„é€²ç¨‹ç‹€æ…‹æª¢æŸ¥"""
        with self.lock:
            if process:
                try:
                    poll = process.poll()
                    
                    # ğŸ”¥ åŠ å¼·æ—¥èªŒ
                    self.logger.debug(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹ PID={process.pid}, poll={poll}")
                    
                    if poll is not None:
                        # ğŸ”¥ é€²ç¨‹å·²çµæŸï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯
                        self.logger.info(f"{db_name}: é€²ç¨‹å·²çµæŸï¼Œè¿”å›ç¢¼={poll}")
                        
                        # æ­£ç¢ºé—œé–‰æ–‡ä»¶å¥æŸ„
                        if hasattr(process, '_log_file_handle') and process._log_file_handle:
                            try:
                                process._log_file_handle.flush()
                                process._log_file_handle.close()
                                self.logger.debug(f"{db_name}: æ—¥èªŒæ–‡ä»¶å¥æŸ„å·²é—œé–‰")
                            except Exception as e:
                                self.logger.warning(f"{db_name}: é—œé–‰æ—¥èªŒå¥æŸ„å¤±æ•—: {e}")
                        
                        resource_manager.unregister_process(db_name)
                        
                        # ğŸ”¥ å¯«å…¥å®Œæˆæ¨™è¨˜åˆ°æ—¥èªŒ
                        if hasattr(process, '_log_file_path'):
                            try:
                                with open(process._log_file_path, 'a') as f:
                                    f.write(f"\n=== é€²ç¨‹çµæŸ ===\n")
                                    f.write(f"è¿”å›ç¢¼: {poll}\n")
                                    f.write(f"çµæŸæ™‚é–“: {datetime.now()}\n")
                            except:
                                pass
                        
                        return poll
                    else:
                        # ğŸ”¥ é€²ç¨‹ä»åœ¨é‹è¡Œï¼Œä½†é©—è­‰ PID æ˜¯å¦çœŸçš„å­˜åœ¨
                        try:
                            os.kill(process.pid, 0)  # æª¢æŸ¥é€²ç¨‹æ˜¯å¦å­˜åœ¨
                            self.logger.debug(f"{db_name}: é€²ç¨‹ {process.pid} ç¢ºå¯¦åœ¨é‹è¡Œ")
                        except OSError:
                            self.logger.warning(f"{db_name}: é€²ç¨‹ {process.pid} ä¸å­˜åœ¨ä½† poll() è¿”å› None")
                            return -1  # é€²ç¨‹ç•°å¸¸æ¶ˆå¤±
                    
                    return None
                    
                except Exception as e:
                    self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
                    return -1
            
            return None
    
    def export_manifest(self, work_dir: str, output_file: str = "vp_manifest.xml") -> bool:
        """å°å‡º manifest"""
        cmd = f"{config_manager.repo_config['repo_command']} manifest -r -o {output_file}"
        self.logger.info(f"å°å‡º manifest: {cmd}")
        
        success, output = self.run_command(cmd, cwd=work_dir, timeout=60)
        
        if success:
            output_path = os.path.join(work_dir, output_file)
            if os.path.exists(output_path):
                self.logger.info(f"æˆåŠŸå°å‡º manifest: {output_path}")
                return True
        
        self.logger.error(f"å°å‡º manifest å¤±æ•—: {output}")
        return False

# =====================================
# ===== ä¸»è¦è™•ç†é¡åˆ¥ =====
# =====================================

class ManifestPinningTool:
    """Manifest å®šç‰ˆå·¥å…·ï¼ˆæ”¹é€²ç‰ˆï¼‰"""

    def __init__(self):
        self.logger = setup_logger(self.__class__.__name__)
        self.sftp_manager = SFTPManager()
        self.repo_manager = RepoManager()
        self.mapping_reader = MappingTableReader()
        self.source_cmd_manager = SourceCommandManager()
        self.report = PinningReport()
        self.output_dir = config_manager.path_config['default_output_dir']
        self.dry_run = False
        
        # ç·šç¨‹å®‰å…¨é–
        self._sftp_lock = threading.Lock()

    def load_mapping_table(self, file_path: str) -> bool:
        """è¼‰å…¥ mapping table"""
        return self.mapping_reader.load_excel(file_path)

    def get_all_dbs(self, db_type: str = 'all') -> List[DBInfo]:
        """å–å¾—æ‰€æœ‰ DB è³‡è¨Š"""
        return self.mapping_reader.get_db_info_list(db_type)

    def process_db_phase1(self, db_info: DBInfo) -> DBInfo:
        """æ”¹é€²ç‰ˆ Phase 1 è™•ç† - ç·šç¨‹å®‰å…¨"""
        db_info.start_time = datetime.now()
        
        try:
            self.logger.info(f"é–‹å§‹è™•ç† {db_info.db_info} (Phase 1)")
            
            # å»ºç«‹æœ¬åœ°ç›®éŒ„
            local_path = os.path.join(self.output_dir, db_info.module, db_info.db_info)
            os.makedirs(local_path, exist_ok=True)
            db_info.local_path = local_path
            
            # Step 1: ä½¿ç”¨é–ä¿è­·æ‰€æœ‰ SFTP æ“ä½œ
            with self._sftp_lock:
                self.logger.info(f"{db_info.db_info}: å¿«é€Ÿæœå°‹ manifest (ç·šç¨‹å®‰å…¨)")
                
                # ç¢ºä¿ SFTP é€£ç·šæœ‰æ•ˆ
                if not self.sftp_manager._ensure_connected():
                    raise Exception("ç„¡æ³•å»ºç«‹ SFTP é€£ç·š")
                
                target_version = db_info.version
                result = self.sftp_manager.find_latest_manifest(
                    db_info.sftp_path, 
                    db_info.db_info,
                    target_version
                )
                
                if not result:
                    raise Exception("æ‰¾ä¸åˆ° manifest æª”æ¡ˆ")
                
                manifest_full_path, manifest_name = result
                db_info.manifest_full_path = manifest_full_path
                db_info.manifest_file = manifest_name
                
                # æå–ç‰ˆæœ¬è™Ÿ
                if not db_info.version:
                    match = re.match(r'manifest_(\d+)\.xml', manifest_name)
                    if match:
                        db_info.version = match.group(1)
                        self.logger.info(f"{db_info.db_info}: æª¢æ¸¬åˆ°ç‰ˆæœ¬ {db_info.version}")
                
                # ä¸‹è¼‰åˆ°æœ¬åœ°
                local_manifest = os.path.join(local_path, manifest_name)
                if not self.sftp_manager.download_file(manifest_full_path, local_manifest):
                    raise Exception("ä¸‹è¼‰ manifest å¤±æ•—")
                
                self.logger.info(f"{db_info.db_info}: manifest ä¸‹è¼‰å®Œæˆ: {manifest_name}")
            
            # SFTP æ“ä½œå®Œæˆï¼Œé–å·²é‡‹æ”¾ï¼Œå…¶ä»–æ“ä½œå¯ä»¥ä¸¦è¡Œ
            
            # Step 2: æª¢æŸ¥ repo ç‹€æ…‹
            db_info.has_existing_repo = self.repo_manager.check_repo_exists(local_path)
            
            # Step 3: ç²å– source command
            self.logger.info(f"{db_info.db_info}: ç²å– source command")
            source_cmd = self.source_cmd_manager.get_source_command(db_info, self.mapping_reader.df)
            if not source_cmd:
                raise Exception("ç„¡æ³•å–å¾— source command")
            
            db_info.actual_source_cmd = source_cmd
            self.logger.info(f"{db_info.db_info}: source command ç²å–æˆåŠŸ")
            
            # Step 4: åŸ·è¡Œ repo init
            self.logger.info(f"{db_info.db_info}: åŸ·è¡Œ repo åˆå§‹åŒ–")
            if not db_info.has_existing_repo:
                if not self.repo_manager.repo_init(local_path, source_cmd):
                    raise Exception("Repo init å¤±æ•—")
            
            # æ‡‰ç”¨ manifest
            if not self.repo_manager.apply_manifest(local_path, local_manifest):
                raise Exception("å¥—ç”¨ manifest å¤±æ•—")
            
            self.logger.info(f"{db_info.db_info}: repo åˆå§‹åŒ–å®Œæˆ")
            
            # Step 5: å•Ÿå‹• repo sync
            if not self.dry_run:
                self.logger.info(f"{db_info.db_info}: å•Ÿå‹• repo sync")
                process = self.repo_manager.start_repo_sync_async(local_path, db_info.db_info)
                if not process:
                    raise Exception("å•Ÿå‹• repo sync å¤±æ•—")
                
                db_info.sync_process = process
                self.logger.info(f"{db_info.db_info}: repo sync å·²å•Ÿå‹• (PID: {process.pid})")
            else:
                db_info.status = DBStatus.SKIPPED
                self.logger.info(f"{db_info.db_info}: æ¸¬è©¦æ¨¡å¼ - è·³é repo sync")
            
            self.logger.info(f"{db_info.db_info}: Phase 1 å®Œæˆ")
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            self.logger.error(f"{db_info.db_info}: Phase 1 å¤±æ•— - {str(e)}")
        # ğŸ”¥ æ–°å¢ï¼šç¢ºä¿ç•¶å‰ç·šç¨‹çš„ SFTP æ“ä½œå®Œå…¨çµæŸ
        finally:
            # å¼·åˆ¶é‡‹æ”¾ SFTP é–ï¼ˆå¦‚æœç•¶å‰ç·šç¨‹æŒæœ‰çš„è©±ï¼‰
            if hasattr(self, '_sftp_lock'):
                try:
                    # é–æœƒåœ¨ with èªå¥çµæŸæ™‚è‡ªå‹•é‡‹æ”¾ï¼Œé€™è£¡åªæ˜¯ç¢ºä¿
                    pass
                except:
                    pass
        
        return db_info

    def process_db_phase2(self, db_info: DBInfo) -> DBInfo:
        """è™•ç† DB çš„ç¬¬äºŒéšæ®µï¼šå®Œæˆå·¥ä½œ"""
        try:
            self.logger.info(f"{db_info.db_info}: é–‹å§‹ Phase 2")
            
            if self.dry_run:
                db_info.status = DBStatus.SKIPPED
                db_info.end_time = datetime.now()
                self.logger.info(f"{db_info.db_info}: æ¸¬è©¦æ¨¡å¼å®Œæˆ")
                return db_info
            
            # æª¢æŸ¥ sync é€²ç¨‹ç‹€æ…‹
            if db_info.sync_process:
                poll = self.repo_manager.check_process_status(
                    db_info.db_info, 
                    db_info.sync_process
                )
                
                if poll is None:
                    self.logger.debug(f"{db_info.db_info}: repo sync ä»åœ¨åŸ·è¡Œä¸­")
                    return db_info  # é‚„åœ¨åŸ·è¡Œä¸­
                elif poll != 0:
                    raise Exception(f"Repo sync å¤±æ•— (è¿”å›ç¢¼: {poll})")
            
            # å°å‡º manifest
            self.logger.info(f"{db_info.db_info}: å°å‡ºç‰ˆæœ¬è³‡è¨Š")
            if not self.repo_manager.export_manifest(db_info.local_path):
                raise Exception("å°å‡º manifest å¤±æ•—")
            
            # å®Œæˆ
            db_info.status = DBStatus.SUCCESS
            db_info.end_time = datetime.now()
            
            elapsed = db_info.end_time - db_info.start_time
            self.logger.info(f"{db_info.db_info}: è™•ç†å®Œæˆ (è€—æ™‚: {elapsed})")
            
        except Exception as e:
            db_info.status = DBStatus.FAILED
            db_info.error_message = str(e)
            db_info.end_time = datetime.now()
            self.logger.error(f"{db_info.db_info}: Phase 2 å¤±æ•— - {str(e)}")
        
        return db_info

    def process_dbs_async(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """ç•°æ­¥è™•ç†å¤šå€‹ DB - å¾¹åº•é¿å… SFTP è¡çª"""
        db_versions = db_versions or {}
        db_infos = []
        
        # æº–å‚™ DB è³‡è¨Š
        all_db_infos = self.get_all_dbs('all')
        
        for db_name in db_list:
            if '#' in db_name:
                db_name, version = db_name.split('#', 1)
            else:
                version = db_versions.get(db_name)
            
            for db_info in all_db_infos:
                if db_info.db_info == db_name:
                    db_info.version = version
                    db_infos.append(db_info)
                    break
        
        if not db_infos:
            self.logger.error("æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
            return

        self.logger.info(f"é–‹å§‹è™•ç† {len(db_infos)} å€‹ DB")
        
        try:
            # Phase 1: æº–å‚™å’Œå•Ÿå‹• sync
            self.logger.info("åŸ·è¡Œ Phase 1: æº–å‚™å·¥ä½œå’Œå•Ÿå‹•åŒæ­¥")
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase1, db_info): db_info for db_info in db_infos}
                
                phase1_results = []
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=300)
                        phase1_results.append(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 1 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        phase1_results.append(db_info)
            
            # ğŸ”¥ é—œéµä¿®å¾©ï¼šPhase 1 å®Œæˆå¾Œç«‹å³æ–·é–‹æ‰€æœ‰ SFTP é€£ç·š
            self.logger.info("Phase 1 å®Œæˆï¼Œæ–·é–‹æ‰€æœ‰ SFTP é€£ç·šä»¥é¿å…è¡çª")
            try:
                if hasattr(self, 'sftp_manager') and self.sftp_manager:
                    self.sftp_manager.disconnect()
                    self.logger.debug("ä¸» SFTP é€£ç·šå·²æ–·é–‹")
            except Exception as e:
                self.logger.debug(f"æ–·é–‹ä¸» SFTP é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

            # ç­‰å¾…æ‰€æœ‰ sync å®Œæˆï¼ˆå¢å¼·ç‰ˆç›£æ§ï¼‰
            if not self.dry_run:
                self.logger.info("ç­‰å¾…æ‰€æœ‰ repo sync å®Œæˆ...ï¼ˆå¢å¼·ç‰ˆé€²åº¦ç›£æ§ï¼‰")
                self._wait_for_all_syncs_enhanced(phase1_results)  # ğŸ‘ˆ ä½¿ç”¨æ–°çš„å‡½æ•¸
            
            with ThreadPoolExecutor(max_workers=config_manager.parallel_config['max_workers']) as executor:
                futures = {executor.submit(self.process_db_phase2, db_info): db_info for db_info in phase1_results}
                
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=60)
                        self.report.add_db(result)
                    except Exception as e:
                        db_info = futures[future]
                        self.logger.error(f"{db_info.db_info}: Phase 2 ç•°å¸¸ - {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = str(e)
                        self.report.add_db(db_info)
            
            self.logger.info("æ‰€æœ‰ DB è™•ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

    def _wait_for_all_syncs_safe(self, db_results: List[DBInfo]):
        """å®Œæ•´ç‰ˆé€²åº¦ç›£æ§ - åŒ…å«éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶"""
        max_wait_time = config_manager.repo_config['sync_timeout']
        start_wait = time.time()
        
        active_syncs = [db for db in db_results if db.sync_process and db.status != DBStatus.FAILED]
        self.logger.info(f"ç›£æ§ {len(active_syncs)} å€‹æ´»èºçš„ repo sync é€²ç¨‹")
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
        progress_tracker = {}
        for db_info in active_syncs:
            progress_tracker[db_info.db_info] = {
                'start_time': db_info.start_time or datetime.now(),
                'last_log_size': 0,
                'estimated_progress': 0,
                'current_activity': 'åˆå§‹åŒ–ä¸­...',
                'log_file': self._get_sync_log_file(db_info),
                'last_check_time': datetime.now(),
                'error_count': 0
            }
        
        check_interval = 30  # 30ç§’æª¢æŸ¥ä¸€æ¬¡
        
        while True:
            all_complete = True
            elapsed = int(time.time() - start_wait)
            
            print("\n" + "="*100)
            print(f"ğŸ“Š Repo Sync é€²åº¦ç›£æ§ - å·²ç­‰å¾… {elapsed}s")
            print("="*100)
            
            for db_info in active_syncs:
                if db_info.status == DBStatus.FAILED:
                    continue
                
                db_name = db_info.db_info
                tracker = progress_tracker[db_name]
                
                # æ§‹å»ºåŒ…å«ç‰ˆæœ¬ä¿¡æ¯çš„é¡¯ç¤ºåç¨±
                manifest_info = ""
                if db_info.manifest_file:
                    manifest_info = f" ({db_info.manifest_file})"
                elif db_info.version:
                    manifest_info = f" (v{db_info.version})"
                
                display_name = f"{db_name}{manifest_info}"
                
                if db_info.sync_process:
                    try:
                        poll = db_info.sync_process.poll()
                        
                        if poll is None:  # ä»åœ¨é‹è¡Œ
                            all_complete = False
                            
                            # ğŸ”§ æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤éœ€è¦è™•ç†
                            error_detected = self._check_for_sync_errors(db_info, tracker)
                            
                            if error_detected:
                                # å˜—è©¦è™•ç†éŒ¯èª¤
                                if self._handle_sync_failure(db_info, error_detected):
                                    tracker['current_activity'] = f"æª¢æ¸¬åˆ°éŒ¯èª¤: {error_detected}ï¼Œæ­£åœ¨é‡è©¦..."
                                    tracker['error_count'] += 1
                                else:
                                    # é‡è©¦å¤±æ•—ï¼Œæ¨™è¨˜ç‚ºå¤±æ•—
                                    db_info.status = DBStatus.FAILED
                                    db_info.error_message = error_detected
                                    runtime = datetime.now() - tracker['start_time']
                                    print(f"âŒ {display_name:30s} | å¤±æ•—     |   0% | "
                                        f"ç”¨æ™‚ {str(runtime).split('.')[0]:8s} | {error_detected}")
                                    continue
                            
                            # æ›´æ–°é€²åº¦ä¿¡æ¯
                            self._update_progress_info(db_info, tracker)
                            
                            # æ§‹å»ºè©³ç´°çš„æ´»å‹•ä¿¡æ¯
                            activity_text = tracker['current_activity']
                            
                            # æ·»åŠ ç•¶å‰é …ç›®ä¿¡æ¯
                            if tracker.get('current_project'):
                                project_short = tracker['current_project'].split('/')[-1]
                                activity_text = f"{activity_text} [{project_short}]"
                            
                            # æ·»åŠ ä¸‹è¼‰é€Ÿåº¦
                            if tracker.get('download_speed'):
                                activity_text = f"{activity_text} {tracker['download_speed']}"
                            
                            # æ·»åŠ éŒ¯èª¤è¨ˆæ•¸ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                            if tracker['error_count'] > 0:
                                activity_text = f"{activity_text} (é‡è©¦:{tracker['error_count']})"
                            
                            # é¡¯ç¤ºè©³ç´°ç‹€æ…‹
                            runtime = datetime.now() - tracker['start_time']
                            print(f"ğŸ”„ {display_name:30s} | é‹è¡Œä¸­ | {tracker['estimated_progress']:3d}% | "
                                f"ç”¨æ™‚ {str(runtime).split('.')[0]:8s} | {activity_text}")
                            
                            # æª¢æŸ¥è¶…æ™‚
                            if time.time() - start_wait > max_wait_time:
                                self.logger.warning(f"{db_name}: repo sync è¶…æ™‚ï¼Œå¼·åˆ¶çµ‚æ­¢")
                                try:
                                    db_info.sync_process.terminate()
                                    db_info.sync_process.wait(timeout=5)
                                except:
                                    try:
                                        db_info.sync_process.kill()
                                    except:
                                        pass
                                db_info.status = DBStatus.FAILED
                                db_info.error_message = "Sync è¶…æ™‚"
                                print(f"â° {display_name:30s} | è¶…æ™‚çµ‚æ­¢")
                                
                        elif poll == 0:  # æˆåŠŸå®Œæˆ
                            runtime = datetime.now() - tracker['start_time']
                            self.logger.info(f"{db_name}: repo sync æˆåŠŸå®Œæˆ")
                            print(f"âœ… {display_name:30s} | å®Œæˆ     | 100% | "
                                f"ç”¨æ™‚ {str(runtime).split('.')[0]:8s} | Sync æˆåŠŸå®Œæˆ")
                            
                        else:  # å¤±æ•—
                            runtime = datetime.now() - tracker['start_time']
                            error_msg = f"Sync å¤±æ•— (è¿”å›ç¢¼: {poll})"
                            
                            # å˜—è©¦å¾æ—¥èªŒä¸­ç²å–æ›´è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯
                            detailed_error = self._extract_error_from_log(tracker.get('log_file', ''))
                            if detailed_error:
                                error_msg = f"{error_msg} - {detailed_error}"
                            
                            # å˜—è©¦è™•ç†å¤±æ•—
                            if self._handle_sync_failure(db_info, error_msg):
                                tracker['current_activity'] = "é€²ç¨‹å¤±æ•—ï¼Œæ­£åœ¨é‡è©¦..."
                                tracker['error_count'] += 1
                                all_complete = False  # é‚„åœ¨é‡è©¦ä¸­
                            else:
                                db_info.status = DBStatus.FAILED
                                db_info.error_message = error_msg
                                print(f"âŒ {display_name:30s} | å¤±æ•—     |   0% | "
                                    f"ç”¨æ™‚ {str(runtime).split('.')[0]:8s} | {error_msg}")
                            
                    except Exception as e:
                        self.logger.error(f"{db_name}: æª¢æŸ¥é€²ç¨‹ç‹€æ…‹å¤±æ•—: {e}")
                        db_info.status = DBStatus.FAILED
                        db_info.error_message = f"é€²ç¨‹ç›£æ§å¤±æ•—: {e}"
                        print(f"âš ï¸  {display_name:30s} | ç›£æ§éŒ¯èª¤ |   0% | {str(e)[:30]}")
            
            # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
            running_count = sum(1 for db in active_syncs 
                            if db.sync_process and db.sync_process.poll() is None 
                            and db.status != DBStatus.FAILED)
            completed_count = sum(1 for db in active_syncs 
                                if db.sync_process and db.sync_process.poll() == 0)
            failed_count = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
            retry_count = sum(1 for db in active_syncs 
                            if progress_tracker.get(db.db_info, {}).get('error_count', 0) > 0)
            
            print("-"*100)
            print(f"ğŸ“ˆ ç¸½è¨ˆ: é‹è¡Œä¸­ {running_count} | å®Œæˆ {completed_count} | å¤±æ•— {failed_count} | é‡è©¦ä¸­ {retry_count}")
                        
            if all_complete or (time.time() - start_wait) > max_wait_time:
                break
            
            # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
            time.sleep(check_interval)
        
        # æœ€çµ‚çµ±è¨ˆ
        completed = sum(1 for db in active_syncs if db.sync_process and db.sync_process.poll() == 0)
        failed = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        total_retries = sum(progress_tracker.get(db.db_info, {}).get('error_count', 0) for db in active_syncs)
        
        print(f"\nğŸ Repo sync æœ€çµ‚çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸ: {completed}")
        print(f"   âŒ å¤±æ•—: {failed}")
        print(f"   ğŸ”„ ç¸½é‡è©¦æ¬¡æ•¸: {total_retries}")
        
        self.logger.info(f"ğŸ Repo sync å®Œæˆçµ±è¨ˆ: æˆåŠŸ {completed}, å¤±æ•— {failed}, ç¸½é‡è©¦ {total_retries}")

    def _handle_sync_failure(self, db_info: DBInfo, error_msg: str) -> bool:
        """è™•ç† sync å¤±æ•—ä¸¦å˜—è©¦é‡è©¦"""
        self.logger.warning(f"{db_info.db_info}: Sync éŒ¯èª¤ - {error_msg}")
        
        # åˆå§‹åŒ–é‡è©¦è¨ˆæ•¸
        if not hasattr(db_info, 'retry_count'):
            db_info.retry_count = 0
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯å¯é‡è©¦çš„éŒ¯èª¤
        retryable_errors = [
            'broken pipe', 'connection', 'timeout', 'network', 'temporary failure',
            'unable to connect', 'connection refused', 'connection timed out',
            'socket timeout', 'ssl error', 'certificate', 'dns'
        ]
        
        is_retryable = any(err in error_msg.lower() for err in retryable_errors)
        max_retries = 3  # æœ€å¤šé‡è©¦3æ¬¡
        
        if is_retryable and db_info.retry_count < max_retries:
            db_info.retry_count += 1
            
            # è¨ˆç®—é‡è©¦å»¶é²ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
            retry_delay = min(30 * (2 ** (db_info.retry_count - 1)), 300)  # æœ€å¤šç­‰5åˆ†é˜
            
            self.logger.info(f"{db_info.db_info}: æº–å‚™é‡è©¦ (ç¬¬ {db_info.retry_count}/{max_retries} æ¬¡)ï¼Œ{retry_delay}ç§’å¾Œé–‹å§‹")
            
            try:
                # æ¸…ç†èˆŠé€²ç¨‹
                if db_info.sync_process:
                    try:
                        if db_info.sync_process.poll() is None:
                            db_info.sync_process.terminate()
                            db_info.sync_process.wait(timeout=10)
                    except:
                        try:
                            db_info.sync_process.kill()
                        except:
                            pass
                    finally:
                        resource_manager.unregister_process(db_info.db_info)
                
                # ç­‰å¾…é‡è©¦å»¶é²
                import time
                time.sleep(retry_delay)
                
                # é‡æ–°å•Ÿå‹• repo sync
                self.logger.info(f"{db_info.db_info}: é–‹å§‹ç¬¬ {db_info.retry_count} æ¬¡é‡è©¦")
                process = self.repo_manager.start_repo_sync_async(
                    db_info.local_path, 
                    db_info.db_info
                )
                
                if process:
                    db_info.sync_process = process
                    self.logger.info(f"{db_info.db_info}: é‡è©¦æˆåŠŸå•Ÿå‹• (PID: {process.pid})")
                    return True
                else:
                    self.logger.error(f"{db_info.db_info}: é‡è©¦å•Ÿå‹•å¤±æ•—")
                    
            except Exception as e:
                self.logger.error(f"{db_info.db_info}: é‡è©¦éç¨‹ä¸­ç™¼ç”Ÿç•°å¸¸: {e}")
        
        # é‡è©¦å¤±æ•—æˆ–ä¸å¯é‡è©¦
        if db_info.retry_count >= max_retries:
            self.logger.error(f"{db_info.db_info}: å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ ({max_retries})ï¼Œæ¨™è¨˜ç‚ºå¤±æ•—")
        else:
            self.logger.error(f"{db_info.db_info}: éŒ¯èª¤ä¸å¯é‡è©¦ï¼Œæ¨™è¨˜ç‚ºå¤±æ•—")
        
        return False

    def _check_for_sync_errors(self, db_info: DBInfo, tracker: dict) -> str:
        """æª¢æŸ¥ sync éç¨‹ä¸­çš„éŒ¯èª¤"""
        try:
            log_file = tracker.get('log_file', '')
            
            if not log_file or not os.path.exists(log_file):
                return ""
            
            current_size = os.path.getsize(log_file)
            last_size = tracker.get('last_log_size', 0)
            
            if current_size <= last_size:
                # æ—¥èªŒæ²’æœ‰å¢é•·ï¼Œæª¢æŸ¥æ˜¯å¦å¡ä½
                last_check = tracker.get('last_check_time', datetime.now())
                if (datetime.now() - last_check).total_seconds() > 300:  # 5åˆ†é˜æ²’æœ‰æ—¥èªŒæ›´æ–°
                    return "æ—¥èªŒåœæ­¢æ›´æ–°ï¼Œå¯èƒ½é€²ç¨‹å¡ä½"
            
            tracker['last_log_size'] = current_size
            tracker['last_check_time'] = datetime.now()
            
            # è®€å–æ–°å¢çš„æ—¥èªŒå…§å®¹
            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(last_size)
                    new_content = f.read()
                    
                    # æª¢æŸ¥å„ç¨®éŒ¯èª¤æ¨¡å¼
                    error_patterns = [
                        (r'brokenpipeerror|broken pipe', 'Broken pipe error'),
                        (r'connection.*refused', 'Connection refused'),
                        (r'connection.*timeout|timeout.*connection', 'Connection timeout'),
                        (r'ssl.*error|certificate.*error', 'SSL/Certificate error'),
                        (r'unable to connect', 'Unable to connect'),
                        (r'network.*unreachable', 'Network unreachable'),
                        (r'temporary failure', 'Temporary failure'),
                        (r'fatal:.*clone', 'Clone failed'),
                        (r'error:.*fetch', 'Fetch failed'),
                    ]
                    
                    for pattern, error_type in error_patterns:
                        if re.search(pattern, new_content, re.IGNORECASE):
                            return error_type
                            
            except Exception as e:
                self.logger.debug(f"æª¢æŸ¥éŒ¯èª¤æ™‚è®€å–æ—¥èªŒå¤±æ•—: {e}")
            
            return ""
            
        except Exception as e:
            self.logger.debug(f"éŒ¯èª¤æª¢æŸ¥ç•°å¸¸: {e}")
            return ""

    def _extract_error_from_log(self, log_file: str) -> str:
        """å¾æ—¥èªŒæ–‡ä»¶ä¸­æå–è©³ç´°éŒ¯èª¤ä¿¡æ¯"""
        try:
            if not log_file or not os.path.exists(log_file):
                return ""
            
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                lines = content.split('\n')
                
                # æŸ¥æ‰¾éŒ¯èª¤è¡Œ
                error_lines = []
                for line in lines[-50:]:  # æª¢æŸ¥æœ€å¾Œ50è¡Œ
                    if any(keyword in line.lower() for keyword in ['error:', 'fatal:', 'exception:', 'failed:']):
                        error_lines.append(line.strip())
                
                if error_lines:
                    return error_lines[-1]  # è¿”å›æœ€å¾Œä¸€å€‹éŒ¯èª¤
                    
        except Exception as e:
            self.logger.debug(f"æå–éŒ¯èª¤ä¿¡æ¯å¤±æ•—: {e}")
        
        return ""

    def _get_sync_log_file(self, db_info: DBInfo) -> str:
        """ç²å– sync æ—¥èªŒæ–‡ä»¶è·¯å¾„ - å„ªå…ˆä½¿ç”¨ unbuffer ç‰ˆæœ¬"""
        try:
            log_dir = os.path.join(db_info.local_path, 'logs')
            if os.path.exists(log_dir):
                # å°‹æ‰¾æœ€æ–°çš„æ—¥èªŒæ–‡ä»¶ï¼Œå„ªå…ˆé¸æ“‡ unbuffer ç‰ˆæœ¬
                log_files = []
                for f in os.listdir(log_dir):
                    if f.startswith('repo_sync_') and f.endswith('.log'):
                        file_path = os.path.join(log_dir, f)
                        mtime = os.path.getmtime(file_path)
                        
                        # ğŸ”¥ çµ¦ä¸åŒé¡å‹çš„æ—¥èªŒä¸åŒå„ªå…ˆç´š
                        priority = 0
                        if 'unbuffer' in f:
                            priority = 100  # æœ€é«˜å„ªå…ˆç´š
                        elif 'script' in f:
                            priority = 50
                        elif 'hotfix' in f or 'fixed' in f:
                            priority = 25
                        # èˆŠç‰ˆæœ¬çš„æ—¥èªŒå„ªå…ˆç´šç‚º 0
                        
                        log_files.append((priority, mtime, file_path))
                
                if log_files:
                    # æŒ‰å„ªå…ˆç´šå’Œä¿®æ”¹æ™‚é–“æ’åº
                    log_files.sort(key=lambda x: (x[0], x[1]), reverse=True)
                    latest_log = log_files[0][2]
                    self.logger.debug(f"{db_info.db_info}: ä½¿ç”¨æ—¥èªŒæ–‡ä»¶: {os.path.basename(latest_log)}")
                    return latest_log
            
            self.logger.debug(f"{db_info.db_info}: æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨: {log_dir}")
        except Exception as e:
            self.logger.debug(f"{db_info.db_info}: ç²å–æ—¥èªŒæ–‡ä»¶å¤±æ•—: {e}")
        
        return ""

    def _update_progress_info(self, db_info: DBInfo, tracker: dict):
        """æ›´æ–°é€²åº¦ä¿¡æ¯ - å°ˆé–€å„ªåŒ– unbuffer è¼¸å‡ºè§£æ"""
        try:
            log_file = tracker.get('log_file')
            
            # ğŸ”¥ æ¯æ¬¡éƒ½é‡æ–°ç²å–æ—¥èªŒæ–‡ä»¶ï¼Œç¢ºä¿ä½¿ç”¨æœ€æ–°çš„ unbuffer æ—¥èªŒ
            current_log_file = self._get_sync_log_file(db_info)
            if current_log_file and current_log_file != log_file:
                tracker['log_file'] = current_log_file
                log_file = current_log_file
                self.logger.debug(f"{db_info.db_info}: åˆ‡æ›åˆ°æ–°æ—¥èªŒæ–‡ä»¶: {os.path.basename(log_file)}")
            
            if not log_file or not os.path.exists(log_file):
                tracker['current_activity'] = 'ç­‰å¾…æ—¥èªŒ...'
                tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                return
            
            # ğŸ”¥ å„ªåŒ–çš„æ—¥èªŒè§£æ - å°ˆé–€è™•ç† unbuffer æ ¼å¼
            try:
                file_size = os.path.getsize(log_file)
                
                # åªè®€å–æœ€å¾Œ 2KB é¿å…è™•ç†å¤§æ–‡ä»¶
                read_size = min(file_size, 2048)
                
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(max(0, file_size - read_size))
                    content = f.read()
                    lines = content.split('\n')[-15:]  # æœ€å¾Œ15è¡Œ
                
                # è§£ææœ€æ–°çš„åŒæ­¥ç‹€æ…‹
                latest_progress = 0
                latest_activity = "åŒæ­¥ä¸­..."
                current_project = ""
                total_projects = 0
                current_count = 0
                
                # ğŸ”¥ é‡é»ï¼šè§£æ unbuffer è¼¸å‡ºçš„ç‰¹å®šæ ¼å¼
                for line in reversed(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è§£æ "Syncing: X% (current/total) time | jobs | project" æ ¼å¼
                    if "Syncing:" in line and "%" in line:
                        import re
                        
                        # åŒ¹é…é€²åº¦ç™¾åˆ†æ¯”å’Œè¨ˆæ•¸
                        progress_match = re.search(r'Syncing:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                        if progress_match:
                            latest_progress = int(progress_match.group(1))
                            current_count = int(progress_match.group(2))
                            total_projects = int(progress_match.group(3))
                            
                            # æå–ç•¶å‰è™•ç†çš„é …ç›®
                            # æ ¼å¼é€šå¸¸æ˜¯: "... | X jobs | time project_path @ ..."
                            if '|' in line:
                                parts = line.split('|')
                                for part in parts:
                                    part = part.strip()
                                    # å°‹æ‰¾åŒ…å«é …ç›®è·¯å¾„çš„éƒ¨åˆ†
                                    if '@' in part:
                                        project_info = part.split('@')[-1].strip()
                                    elif '/' in part and 'job' not in part and ':' not in part:
                                        project_info = part
                                    else:
                                        continue
                                    
                                    # æå–é …ç›®åç¨±
                                    if '/' in project_info:
                                        current_project = project_info.split('/')[-1]
                                    else:
                                        current_project = project_info
                                    break
                            
                            # æ§‹å»ºæ´»å‹•æè¿°
                            latest_activity = f"åŒæ­¥: {current_count}/{total_projects}"
                            if current_project:
                                # é™åˆ¶é …ç›®åç¨±é•·åº¦
                                project_name = current_project[:20] + "..." if len(current_project) > 20 else current_project
                                latest_activity += f" - {project_name}"
                            
                            break
                    
                    # ğŸ”¥ è§£æå…¶ä»–ç‹€æ…‹ä¿¡æ¯
                    elif "Fetching project" in line:
                        project_match = re.search(r'Fetching project\s+([^\s]+)', line)
                        if project_match:
                            project_path = project_match.group(1)
                            current_project = project_path.split('/')[-1]
                            latest_activity = f"ç²å–: {current_project}"
                    
                    elif "Skipped fetching project" in line:
                        project_match = re.search(r'Skipped fetching project\s+([^\s]+)', line)
                        if project_match:
                            project_path = project_match.group(1)
                            current_project = project_path.split('/')[-1]
                            latest_activity = f"è·³é: {current_project}"
                
                # ğŸ”¥ æ›´æ–°è¿½è¹¤ä¿¡æ¯
                if latest_progress > 0:
                    tracker['estimated_progress'] = latest_progress
                else:
                    # å¦‚æœæ²’æœ‰è§£æåˆ°é€²åº¦ï¼Œä½¿ç”¨æ™‚é–“ä¼°ç®—
                    tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                
                tracker['current_activity'] = latest_activity
                tracker['current_project'] = current_project
                tracker['total_projects'] = total_projects
                tracker['current_count'] = current_count
                tracker['last_update'] = datetime.now()
                
                # ğŸ”¥ èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
                if latest_progress > 0:
                    self.logger.debug(f"{db_info.db_info}: è§£ææˆåŠŸ - {latest_progress}% ({current_count}/{total_projects}) {current_project}")
                    
            except Exception as e:
                self.logger.debug(f"{db_info.db_info}: è§£ææ—¥èªŒå¤±æ•—: {e}")
                tracker['current_activity'] = 'è§£æå¤±æ•—'
                tracker['estimated_progress'] = self._get_time_based_progress(tracker)
                
        except Exception as e:
            self.logger.debug(f"{db_info.db_info}: é€²åº¦æ›´æ–°å¤±æ•—: {e}")
            tracker['current_activity'] = 'æ›´æ–°å¤±æ•—'
            tracker['estimated_progress'] = self._get_time_based_progress(tracker)

    def _get_time_based_progress(self, tracker: dict) -> int:
        """åŸºæ–¼æ™‚é–“çš„é€²åº¦ä¼°ç®—"""
        runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
        # æ¯åˆ†é˜ç´„1.5%çš„é€²åº¦ï¼Œæœ€å¤š95%
        return min(int(runtime_minutes * 1.5), 95)

    def _extract_progress_from_lines(self, lines: list) -> int:
        """å¾æ—¥èªŒè¡Œä¸­æå–é€²åº¦ç™¾åˆ†æ¯”"""
        for line in reversed(lines):
            # å°‹æ‰¾ç™¾åˆ†æ¯”
            match = re.search(r'(\d+)%', line)
            if match:
                percent = int(match.group(1))
                if 0 <= percent <= 100:
                    return percent
        return None

    def _extract_project_from_lines(self, lines: list) -> str:
        for line in reversed(lines):
            line = line.strip()
            if not line:
                continue
                
            # ğŸ”¥ æ”¹é€²çš„æ¨¡å¼ï¼ŒåŒ…å«æ›´å¤š repo sync çš„å¯¦éš›è¼¸å‡º
            project_patterns = [
                # æ¨™æº–æ¨¡å¼
                r'Fetching project\s+([^\s]+)',
                r'Skipped fetching project\s+([^\s]+)',
                r'Checking out project\s+([^\s]+)',
                
                # Git ç›¸é—œè¼¸å‡º
                r'remote:\s+.*?([a-zA-Z0-9_/\-\.]+/[a-zA-Z0-9_/\-\.]+)',
                r'From\s+.*?:([a-zA-Z0-9_/\-\.]+)',
                
                # ğŸ”¥ æ–°å¢ï¼šè™•ç† heads, refs ç­‰
                r'refs/heads/\S*\s+([a-zA-Z0-9_/\-\.]+)',
                r'Updating\s+references:\s*([a-zA-Z0-9_/\-\.]+)',
                
                # ğŸ”¥ é€šç”¨é …ç›®è·¯å¾‘ï¼ˆæ›´å¯¬é¬†ï¼‰
                r'([a-zA-Z][a-zA-Z0-9_]*(?:/[a-zA-Z0-9_\-\.]+){2,})',
            ]
            
            for pattern in project_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    project = match.group(1).strip()
                    if len(project) > 5 and '/' in project:  # åŸºæœ¬é©—è­‰
                        return self._simplify_project_name(project)
        
        return ""

    def _simplify_project_name(self, project: str) -> str:
        """ç°¡åŒ–é …ç›®åç¨±é¡¯ç¤º"""
        # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´
        prefixes_to_remove = [
            'platform/',
            'device/',
            'vendor/',
            'external/',
            'hardware/',
            'frameworks/',
            'system/',
        ]
        
        simplified = project
        for prefix in prefixes_to_remove:
            if simplified.startswith(prefix):
                simplified = simplified[len(prefix):]
                break
        
        # å¦‚æœé‚„æ˜¯å¤ªé•·ï¼Œåªå–æœ€å¾Œå…©å€‹éƒ¨åˆ†
        parts = simplified.split('/')
        if len(parts) > 2:
            simplified = '/'.join(parts[-2:])
        
        # é™åˆ¶é•·åº¦
        if len(simplified) > 20:
            simplified = simplified[:17] + "..."
        
        return simplified

    def _is_valid_project_name(self, project: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„é …ç›®åç¨±"""
        # éæ¿¾æ¢ä»¶
        invalid_patterns = [
            r'^\d+$',  # ç´”æ•¸å­—
            r'^[^a-zA-Z]',  # ä¸ä»¥å­—æ¯é–‹é ­
            r'\.(log|txt|xml)$',  # æ–‡ä»¶æ“´å±•å
            r'^(http|https|ftp)://',  # URL
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, project, re.IGNORECASE):
                return False
        
        # é•·åº¦æª¢æŸ¥
        if len(project) < 3 or len(project) > 50:
            return False
        
        # å¿…é ˆåŒ…å«è·¯å¾‘åˆ†éš”ç¬¦æˆ–ç‰¹å®šé—œéµå­—
        valid_keywords = ['platform', 'vendor', 'device', 'hardware', 'external', 
                        'frameworks', 'system', 'kernel', 'bootable']
        
        if '/' in project or any(keyword in project.lower() for keyword in valid_keywords):
            return True
        
        return False

    def _wait_for_all_syncs_enhanced(self, db_results: List[DBInfo]):
        """å¢å¼·ç‰ˆé€²åº¦ç›£æ§ - æ›´ç²¾ç¾çš„é¡¯ç¤ºæ ¼å¼"""
        max_wait_time = config_manager.repo_config['sync_timeout']
        start_wait = time.time()
        
        active_syncs = [db for db in db_results if db.sync_process and db.status != DBStatus.FAILED]
        self.logger.info(f"ğŸ” ç›£æ§ {len(active_syncs)} å€‹æ´»èºçš„ repo sync é€²ç¨‹")
        
        # åˆå§‹åŒ–é€²åº¦è¿½è¸ª
        progress_tracker = {}
        for db_info in active_syncs:
            progress_tracker[db_info.db_info] = {
                'start_time': db_info.start_time or datetime.now(),
                'log_file': self._get_sync_log_file(db_info),
                'error_count': 0,
                'estimated_progress': 0,
                'current_activity': 'åˆå§‹åŒ–ä¸­...',
                'current_project': '',
                'last_update': datetime.now()
            }
        
        check_interval = 15  # 15ç§’æª¢æŸ¥ä¸€æ¬¡
        
        while True:
            all_complete = True
            elapsed = int(time.time() - start_wait)
            
            # æ¸…å±ä¸¦é¡¯ç¤ºæ¨™é¡Œ
            print("\033[2J\033[H")  # æ¸…å±
            print(f"ğŸ”„ Repo Sync ç›£æ§ - {elapsed//60:02d}:{elapsed%60:02d}")
            print("="*80)
            
            completed_count = 0
            failed_count = 0
            
            for db_info in active_syncs:
                if db_info.status == DBStatus.FAILED:
                    failed_count += 1
                    continue
                
                db_name = db_info.db_info
                tracker = progress_tracker[db_name]
                
                # ç°¡åŒ–çš„é¡¯ç¤ºåç¨±
                if db_info.version:
                    display_name = f"{db_name} v{db_info.version}"
                else:
                    display_name = db_name
                
                if db_info.sync_process:
                    poll = db_info.sync_process.poll()
                    
                    if poll is None:  # ä»åœ¨é‹è¡Œ
                        all_complete = False
                        self._update_progress_info(db_info, tracker)
                        
                        runtime = datetime.now() - tracker['start_time']
                        runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                        
                        # ç°¡åŒ–çš„é€²åº¦æ¢
                        progress = tracker['estimated_progress']
                        bar_length = 20
                        filled = int(bar_length * progress / 100)
                        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
                        
                        # ç°¡åŒ–çš„æ´»å‹•ä¿¡æ¯
                        activity = tracker.get('current_project', '').split('/')[-1] or 'åŒæ­¥ä¸­'
                        if len(activity) > 15:
                            activity = activity[:12] + "..."
                        
                        print(f"ğŸ”„ {display_name:20s} â”‚{bar}â”‚ {progress:3d}% â”‚ {runtime_str} â”‚ {activity}")
                        
                    elif poll == 0:  # æˆåŠŸå®Œæˆ
                        completed_count += 1
                        runtime = datetime.now() - tracker['start_time']
                        runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                        
                        bar = "â–ˆ" * 20
                        print(f"âœ… {display_name:20s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ å®Œæˆ")
                        
                    else:  # å¤±æ•—
                        failed_count += 1
                        db_info.status = DBStatus.FAILED
                        print(f"âŒ {display_name:20s} â”‚{'':20s}â”‚   0% â”‚      â”‚ å¤±æ•—")
            
            # ç°¡åŒ–çš„çµ±è¨ˆä¿¡æ¯
            running_count = len(active_syncs) - completed_count - failed_count
            total_progress = sum(progress_tracker[db.db_info]['estimated_progress'] 
                            for db in active_syncs if db.status != DBStatus.FAILED)
            avg_progress = total_progress / max(len(active_syncs) - failed_count, 1)
            
            print("-" * 80)
            print(f"ğŸ“Š é‹è¡Œ:{running_count} â”‚ å®Œæˆ:{completed_count} â”‚ å¤±æ•—:{failed_count} â”‚ ç¸½é€²åº¦:{avg_progress:.1f}%")
            
            if all_complete or elapsed > max_wait_time:
                break
            
            time.sleep(check_interval)  # 15ç§’æ›´æ–°ä¸€æ¬¡
        
        # ğŸ æœ€çµ‚çµ±è¨ˆ
        self._display_final_summary(active_syncs, elapsed, progress_tracker)

    def _create_progress_bar(self, percentage: int, width: int = 20) -> str:
        """å‰µå»ºå¯è¦–åŒ–é€²åº¦æ¢"""
        filled = int(width * percentage / 100)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"{bar} {percentage:3d}%"

    def _build_display_name(self, db_info: DBInfo) -> str:
        """æ§‹å»ºæ›´è©³ç´°çš„é¡¯ç¤ºåç¨±"""
        name = db_info.db_info
        if db_info.version:
            name += f" v{db_info.version}"
        elif db_info.manifest_file:
            # å¾ manifest æ–‡ä»¶åæå–ç‰ˆæœ¬
            match = re.search(r'manifest_(\d+)\.xml', db_info.manifest_file)
            if match:
                name += f" v{match.group(1)}"
        return name

    def enhanced_monitor_display(self, active_syncs: list, elapsed: int, progress_tracker: dict):
        """å¢å¼·ç‰ˆé€²åº¦é¡¯ç¤º"""
        print("\033[2J\033[H")  # æ¸…å±
        print(f"ğŸ”„ Repo Sync å¯¦æ™‚ç›£æ§ - {elapsed//60:02d}:{elapsed%60:02d}")
        print("="*80)
        
        completed_count = 0
        failed_count = 0
        total_progress = 0
        
        for db_info in active_syncs:
            if db_info.status == DBStatus.FAILED:
                failed_count += 1
                continue
            
            db_name = db_info.db_info
            tracker = progress_tracker.get(db_name, {})
            
            # æ§‹å»ºé¡¯ç¤ºåç¨±
            display_name = self._build_display_name(db_info)
            
            if db_info.sync_process:
                poll = db_info.sync_process.poll()
                
                if poll is None:  # ä»åœ¨é‹è¡Œ
                    self._update_progress_info(db_info, tracker)
                    
                    progress = tracker.get('estimated_progress', 0)
                    activity = tracker.get('current_activity', 'åŒæ­¥ä¸­')
                    current_count = tracker.get('current_count', 0)
                    total_projects = tracker.get('total_projects', 0)
                    
                    total_progress += progress
                    
                    # é€²åº¦æ¢
                    bar_length = 20
                    filled = int(bar_length * progress / 100)
                    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
                    
                    # é …ç›®ä¿¡æ¯
                    project_info = ""
                    if total_projects > 0:
                        project_info = f" ({current_count}/{total_projects})"
                    
                    # é‹è¡Œæ™‚é–“
                    runtime = datetime.now() - tracker.get('start_time', datetime.now())
                    runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                    
                    print(f"ğŸ”„ {display_name:20s} â”‚{bar}â”‚ {progress:3d}% â”‚ {runtime_str} â”‚ {activity[:30]}{project_info}")
                    
                elif poll == 0:  # æˆåŠŸå®Œæˆ
                    completed_count += 1
                    runtime = datetime.now() - tracker.get('start_time', datetime.now())
                    runtime_str = f"{int(runtime.total_seconds()//60)}:{int(runtime.total_seconds()%60):02d}"
                    
                    bar = "â–ˆ" * 20
                    print(f"âœ… {display_name:20s} â”‚{bar}â”‚ 100% â”‚ {runtime_str} â”‚ å®Œæˆ")
                    
                else:  # å¤±æ•—
                    failed_count += 1
                    print(f"âŒ {display_name:20s} â”‚{'':20s}â”‚   0% â”‚      â”‚ å¤±æ•—")
        
        # ç¸½é«”çµ±è¨ˆ
        running_count = len(active_syncs) - completed_count - failed_count
        avg_progress = total_progress / max(len(active_syncs) - failed_count, 1)
        
        print("-" * 80)
        print(f"ğŸ“Š é‹è¡Œ:{running_count} â”‚ å®Œæˆ:{completed_count} â”‚ å¤±æ•—:{failed_count} â”‚ å¹³å‡é€²åº¦:{avg_progress:.1f}%")
        
    def _display_final_summary(self, active_syncs: list, elapsed: int, progress_tracker: dict):
        """é¡¯ç¤ºæœ€çµ‚æ‘˜è¦"""
        completed = sum(1 for db in active_syncs if db.sync_process and db.sync_process.poll() == 0)
        failed = sum(1 for db in active_syncs if db.status == DBStatus.FAILED)
        total_retries = sum(progress_tracker.get(db.db_info, {}).get('error_count', 0) for db in active_syncs)
        
        print(f"\nğŸ Repo Sync æœ€çµ‚å ±å‘Š")
        print("=" * 60)
        print(f"â±ï¸  ç¸½ç”¨æ™‚: {elapsed//60:02d}:{elapsed%60:02d}")
        print(f"âœ… æˆåŠŸ: {completed}")
        print(f"âŒ å¤±æ•—: {failed}")
        print(f"ğŸ”„ ç¸½é‡è©¦æ¬¡æ•¸: {total_retries}")
        print(f"ğŸ“Š æˆåŠŸç‡: {(completed/(completed+failed)*100):.1f}%" if (completed+failed) > 0 else "0.0%")
        print("=" * 60)
        
        self.logger.info(f"ğŸ Repo sync å®Œæˆçµ±è¨ˆ: æˆåŠŸ {completed}, å¤±æ•— {failed}, ç¸½é‡è©¦ {total_retries}")
                
    def _combine_progress_info(self, log_progress: dict, fs_progress: dict, tracker: dict) -> dict:
        """æ™ºèƒ½çµåˆå¤šç¨®é€²åº¦ä¿¡æ¯æº"""
        runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
        
        # ç¢ºå®šæœ€å¯é çš„é€²åº¦ç™¾åˆ†æ¯”
        if log_progress.get('log_valid') and log_progress['percentage'] > 0:
            # æ—¥èªŒè§£ææœ‰æ•ˆï¼Œå„ªå…ˆä½¿ç”¨
            percentage = log_progress['percentage']
            primary_source = 'log'
        elif fs_progress.get('fs_valid') and fs_progress['percentage'] > 0:
            # æ–‡ä»¶ç³»çµ±æª¢æŸ¥æœ‰æ•ˆ
            percentage = fs_progress['percentage']
            primary_source = 'filesystem'
        else:
            # ä½¿ç”¨æ™‚é–“ä¼°ç®—
            percentage = min(int(runtime_minutes * 2), 95)  # æ¯åˆ†é˜ç´„2%ï¼Œæœ€å¤š95%
            primary_source = 'time_based'
        
        # æ§‹å»ºæ´»å‹•æè¿°
        activity_parts = []
        
        # é …ç›®ä¿¡æ¯
        if log_progress.get('current_project'):
            project_short = log_progress['current_project'].split('/')[-1]
            activity_parts.append(f"ğŸ“ {project_short}")
        elif fs_progress.get('project_count', 0) > 0:
            activity_parts.append(f"ğŸ“ {fs_progress['project_count']} å€‹é …ç›®")
        
        # éšæ®µä¿¡æ¯
        if log_progress.get('sync_phase') and log_progress['sync_phase'] != 'unknown':
            phase_emoji = {
                'initializing': 'ğŸ”„',
                'downloading': 'â¬‡ï¸',
                'resolving': 'ğŸ”§',
                'syncing': 'âš¡',
                'updating': 'ğŸ“',
                'finalizing': 'âœ…'
            }
            emoji = phase_emoji.get(log_progress['sync_phase'], 'âš™ï¸')
            activity_parts.append(f"{emoji} {log_progress.get('current_activity', 'è™•ç†ä¸­')}")
        
        # é€Ÿåº¦ä¿¡æ¯
        if log_progress.get('download_speed'):
            activity_parts.append(f"ğŸš€ {log_progress['download_speed']}")
        
        # æ–‡ä»¶é€²åº¦
        if log_progress.get('files_progress'):
            activity_parts.append(f"ğŸ“Š {log_progress['files_progress']}")
        
        # å¦‚æœæ²’æœ‰è©³ç´°ä¿¡æ¯ï¼Œä½¿ç”¨åŸºæœ¬æè¿°
        if not activity_parts:
            phase_desc = log_progress.get('current_activity') or fs_progress.get('activity') or 'åŒæ­¥ä¸­...'
            activity_parts.append(phase_desc)
        
        # æ·»åŠ æ•¸æ“šæºæ¨™è¨˜ï¼ˆèª¿è©¦ç”¨ï¼‰
        source_emoji = {'log': 'ğŸ“', 'filesystem': 'ğŸ’¾', 'time_based': 'â±ï¸'}
        activity_parts.append(f"{source_emoji.get(primary_source, 'â“')}")
        
        # æ™‚é–“ä¼°ç®—
        estimated_remaining = ''
        if percentage > 5 and runtime_minutes > 1:
            estimated_total_time = runtime_minutes * (100 / percentage)
            remaining_time = max(0, estimated_total_time - runtime_minutes)
            if remaining_time > 1:
                estimated_remaining = f" (é è¨ˆå‰©é¤˜: {remaining_time:.0f}åˆ†)"
        
        return {
            'estimated_progress': percentage,
            'current_activity': ' | '.join(activity_parts) + estimated_remaining,
            'current_project': log_progress.get('current_project', ''),
            'download_speed': log_progress.get('download_speed', ''),
            'files_progress': log_progress.get('files_progress', ''),
            'sync_phase': log_progress.get('sync_phase', 'processing'),
            'data_source': primary_source,
            'runtime_minutes': runtime_minutes
        }

    def _check_filesystem_progress(self, db_info: DBInfo, tracker: dict) -> dict:
        """æª¢æŸ¥æ–‡ä»¶ç³»çµ±å¯¦éš›é€²åº¦ - ä¿®æ­£è¨ˆç®—é‚è¼¯"""
        fs_progress = {
            'percentage': 0,
            'activity': 'æª¢æŸ¥ä¸­...',
            'project_count': 0,
            'fs_valid': False
        }
        
        try:
            # ç°¡åŒ–çš„é€²åº¦è¨ˆç®— - é¿å…ç•°å¸¸å€¼
            runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
            
            # æª¢æŸ¥å¯¦éš›çš„é …ç›®ç›®éŒ„
            git_count = 0
            for root, dirs, files in os.walk(db_info.local_path):
                if '.git' in dirs and '/.repo/' not in root:
                    git_count += 1
            
            # åŸºæ–¼æ™‚é–“çš„ä¿å®ˆä¼°ç®—ï¼ˆæ¯åˆ†é˜ç´„1-2%ï¼‰
            time_based_progress = min(runtime_minutes * 1.5, 95)
            
            # å¦‚æœæœ‰å¯¦éš›é …ç›®ï¼Œä½¿ç”¨æ··åˆä¼°ç®—
            if git_count > 0:
                # å‡è¨­ç¸½å…±éœ€è¦50-100å€‹é …ç›®ï¼ˆæ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´ï¼‰
                estimated_total = max(50, git_count * 2)
                project_progress = min((git_count / estimated_total) * 100, 95)
                # å–æ™‚é–“ä¼°ç®—å’Œé …ç›®ä¼°ç®—çš„å¹³å‡å€¼
                final_progress = min((time_based_progress + project_progress) / 2, 95)
            else:
                final_progress = time_based_progress
            
            fs_progress.update({
                'percentage': max(1, int(final_progress)),  # ç¢ºä¿è‡³å°‘1%
                'activity': f'{git_count}é …ç›®' if git_count > 0 else 'åˆå§‹åŒ–',
                'project_count': git_count,
                'fs_valid': True
            })
            
        except Exception as e:
            # å‚™ç”¨ä¼°ç®—
            runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
            fs_progress.update({
                'percentage': max(1, min(int(runtime_minutes * 1.5), 95)),
                'activity': 'åŒæ­¥ä¸­',
                'fs_valid': False
            })
        
        return fs_progress

    def _parse_repo_sync_progress_enhanced(self, log_file: str) -> dict:
        """å¢å¼·ç‰ˆæ—¥èªŒè§£æ - æ›´æ™ºèƒ½çš„æ¨¡å¼åŒ¹é…"""
        progress_info = {
            'percentage': 0,
            'current_activity': '',
            'current_project': '',
            'download_speed': '',
            'files_progress': '',
            'sync_phase': 'unknown',
            'log_valid': False
        }
        
        if not log_file or not os.path.exists(log_file):
            return progress_info
        
        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                # è®€å–æœ€å¾Œ 10KB çš„å…§å®¹ä»¥ç²å–æœ€æ–°ç‹€æ…‹
                f.seek(max(0, os.path.getsize(log_file) - 10240))
                recent_content = f.read()
                recent_lines = recent_content.split('\n')[-30:]  # æœ€å¾Œ30è¡Œ
            
            # ğŸ¯ æ™ºèƒ½è§£æå„ç¨® repo sync éšæ®µ
            current_phase = self._detect_sync_phase(recent_lines)
            progress_info['sync_phase'] = current_phase
            
            for line in reversed(recent_lines):  # å¾æœ€æ–°çš„è¡Œé–‹å§‹è§£æ
                line = line.strip()
                if not line:
                    continue
                
                # ğŸ“Š Project ä¿¡æ¯è§£æ (å„ªå…ˆç´šæœ€é«˜)
                project_info = self._extract_project_info(line)
                if project_info:
                    progress_info['current_project'] = project_info
                    progress_info['current_activity'] = f"è™•ç†é …ç›®: {project_info}"
                    progress_info['log_valid'] = True
                
                # ğŸ“ˆ ç™¾åˆ†æ¯”é€²åº¦è§£æ
                percentage = self._extract_percentage(line)
                if percentage is not None:
                    progress_info['percentage'] = percentage
                    progress_info['log_valid'] = True
                
                # ğŸš€ ä¸‹è¼‰é€Ÿåº¦è§£æ
                speed = self._extract_download_speed(line)
                if speed:
                    progress_info['download_speed'] = speed
                    progress_info['log_valid'] = True
                
                # ğŸ“ æ–‡ä»¶é€²åº¦è§£æ
                file_progress = self._extract_file_progress(line)
                if file_progress:
                    progress_info['files_progress'] = file_progress
                    progress_info['log_valid'] = True
                
                # å¦‚æœå·²ç¶“ç²å¾—è¶³å¤ ä¿¡æ¯ï¼Œåœæ­¢è§£æ
                if (progress_info['current_project'] and 
                    progress_info['percentage'] > 0):
                    break
            
            # æ ¹æ“šéšæ®µèª¿æ•´æ´»å‹•æè¿°
            if not progress_info['current_activity']:
                progress_info['current_activity'] = self._get_phase_description(current_phase)
                
        except Exception as e:
            self.logger.debug(f"æ—¥èªŒè§£æå¤±æ•—: {e}")
        
        return progress_info

    def _get_phase_description(self, phase: str) -> str:
        """æ ¹æ“šéšæ®µç²å–æè¿°"""
        phase_descriptions = {
            'initializing': 'åˆå§‹åŒ–é€£æ¥...',
            'downloading': 'ä¸‹è¼‰æºä»£ç¢¼...',
            'resolving': 'è§£æå’Œè§£å£“...',
            'syncing': 'åŒæ­¥å·¥ä½œæ¨¹...',
            'updating': 'æ›´æ–°æ–‡ä»¶...',
            'finalizing': 'å®ŒæˆåŒæ­¥...',
            'processing': 'è™•ç†ä¸­...'
        }
        
        return phase_descriptions.get(phase, 'åŒæ­¥ä¸­...')

    def _extract_file_progress(self, line: str) -> str:
        """æå–æ–‡ä»¶é€²åº¦ä¿¡æ¯"""
        file_patterns = [
            r'\((\d+/\d+)\)',
            r'(\d+)\s+of\s+(\d+)',
            r'files:\s*(\d+/\d+)',
        ]
        
        for pattern in file_patterns:
            match = re.search(pattern, line)
            if match:
                if len(match.groups()) == 1:
                    return match.group(1)
                else:
                    return f"{match.group(1)}/{match.group(2)}"
        
        return ''

    def _extract_download_speed(self, line: str) -> str:
        """æå–ä¸‹è¼‰é€Ÿåº¦"""
        speed_patterns = [
            r'([\d.]+\s*[KMG]?B/s)',
            r'@\s*([\d.]+\s*[KMG]?B/s)',
            r'speed:\s*([\d.]+\s*[KMG]?B/s)',
        ]
        
        for pattern in speed_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return ''

    def _extract_percentage(self, line: str) -> int:
        """æå–ç™¾åˆ†æ¯”é€²åº¦"""
        # åŒ¹é…å„ç¨®ç™¾åˆ†æ¯”æ ¼å¼
        percentage_patterns = [
            r'(\d+)%',
            r'(\d+)\s*percent',
            r'progress:\s*(\d+)',
        ]
        
        for pattern in percentage_patterns:
            match = re.search(pattern, line)
            if match:
                percentage = int(match.group(1))
                if 0 <= percentage <= 100:
                    return percentage
        
        return None

    def _extract_project_info(self, line: str) -> str:
        """æå–ç•¶å‰è™•ç†çš„é …ç›®ä¿¡æ¯"""
        # ğŸ¯ æ›´ç²¾ç¢ºçš„é …ç›®åç¨±åŒ¹é…
        project_patterns = [
            r'Fetching project\s+(.+?)(?:\s|$)',
            r'project\s+([^\s]+(?:/[^\s]+)*)',
            r'Syncing\s+(.+?)(?:\s|$)',
            r'Updating\s+(.+?)(?:\s|$)',
            r'Checking out\s+(.+?)(?:\s|$)',
        ]
        
        for pattern in project_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                project = match.group(1).strip()
                # æ¸…ç†é …ç›®åç¨±
                project = re.sub(r'[^\w/\-_.]', '', project)
                if len(project) > 3:  # éæ¿¾å¤ªçŸ­çš„åŒ¹é…
                    return project
        
        return ''

    def _detect_sync_phase(self, lines: list) -> str:
        """æª¢æ¸¬ç•¶å‰ sync éšæ®µ"""
        recent_text = ' '.join(lines[-10:]).lower()
        
        phase_patterns = [
            ('initializing', ['remote: counting', 'remote: enumerating']),
            ('downloading', ['receiving objects', 'downloading', 'fetching']),
            ('resolving', ['resolving deltas', 'unpacking objects']),
            ('syncing', ['syncing work tree', 'checking out']),
            ('updating', ['updating files', 'updating references']),
            ('finalizing', ['done', 'completed'])
        ]
        
        for phase, patterns in phase_patterns:
            if any(pattern in recent_text for pattern in patterns):
                return phase
        
        return 'processing'

    def _fallback_progress_estimation(self, db_info: DBInfo, tracker: dict):
        """å‚™ç”¨é€²åº¦ä¼°ç®—æ–¹æ³•"""
        runtime_minutes = (datetime.now() - tracker['start_time']).total_seconds() / 60
        
        if runtime_minutes < 2:
            tracker['estimated_progress'] = min(int(runtime_minutes * 15), 30)
            tracker['current_activity'] = "åˆå§‹åŒ– repo åŒæ­¥..."
        elif runtime_minutes < 10:
            tracker['estimated_progress'] = min(30 + int((runtime_minutes - 2) * 5), 70)
            tracker['current_activity'] = "ä¸‹è¼‰é …ç›®å’Œæºä»£ç¢¼..."
        else:
            tracker['estimated_progress'] = min(70 + int((runtime_minutes - 10) * 2), 95)
            tracker['current_activity'] = "å®ŒæˆåŒæ­¥å’Œæª¢å‡º..."
            
    def _parse_repo_sync_progress(self, log_lines: list) -> dict:
        """è§£æ repo sync æ—¥èªŒä¸­çš„è©³ç´°é€²åº¦ä¿¡æ¯"""
        progress_info = {
            'percentage': 0,
            'current_activity': '',
            'current_project': '',
            'download_speed': '',
            'files_progress': ''
        }
        
        for line in log_lines:
            line = line.strip()
            if not line:
                continue
                
            # ğŸ” è§£æå„ç¨®é€²åº¦æ¨¡å¼
            
            # 1. é …ç›®ç²å–é€²åº¦: "Fetching project platform/build"
            if 'Fetching project' in line:
                project_match = re.search(r'Fetching project\s+(.+)', line)
                if project_match:
                    project_name = project_match.group(1)
                    progress_info['current_project'] = project_name
                    progress_info['current_activity'] = f"æ­£åœ¨ç²å–é …ç›®: {project_name}"
            
            # 2. åŒæ­¥é€²åº¦: "Syncing work tree: 45% (123/456)"
            elif 'Syncing work tree:' in line:
                sync_match = re.search(r'Syncing work tree:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                if sync_match:
                    percentage = int(sync_match.group(1))
                    current = sync_match.group(2)
                    total = sync_match.group(3)
                    progress_info['percentage'] = percentage
                    progress_info['files_progress'] = f"{current}/{total}"
                    progress_info['current_activity'] = f"åŒæ­¥å·¥ä½œæ¨¹: {percentage}% ({current}/{total})"
            
            # 3. æª¢å‡ºé€²åº¦: "Checking out files: 67% (234/567)"
            elif 'Checking out files:' in line:
                checkout_match = re.search(r'Checking out files:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                if checkout_match:
                    percentage = int(checkout_match.group(1))
                    current = checkout_match.group(2)
                    total = checkout_match.group(3)
                    progress_info['percentage'] = percentage
                    progress_info['files_progress'] = f"{current}/{total}"
                    progress_info['current_activity'] = f"æª¢å‡ºæ–‡ä»¶: {percentage}% ({current}/{total})"
            
            # 4. Git æ“ä½œ: "remote: Counting objects: 12345"
            elif 'remote: Counting objects:' in line:
                objects_match = re.search(r'remote: Counting objects:\s*(\d+)', line)
                if objects_match:
                    count = objects_match.group(1)
                    progress_info['current_activity'] = f"è¨ˆç®—å°è±¡: {count} å€‹"
            
            # 5. Git æ¥æ”¶: "remote: Compressing objects: 100% (456/456)"
            elif 'remote: Compressing objects:' in line:
                compress_match = re.search(r'remote: Compressing objects:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                if compress_match:
                    percentage = int(compress_match.group(1))
                    current = compress_match.group(2)
                    total = compress_match.group(3)
                    progress_info['current_activity'] = f"å£“ç¸®å°è±¡: {percentage}% ({current}/{total})"
            
            # 6. æ¥æ”¶å°è±¡: "Receiving objects: 78% (1234/5678), 12.34 MiB | 1.23 MiB/s"
            elif 'Receiving objects:' in line:
                receive_match = re.search(r'Receiving objects:\s*(\d+)%\s*\((\d+)/(\d+)\)(?:,\s*[\d.]+\s*\w+\s*\|\s*([\d.]+\s*\w+/s))?', line)
                if receive_match:
                    percentage = int(receive_match.group(1))
                    current = receive_match.group(2)
                    total = receive_match.group(3)
                    speed = receive_match.group(4) if receive_match.group(4) else ""
                    
                    progress_info['percentage'] = percentage
                    progress_info['files_progress'] = f"{current}/{total}"
                    progress_info['download_speed'] = speed
                    
                    speed_text = f" @ {speed}" if speed else ""
                    progress_info['current_activity'] = f"æ¥æ”¶å°è±¡: {percentage}% ({current}/{total}){speed_text}"
            
            # 7. è§£æå°è±¡: "Resolving deltas: 89% (456/789)"
            elif 'Resolving deltas:' in line:
                delta_match = re.search(r'Resolving deltas:\s*(\d+)%\s*\((\d+)/(\d+)\)', line)
                if delta_match:
                    percentage = int(delta_match.group(1))
                    current = delta_match.group(2)
                    total = delta_match.group(3)
                    progress_info['current_activity'] = f"è§£æå¢é‡: {percentage}% ({current}/{total})"
            
            # 8. é …ç›®åŒæ­¥å®Œæˆ: "project platform/build/"
            elif 'project ' in line and '/' in line and not 'Fetching' in line:
                project_match = re.search(r'project\s+([^\s]+)', line)
                if project_match:
                    project_name = project_match.group(1)
                    progress_info['current_project'] = project_name
                    progress_info['current_activity'] = f"è™•ç†é …ç›®: {project_name}"
            
            # 9. éŒ¯èª¤ä¿¡æ¯
            elif 'error:' in line.lower() or 'warning:' in line.lower():
                if 'error:' in line.lower():
                    progress_info['current_activity'] = "âš ï¸ é‡åˆ°éŒ¯èª¤ï¼Œæ­£åœ¨é‡è©¦..."
                else:
                    progress_info['current_activity'] = "âš ï¸ è­¦å‘Šä¿¡æ¯ï¼Œç¹¼çºŒè™•ç†..."
        
        return progress_info
        
    def process_selected_dbs(self, db_list: List[str], db_versions: Dict[str, str] = None):
        """è™•ç†é¸å®šçš„ DB"""
        if config_manager.repo_config['async_sync'] and not self.dry_run:
            self.process_dbs_async(db_list, db_versions)
        else:
            # åŒæ­¥è™•ç†é‚è¼¯
            for db_name in db_list:
                if '#' in db_name:
                    db_name, version = db_name.split('#', 1)
                else:
                    version = db_versions.get(db_name) if db_versions else None
                
                for db_info in self.get_all_dbs('all'):
                    if db_info.db_info == db_name:
                        db_info.version = version
                        
                        db_info = self.process_db_phase1(db_info)
                        
                        if not self.dry_run and db_info.sync_process:
                            self.logger.info(f"ç­‰å¾… {db_name} sync å®Œæˆ...")
                            db_info.sync_process.wait()
                        
                        db_info = self.process_db_phase2(db_info)
                        self.report.add_db(db_info)
                        break

    def generate_report(self, output_file: str = None):
        """ç”¢ç”Ÿæ”¹é€²ç‰ˆå ±å‘Š"""
        self.report.finalize()
        
        if not output_file:
            output_file = os.path.join(
                self.output_dir, 
                config_manager.path_config['report_filename']
            )
        
        try:
            self.logger.info("é–‹å§‹ç”¢ç”Ÿ Excel å ±å‘Š")
            
            report_data = []
            for db in self.report.db_details:
                db_dict = db.to_dict()
                # æ–°å¢æ¬„ä½
                db_dict['å®Œæ•´_JIRA_é€£çµ'] = db.jira_link or 'æœªæ‰¾åˆ°'
                db_dict['å®Œæ•´_repo_init_æŒ‡ä»¤'] = db.actual_source_cmd or 'æœªè¨˜éŒ„'
                db_dict['manifest_ç‰ˆæœ¬'] = db.version or 'æœªæŒ‡å®š'
                db_dict['manifest_æª”æ¡ˆ'] = db.manifest_file or 'æœªä¸‹è¼‰'
                report_data.append(db_dict)
            
            df = pd.DataFrame(report_data)
            
            # é‡æ–°æ’åˆ—æ¬„ä½é †åºï¼ŒæŠŠé‡è¦æ¬„ä½æ”¾å‰é¢
            important_columns = [
                'sn', 'module', 'db_type', 'db_info', 'status',
                'manifest_ç‰ˆæœ¬', 'manifest_æª”æ¡ˆ', 'å®Œæ•´_JIRA_é€£çµ', 'å®Œæ•´_repo_init_æŒ‡ä»¤',
                'start_time', 'end_time', 'error_message'
            ]
            
            # ç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½å­˜åœ¨ï¼Œä¸¦é‡æ–°æ’åº
            existing_columns = [col for col in important_columns if col in df.columns]
            other_columns = [col for col in df.columns if col not in important_columns]
            df = df[existing_columns + other_columns]
            
            # å»ºç«‹æ‘˜è¦
            summary = {
                'é …ç›®': ['ç¸½ DB æ•¸', 'æˆåŠŸ', 'å¤±æ•—', 'è·³é', 'åŸ·è¡Œæ™‚é–“'],
                'æ•¸å€¼': [
                    self.report.total_dbs,
                    self.report.successful_dbs,
                    self.report.failed_dbs,
                    self.report.skipped_dbs,
                    str(self.report.end_time - self.report.start_time) if self.report.end_time else 'N/A'
                ]
            }
            summary_df = pd.DataFrame(summary)
            
            # å¯«å…¥æ”¹é€²ç‰ˆ Excel
            self._write_enhanced_excel(df, summary_df, output_file)
            
            self.logger.info(f"Excel å ±å‘Šå·²ç”¢ç”Ÿ: {output_file}")
            print(f"\nğŸ“Š Excel å ±å‘Šå·²ç”¢ç”Ÿ: {output_file}")
            
        except Exception as e:
            self.logger.error(f"ç”¢ç”Ÿå ±å‘Šå¤±æ•—: {str(e)}")

    def _write_enhanced_excel(self, main_df: pd.DataFrame, summary_df: pd.DataFrame, output_file: str):
        """å¯«å…¥æ”¹é€²ç‰ˆ Excelï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰"""
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # å¯«å…¥ä¸»è¦è³‡æ–™
                main_df.to_excel(writer, sheet_name='è©³ç´°è³‡è¨Š', index=False)
                summary_df.to_excel(writer, sheet_name='æ‘˜è¦', index=False)
                
                # æ ¼å¼åŒ–å·¥ä½œè¡¨
                for sheet_name in ['è©³ç´°è³‡è¨Š', 'æ‘˜è¦']:
                    worksheet = writer.sheets[sheet_name]
                    self._format_worksheet_enhanced(worksheet)
            
            self.logger.info(f"æˆåŠŸå¯«å…¥å¢å¼·ç‰ˆ Excel æª”æ¡ˆ: {output_file}")
            
        except Exception as e:
            self.logger.error(f"å¯«å…¥ Excel æª”æ¡ˆå¤±æ•—: {str(e)}")
            raise

    def _format_worksheet_enhanced(self, worksheet):
        """æ ¼å¼åŒ–å·¥ä½œè¡¨ï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰"""
        try:
            # è¨­å®šè¡¨é ­æ ¼å¼ï¼ˆè—åº•ç™½å­—ï¼‰
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            header_font = Font(color="FFFFFF", bold=True)
            header_alignment = Alignment(horizontal="center", vertical="center")
            
            # æ‡‰ç”¨åˆ°ç¬¬ä¸€è¡Œï¼ˆè¡¨é ­ï¼‰
            for cell in worksheet[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = header_alignment
            
            # è‡ªå‹•èª¿æ•´æ¬„å¯¬
            for column in worksheet.columns:
                max_length = 0
                column_letter = get_column_letter(column[0].column)
                
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                
                # è¨­å®šæœ€å°å¯¬åº¦å’Œæœ€å¤§å¯¬åº¦
                adjusted_width = min(max(max_length + 2, 10), 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
            
            # è¨­å®šè¡Œé«˜
            for row in worksheet.iter_rows():
                worksheet.row_dimensions[row[0].row].height = 20
            
            # å‡çµç¬¬ä¸€è¡Œ
            worksheet.freeze_panes = 'A2'
            
        except Exception as e:
            self.logger.warning(f"æ ¼å¼åŒ–å·¥ä½œè¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# =====================================
# ===== äº’å‹•å¼ä»‹é¢ =====
# =====================================

class InteractiveUI:
    """äº’å‹•å¼ä½¿ç”¨è€…ä»‹é¢"""
    
    def __init__(self):
        self.tool = ManifestPinningTool()
        self.logger = setup_logger(self.__class__.__name__)
        self.selected_dbs = []
        self.db_versions = {}
        self.selected_db_type = 'all'
        
        self._load_default_config()
    
    def _load_default_config(self):
        """è¼‰å…¥é è¨­é…ç½®"""
        if config_manager.default_execution_config.get('sftp_override'):
            config_manager.apply_overrides(
                config_manager.default_execution_config['sftp_override'],
                source='default_config'
            )
        
        if config_manager.default_execution_config.get('output_dir'):
            self.tool.output_dir = config_manager.default_execution_config['output_dir']
    
    def setup_sftp(self):
        """è¨­å®š SFTP é€£ç·šè³‡è¨Š"""
        print("\nç›®å‰ SFTP è¨­å®š:")
        print(f"  Host: {config_manager.sftp_config['host']}")
        print(f"  Port: {config_manager.sftp_config['port']}")
        print(f"  Username: {config_manager.sftp_config['username']}")
        
        if input("\næ˜¯å¦è¦ä¿®æ”¹è¨­å®š? (y/N): ").strip().lower() == 'y':
            host = input(f"Host [{config_manager.sftp_config['host']}]: ").strip()
            if host:
                config_manager.sftp_config['host'] = host
                
            port = input(f"Port [{config_manager.sftp_config['port']}]: ").strip()
            if port:
                config_manager.sftp_config['port'] = int(port)
                
            username = input(f"Username [{config_manager.sftp_config['username']}]: ").strip()
            if username:
                config_manager.sftp_config['username'] = username
                
            password = input("Password (ç•™ç©ºä¿æŒåŸå€¼): ").strip()
            if password:
                config_manager.sftp_config['password'] = password
            
            print("âœ… SFTP è¨­å®šå·²æ›´æ–°")
    
    def display_current_settings(self):
        """é¡¯ç¤ºç›®å‰è¨­å®š"""
        print("\nç›®å‰è¨­å®š:")
        print(f"  Mapping table: {'å·²è¼‰å…¥' if self.tool.mapping_reader.df is not None else 'æœªè¼‰å…¥'}")
        print(f"  DB é¡å‹: {self.selected_db_type}")
        print(f"  é¸æ“‡çš„ DB: {len(self.selected_dbs)} å€‹")
        print(f"  è¨­å®šç‰ˆæœ¬çš„ DB: {len(self.db_versions)} å€‹")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        print(f"  å¹³è¡Œè™•ç†: {'å•Ÿç”¨' if config_manager.parallel_config['enable_parallel'] else 'é—œé–‰'}")
        print(f"  Worker æ•¸é‡: {config_manager.parallel_config['max_workers']}")
    
    def load_mapping_table(self):
        """è¼‰å…¥ mapping table"""
        default_mapping = config_manager.default_execution_config.get('mapping_table')
        
        if default_mapping and os.path.exists(default_mapping):
            print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ mapping table: {default_mapping}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­æª”æ¡ˆ? (Y/n): ").strip().lower() != 'n':
                file_path = default_mapping
            else:
                file_path = input(
                    f"è«‹è¼¸å…¥ mapping table è·¯å¾‘ [{config_manager.path_config['default_mapping_table']}]: "
                ).strip()
                if not file_path:
                    file_path = config_manager.path_config['default_mapping_table']
        else:
            default_path = config_manager.path_config['default_mapping_table']
            file_path = input(f"è«‹è¼¸å…¥ mapping table è·¯å¾‘ [{default_path}]: ").strip()
            if not file_path:
                file_path = default_path
        
        if not os.path.exists(file_path):
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            return
        
        if self.tool.load_mapping_table(file_path):
            print(f"âœ… æˆåŠŸè¼‰å…¥ mapping tableï¼Œå…± {len(self.tool.mapping_reader.df)} ç­†è³‡æ–™")
            all_dbs = self.tool.get_all_dbs()
            unique_db_names = list(set([db.db_info for db in all_dbs]))
            print(f"   æ‰¾åˆ° {len(unique_db_names)} å€‹ä¸é‡è¤‡çš„ DB")
            
            if config_manager.default_execution_config.get('db_type'):
                self.selected_db_type = config_manager.default_execution_config['db_type']
                print(f"   ğŸ“Œ ä½¿ç”¨é è¨­ DB é¡å‹: {self.selected_db_type}")
        else:
            print("âŒ è¼‰å…¥å¤±æ•—")
    
    def select_db_type(self):
        """é¸æ“‡ DB é¡å‹"""
        default_type = config_manager.default_execution_config.get('db_type')
        
        if default_type and default_type in ['all', 'master', 'premp', 'mp', 'mpbackup']:
            print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ DB é¡å‹: {default_type}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­å€¼? (Y/n): ").strip().lower() != 'n':
                self.selected_db_type = default_type
                print(f"âœ… å·²é¸æ“‡ DB é¡å‹: {self.selected_db_type}")
                return self.selected_db_type
        
        print("\né¸æ“‡ DB é¡å‹:")
        print("1. All (æ‰€æœ‰é¡å‹)")
        print("2. Master")
        print("3. PreMP")
        print("4. MP")
        print("5. MP Backup")
        
        choice = input("è«‹é¸æ“‡: ").strip()
        
        type_map = {
            '1': 'all',
            '2': 'master',
            '3': 'premp',
            '4': 'mp',
            '5': 'mpbackup'
        }
        
        self.selected_db_type = type_map.get(choice, 'all')
        print(f"âœ… å·²é¸æ“‡ DB é¡å‹: {self.selected_db_type}")
        return self.selected_db_type
    
    def select_dbs(self):
        """é¸æ“‡è¦å®šç‰ˆçš„ DB"""
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return []
        
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        if not all_db_infos:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {self.selected_db_type} é¡å‹çš„ DB")
            return []
        
        # å–å¾—ä¸é‡è¤‡çš„ DB åˆ—è¡¨
        unique_dbs = {}
        for db_info in all_db_infos:
            if db_info.db_info not in unique_dbs:
                unique_dbs[db_info.db_info] = db_info
        
        db_list = list(unique_dbs.keys())
        db_list.sort()
        
        # æª¢æŸ¥é è¨­ DB åˆ—è¡¨
        default_dbs = config_manager.default_execution_config.get('selected_dbs')
        
        if default_dbs:
            if default_dbs in ['all', '*']:
                print(f"\nğŸ“Œ é è¨­é…ç½®ç‚ºé¸æ“‡æ‰€æœ‰ {self.selected_db_type} é¡å‹çš„ DB")
                if input(f"æ˜¯å¦é¸æ“‡å…¨éƒ¨ {len(db_list)} å€‹ DB? (Y/n): ").strip().lower() != 'n':
                    self.selected_dbs = db_list
                    print(f"âœ… å·²é¸æ“‡æ‰€æœ‰ {len(db_list)} å€‹ DB")
                    return db_list
            
            elif isinstance(default_dbs, list) and len(default_dbs) > 0:
                parsed_dbs = []
                for db_spec in default_dbs:
                    if '#' in db_spec:
                        db_name, version = db_spec.split('#', 1)
                        if db_name in db_list:
                            parsed_dbs.append(db_name)
                            self.db_versions[db_name] = version
                    else:
                        if db_spec in db_list:
                            parsed_dbs.append(db_spec)
                
                if parsed_dbs:
                    print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ DB åˆ—è¡¨: {', '.join(default_dbs)}")
                    print(f"   å…¶ä¸­ {len(parsed_dbs)} å€‹ DB å­˜åœ¨æ–¼ç•¶å‰ mapping table")
                    if input("æ˜¯å¦ä½¿ç”¨é è¨­ DB åˆ—è¡¨? (Y/n): ").strip().lower() != 'n':
                        self.selected_dbs = parsed_dbs
                        print(f"âœ… å·²é¸æ“‡ {len(parsed_dbs)} å€‹ DB")
                        return parsed_dbs
        
        # é¡¯ç¤º DB åˆ—è¡¨å’Œé¸æ“‡é‚è¼¯
        print(f"\næ‰¾åˆ° {len(db_list)} å€‹ä¸é‡è¤‡çš„ DB (é¡å‹: {self.selected_db_type})")
        
        print("\nDB åˆ—è¡¨:")
        for i, db in enumerate(db_list, 1):
            db_info = unique_dbs[db]
            print(f"{i:3d}. {db:10s} - {db_info.module:10s} ({db_info.db_type})")
        
        print("\né¸æ“‡æ–¹å¼:")
        print("1. å…¨é¸")
        print("2. è¼¸å…¥ç·¨è™Ÿç¯„åœ (å¦‚: 1-5,7,9-12)")
        print("3. è¼¸å…¥ DB åç¨±åˆ—è¡¨ (é€—è™Ÿåˆ†éš”)")
        
        choice = input("è«‹é¸æ“‡: ").strip()
        
        selected = []
        
        if choice == '1':
            selected = db_list
        elif choice == '2':
            indices_input = input("è«‹è¼¸å…¥ç·¨è™Ÿç¯„åœ: ").strip()
            try:
                indices = self._parse_number_range(indices_input, len(db_list))
                selected = [db_list[i-1] for i in indices if 1 <= i <= len(db_list)]
            except Exception as e:
                print(f"âŒ ç„¡æ•ˆçš„ç·¨è™Ÿç¯„åœ: {e}")
                return []
        elif choice == '3':
            db_names = input("è«‹è¼¸å…¥ DB åç¨± (å¦‚: DB2302,DB2575): ").strip()
            input_dbs = [db.strip() for db in db_names.split(',')]
            for db in input_dbs:
                if db in db_list:
                    selected.append(db)
                else:
                    print(f"âš ï¸ DB {db} ä¸å­˜åœ¨ï¼Œå·²è·³é")
        
        self.selected_dbs = selected
        print(f"âœ… å·²é¸æ“‡ {len(selected)} å€‹ DB")
        return selected
    
    def _parse_number_range(self, range_str: str, max_num: int) -> List[int]:
        """è§£ææ•¸å­—ç¯„åœå­—ä¸²"""
        result = []
        parts = range_str.split(',')
        
        for part in parts:
            part = part.strip()
            if '-' in part:
                start, end = part.split('-', 1)
                start = int(start.strip())
                end = int(end.strip())
                
                if start > end:
                    start, end = end, start
                
                for i in range(start, min(end + 1, max_num + 1)):
                    if i > 0:
                        result.append(i)
            else:
                num = int(part)
                if 1 <= num <= max_num:
                    result.append(num)
        
        return sorted(list(set(result)))
    
    def setup_db_versions(self):
        """è¨­å®š DB ç‰ˆæœ¬"""
        if not self.selected_dbs:
            print("âŒ è«‹å…ˆé¸æ“‡ DB")
            return
        
        default_versions = config_manager.default_execution_config.get('db_versions', {})
        
        if default_versions:
            applicable_versions = {
                db: ver for db, ver in default_versions.items() 
                if db in self.selected_dbs
            }
            
            if applicable_versions:
                print(f"\nğŸ“Œ æ‰¾åˆ°é è¨­ç‰ˆæœ¬è¨­å®š:")
                for db, ver in applicable_versions.items():
                    print(f"   {db}: ç‰ˆæœ¬ {ver}")
                
                if input("æ˜¯å¦ä½¿ç”¨é è¨­ç‰ˆæœ¬è¨­å®š? (Y/n): ").strip().lower() != 'n':
                    self.db_versions.update(applicable_versions)
                    print(f"âœ… å·²å¥—ç”¨ {len(applicable_versions)} å€‹ DB çš„é è¨­ç‰ˆæœ¬")
                    
                    unset_dbs = [db for db in self.selected_dbs if db not in self.db_versions]
                    if unset_dbs:
                        print(f"\né‚„æœ‰ {len(unset_dbs)} å€‹ DB æœªè¨­å®šç‰ˆæœ¬:")
                        for db in unset_dbs:
                            version = input(f"{db} çš„ç‰ˆæœ¬ [æœ€æ–°]: ").strip()
                            if version:
                                self.db_versions[db] = version
                    return
        
        print("\nè¨­å®š DB ç‰ˆæœ¬ (ç•™ç©ºä½¿ç”¨æœ€æ–°ç‰ˆæœ¬)")
        for db in self.selected_dbs:
            if db in self.db_versions:
                print(f"{db}: å·²è¨­å®šç‰ˆæœ¬ {self.db_versions[db]}")
                continue
            
            version = input(f"{db} çš„ç‰ˆæœ¬ [æœ€æ–°]: ").strip()
            if version:
                self.db_versions[db] = version
        
        if self.db_versions:
            print(f"âœ… å·²è¨­å®š {len(self.db_versions)} å€‹ DB çš„ç‰ˆæœ¬")
    
    def execute_pinning(self):
        """åŸ·è¡Œå®šç‰ˆ"""
        if not self.selected_dbs:
            print("âŒ è«‹å…ˆé¸æ“‡è¦å®šç‰ˆçš„ DB")
            return
        
        if not self.tool.mapping_reader or self.tool.mapping_reader.df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥ mapping table")
            return
        
        print("\n" + "="*60)
        print("æº–å‚™åŸ·è¡Œå®šç‰ˆ")
        print("="*60)
        print(f"ğŸ“Œ DB æ•¸é‡: {len(self.selected_dbs)}")
        print(f"ğŸ” è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # è©¢å•è¼¸å‡ºç›®éŒ„
        default_output = config_manager.default_execution_config.get('output_dir')
        if default_output:
            print(f"ğŸ“Œ æ‰¾åˆ°é è¨­è¼¸å‡ºç›®éŒ„: {default_output}")
            if input("æ˜¯å¦ä½¿ç”¨é è¨­è¼¸å‡ºç›®éŒ„? (Y/n): ").strip().lower() != 'n':
                self.tool.output_dir = default_output
            else:
                output_dir = input(f"è¼¸å‡ºç›®éŒ„ [{self.tool.output_dir}]: ").strip()
                if output_dir:
                    self.tool.output_dir = output_dir
        else:
            output_dir = input(f"è¼¸å‡ºç›®éŒ„ [{self.tool.output_dir}]: ").strip()
            if output_dir:
                self.tool.output_dir = output_dir
        
        # ç¢ºèªåŸ·è¡Œ
        if config_manager.default_execution_config.get('auto_confirm'):
            print("ğŸ“Œ è‡ªå‹•ç¢ºèªåŸ·è¡Œï¼ˆæ ¹æ“šé è¨­é…ç½®ï¼‰")
        else:
            if input("\nç¢ºèªé–‹å§‹åŸ·è¡Œ? (Y/n): ").strip().lower() == 'n':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                return
        
        print("\nğŸš€ é–‹å§‹åŸ·è¡Œå®šç‰ˆ...")
        
        print("ğŸŒ æº–å‚™ SFTP é€£ç·šï¼ˆæ¯å€‹ DB ä½¿ç”¨ç¨ç«‹é€£ç·šï¼‰...")
        
        try:
            # åŸ·è¡Œå®šç‰ˆ
            self.tool.process_selected_dbs(self.selected_dbs, self.db_versions)
            
            # ç”¢ç”Ÿå ±å‘Š
            print("\nğŸ“Š ç”¢ç”Ÿå ±å‘Š...")
            report_path = os.path.join(
                self.tool.output_dir, 
                config_manager.path_config['report_filename']
            )
            self.tool.generate_report(report_path)
            
            print("\nâœ¨ å®šç‰ˆå®Œæˆï¼")
            
        finally:
            resource_manager.cleanup_all()
    
    def quick_execute_with_defaults(self):
        """ä½¿ç”¨é è¨­é…ç½®å¿«é€ŸåŸ·è¡Œ"""
        print("\n" + "="*60)
        print("å¿«é€ŸåŸ·è¡Œæ¨¡å¼ - ä½¿ç”¨é è¨­é…ç½®")
        print("="*60)
        
        # è‡ªå‹•è¼‰å…¥ mapping table
        if config_manager.default_execution_config.get('mapping_table'):
            file_path = config_manager.default_execution_config['mapping_table']
            if os.path.exists(file_path):
                print(f"ğŸ“Œ è¼‰å…¥é è¨­ mapping table: {file_path}")
                if self.tool.load_mapping_table(file_path):
                    print(f"âœ… æˆåŠŸè¼‰å…¥")
                else:
                    print("âŒ è¼‰å…¥å¤±æ•—")
                    return
            else:
                print(f"âŒ é è¨­ mapping table ä¸å­˜åœ¨: {file_path}")
                return
        else:
            print("âŒ æœªè¨­å®šé è¨­ mapping table")
            return
        
        # è¨­å®š DB é¡å‹
        if config_manager.default_execution_config.get('db_type'):
            self.selected_db_type = config_manager.default_execution_config['db_type']
            print(f"ğŸ“Œ ä½¿ç”¨é è¨­ DB é¡å‹: {self.selected_db_type}")
        
        # é¸æ“‡ DB
        default_dbs = config_manager.default_execution_config.get('selected_dbs')
        all_db_infos = self.tool.get_all_dbs(self.selected_db_type)
        
        if default_dbs:
            if default_dbs in ['all', '*']:
                unique_dbs = list(set([db.db_info for db in all_db_infos]))
                self.selected_dbs = unique_dbs
                print(f"ğŸ“Œ é¸æ“‡æ‰€æœ‰ {self.selected_db_type} é¡å‹çš„ DB: {len(unique_dbs)} å€‹")
            
            elif isinstance(default_dbs, list) and len(default_dbs) > 0:
                parsed_dbs = []
                for db_spec in default_dbs:
                    if '#' in db_spec:
                        db_name, version = db_spec.split('#', 1)
                        parsed_dbs.append(db_name)
                        self.db_versions[db_name] = version
                    else:
                        parsed_dbs.append(db_spec)
                self.selected_dbs = parsed_dbs
                print(f"ğŸ“Œ ä½¿ç”¨é è¨­ DB åˆ—è¡¨: {len(parsed_dbs)} å€‹")
        else:
            print("âš ï¸ é è¨­é…ç½®æœªæŒ‡å®š DB åˆ—è¡¨ï¼Œç„¡æ³•è‡ªå‹•åŸ·è¡Œ")
            return
        
        # è¨­å®šç‰ˆæœ¬
        if config_manager.default_execution_config.get('db_versions'):
            self.db_versions.update(config_manager.default_execution_config['db_versions'])
            print(f"ğŸ“Œ å¥—ç”¨é è¨­ç‰ˆæœ¬è¨­å®š: {len(self.db_versions)} å€‹")
        
        # è¨­å®šè¼¸å‡ºç›®éŒ„
        if config_manager.default_execution_config.get('output_dir'):
            self.tool.output_dir = config_manager.default_execution_config['output_dir']
            print(f"ğŸ“Œ ä½¿ç”¨é è¨­è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # é¡¯ç¤ºæ‘˜è¦
        print("\nåŸ·è¡Œæ‘˜è¦:")
        print(f"  DB é¡å‹: {self.selected_db_type}")
        print(f"  DB æ•¸é‡: {len(self.selected_dbs)}")
        print(f"  è¨­å®šç‰ˆæœ¬: {len(self.db_versions)} å€‹")
        print(f"  è¼¸å‡ºç›®éŒ„: {self.tool.output_dir}")
        
        # ç¢ºèªåŸ·è¡Œ
        if not config_manager.default_execution_config.get('auto_confirm'):
            if input("\nç¢ºèªåŸ·è¡Œ? (Y/n): ").strip().lower() == 'n':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆ")
                return
        
        # åŸ·è¡Œå®šç‰ˆ
        self.execute_pinning()
    
    def test_jira_connection(self):
        """æ¸¬è©¦ JIRA é€£ç·š"""
        print("\næ¸¬è©¦ JIRA é€£ç·š...")
        
        if not hasattr(self.tool, 'source_cmd_manager'):
            self.tool.source_cmd_manager = SourceCommandManager()
        
        if self.tool.source_cmd_manager.jira_client.connect():
            print("âœ… JIRA é€£ç·šæˆåŠŸï¼")
            
            if input("\næ˜¯å¦è¦æ¸¬è©¦æŸ¥è©¢ DB çš„ source command? (y/N): ").strip().lower() == 'y':
                db_name = input("è«‹è¼¸å…¥ DB åç¨± (ä¾‹å¦‚: DB2302): ").strip()
                if db_name:
                    print(f"\næŸ¥è©¢ {db_name} çš„ source command...")
                    
                    test_db_info = DBInfo(
                        sn=0,
                        module="Test",
                        db_type="master",
                        db_info=db_name,
                        db_folder="",
                        sftp_path=""
                    )
                    
                    cmd = self.tool.source_cmd_manager.get_source_command(
                        test_db_info,
                        self.tool.mapping_reader.df if self.tool.mapping_reader else None
                    )
                    
                    if cmd:
                        print(f"\næ‰¾åˆ°çš„ source command:")
                        print(f"  {cmd}")
                    else:
                        print(f"\næœªæ‰¾åˆ° {db_name} çš„ source command")
        else:
            print("âŒ JIRA é€£ç·šå¤±æ•—ï¼")
    
    def test_sftp_connection(self):
        """æ¸¬è©¦ SFTP é€£ç·š"""
        print("\næ¸¬è©¦ SFTP é€£ç·š...")
        print(f"ä¼ºæœå™¨: {config_manager.sftp_config['host']}:{config_manager.sftp_config['port']}")
        print(f"ç”¨æˆ¶: {config_manager.sftp_config['username']}")
        
        # å‰µå»ºè‡¨æ™‚ SFTP ç®¡ç†å™¨é€²è¡Œæ¸¬è©¦
        test_sftp_manager = SFTPManager()
        
        if test_sftp_manager.connect():
            print("âœ… SFTP é€£ç·šæˆåŠŸï¼")
            
            if input("\næ˜¯å¦è¦æ¸¬è©¦è·¯å¾‘å­˜å–? (y/N): ").strip().lower() == 'y':
                path = input("è«‹è¼¸å…¥è¦æ¸¬è©¦çš„è·¯å¾‘: ").strip()
                if path:
                    try:
                        result = test_sftp_manager.find_latest_manifest(path)
                        if result:
                            print(f"âœ… æ‰¾åˆ° manifest: {result[1]}")
                        else:
                            print("âŒ æ²’æœ‰æ‰¾åˆ° manifest")
                    except Exception as e:
                        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
            
            test_sftp_manager.disconnect()
        else:
            print("âŒ SFTP é€£ç·šå¤±æ•—ï¼")
    
    def display_menu(self) -> str:
        """é¡¯ç¤ºä¸»é¸å–®"""
        print("\n" + "="*60)
        print("Manifest å®šç‰ˆå·¥å…· - ä¸»é¸å–® (æ”¹é€²ç‰ˆ)")
        
        has_defaults = any([
            config_manager.default_execution_config.get('mapping_table'),
            config_manager.default_execution_config.get('db_type'),
            config_manager.default_execution_config.get('selected_dbs'),
            config_manager.default_execution_config.get('db_versions'),
            config_manager.default_execution_config.get('output_dir')
        ])
        
        if has_defaults:
            print("(ğŸ“Œ å·²è¼‰å…¥é è¨­é…ç½®)")
        
        print("="*60)
        print("1. è¼‰å…¥ mapping table")
        print("2. è¨­å®š SFTP é€£ç·šè³‡è¨Š")
        print("3. é¸æ“‡ DB é¡å‹ (master/premp/mp/mpbackup/all)")
        print("4. é¸æ“‡è¦å®šç‰ˆçš„ DB")
        print("5. è¨­å®š DB ç‰ˆæœ¬")
        print("6. é–‹å§‹åŸ·è¡Œå®šç‰ˆ")
        print("7. é¡¯ç¤ºç›®å‰è¨­å®š")
        print("8. å¿«é€ŸåŸ·è¡Œï¼ˆä½¿ç”¨æ‰€æœ‰é è¨­å€¼ï¼‰")
        print("9. æ¸¬è©¦ JIRA é€£ç·š")
        print("10. æ¸¬è©¦ SFTP é€£ç·š")
        print("0. çµæŸç¨‹å¼")
        print("="*60)
        
        return input("è«‹é¸æ“‡åŠŸèƒ½: ").strip()
    
    def run_interactive(self):
        """åŸ·è¡Œäº’å‹•å¼ä»‹é¢"""
        print("\næ­¡è¿ä½¿ç”¨ Manifest å®šç‰ˆå·¥å…·ï¼")
        print(f"ç‰ˆæœ¬: {__version__} (æ”¹é€²ç‰ˆ)")
        print("æ”¹é€²å…§å®¹: ä¿®å¾© SFTP Garbage packet å•é¡Œã€æ”¹å–„æ—¥èªŒè¼¸å‡º")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é è¨­é…ç½®
        has_complete_defaults = all([
            config_manager.default_execution_config.get('mapping_table'),
            os.path.exists(config_manager.default_execution_config.get('mapping_table', ''))
        ])
        
        if has_complete_defaults:
            print("\nğŸ“Œ åµæ¸¬åˆ°å®Œæ•´çš„é è¨­é…ç½®")
            if input("æ˜¯å¦è¦ä½¿ç”¨é è¨­é…ç½®å¿«é€ŸåŸ·è¡Œ? (y/N): ").strip().lower() == 'y':
                self.quick_execute_with_defaults()
                return
        
        while True:
            try:
                choice = self.display_menu()
                
                if choice == '0':
                    print("\nğŸ‘‹ å†è¦‹ï¼")
                    break
                elif choice == '1':
                    self.load_mapping_table()
                elif choice == '2':
                    self.setup_sftp()
                elif choice == '3':
                    self.select_db_type()
                elif choice == '4':
                    self.select_dbs()
                elif choice == '5':
                    self.setup_db_versions()
                elif choice == '6':
                    self.execute_pinning()
                elif choice == '7':
                    self.display_current_settings()
                elif choice == '8':
                    self.quick_execute_with_defaults()
                elif choice == '9':
                    self.test_jira_connection()
                elif choice == '10':
                    self.test_sftp_connection()
                else:
                    print("âŒ ç„¡æ•ˆçš„é¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·")
                if input("ç¢ºå®šè¦çµæŸç¨‹å¼å—? (Y/n): ").strip().lower() != 'n':
                    break
            except Exception as e:
                self.logger.error(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                if input("æ˜¯å¦ç¹¼çºŒ? (Y/n): ").strip().lower() == 'n':
                    break
        
        resource_manager.cleanup_all()

# =====================================
# ===== ä¸»ç¨‹å¼ =====
# =====================================

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(
        description='Manifest å®šç‰ˆå·¥å…· - è‡ªå‹•åŒ– repo å®šç‰ˆè™•ç† (æ”¹é€²ç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
ç‰ˆæœ¬: {__version__}
ä½œè€…: {__author__}
æ—¥æœŸ: {__date__}

æ”¹é€²å…§å®¹:
- ä¿®å¾© SFTP "Garbage packet received" éŒ¯èª¤
- æ”¹ç‚ºæ­£å¸¸é †åºæ—¥èªŒè¼¸å‡ºï¼Œä¾¿æ–¼ debug
- æ”¹é€² Excel å ±å‘Šæ ¼å¼ï¼ˆè‡ªå‹•é©å¯¬ã€è¡¨é ­è—åº•ç™½å­—ï¼‰
- è¨˜éŒ„å®Œæ•´ repo init æŒ‡ä»¤å’Œ JIRA é€£çµ

ç¯„ä¾‹:
  # ä½¿ç”¨äº’å‹•å¼ä»‹é¢
  python {sys.argv[0]}
  
  # è™•ç†æŒ‡å®šçš„ DB
  python {sys.argv[0]} -m mapping.xlsx -d DB2302,DB2575 -o ./output
  
  # æ¸¬è©¦æ¨¡å¼
  python {sys.argv[0]} -m mapping.xlsx --dry-run
        """
    )
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('-m', '--mapping', 
                        type=str,
                        help='Mapping table Excel æª”æ¡ˆè·¯å¾‘')
    
    parser.add_argument('-o', '--output', 
                        type=str,
                        help=f'è¼¸å‡ºç›®éŒ„ (é è¨­: {config_manager.path_config["default_output_dir"]})')
    
    parser.add_argument('-t', '--type',
                        choices=['all', 'master', 'premp', 'mp', 'mpbackup'],
                        default='all',
                        help='è¦è™•ç†çš„ DB é¡å‹ (é è¨­: all)')
    
    parser.add_argument('-d', '--dbs', 
                        type=str,
                        help='è¦è™•ç†çš„ DB åˆ—è¡¨ï¼Œé€—è™Ÿåˆ†éš” (å¯åŒ…å«ç‰ˆæœ¬ï¼Œå¦‚: DB2302#3,DB2575)')
    
    parser.add_argument('-v', '--versions', 
                        type=str,
                        help='DB ç‰ˆæœ¬è¨­å®šï¼Œæ ¼å¼: DB2302#3,DB2575#186')
    
    parser.add_argument('--dry-run', 
                        action='store_true',
                        help='æ¸¬è©¦æ¨¡å¼ï¼Œåªé¡¯ç¤ºå°‡è¦åŸ·è¡Œçš„å‹•ä½œï¼Œä¸å¯¦éš›åŸ·è¡Œ')
    
    parser.add_argument('--debug', 
                        action='store_true',
                        help='å•Ÿç”¨ debug æ¨¡å¼ï¼Œé¡¯ç¤ºè©³ç´°æ—¥èªŒ')
    
    parser.add_argument('--version', 
                        action='version',
                        version=f'%(prog)s {__version__}')
    
    # è§£æåƒæ•¸
    args = parser.parse_args()
    
    # è¨­å®šæ—¥èªŒç­‰ç´š
    if args.debug:
        config_manager.log_config['level'] = logging.DEBUG
        logging.getLogger().setLevel(logging.DEBUG)
        for handler in logging.getLogger().handlers:
            handler.setLevel(logging.DEBUG)
        print("ğŸ” Debug æ¨¡å¼å·²å•Ÿç”¨")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¸¬è©¦æ¨¡å¼
    if args.dry_run:
        print("\n" + "="*60)
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ (Dry Run) - ä¸æœƒå¯¦éš›åŸ·è¡Œä»»ä½•æ“ä½œ")
        print("="*60)
    
    # æ±ºå®šåŸ·è¡Œæ¨¡å¼
    if args.mapping:
        # å‘½ä»¤åˆ—æ¨¡å¼
        print("\n" + "="*60)
        print(f"ğŸ“‹ Manifest å®šç‰ˆå·¥å…· v{__version__} - å‘½ä»¤åˆ—æ¨¡å¼ (æ”¹é€²ç‰ˆ)")
        print("="*60)
        
        tool = ManifestPinningTool()
        
        if args.dry_run:
            tool.dry_run = True
        
        try:
            # è¼‰å…¥ mapping table
            print(f"\nğŸ“‚ è¼‰å…¥ mapping table: {args.mapping}")
            if not os.path.exists(args.mapping):
                print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.mapping}")
                sys.exit(1)
            
            if not tool.load_mapping_table(args.mapping):
                print("âŒ ç„¡æ³•è¼‰å…¥ mapping table")
                sys.exit(1)
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ mapping table")
            
            # è¨­å®šè¼¸å‡ºç›®éŒ„
            tool.output_dir = args.output or config_manager.path_config['default_output_dir']
            os.makedirs(tool.output_dir, exist_ok=True)
            print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {tool.output_dir}")

            print(f"\nğŸŒ æº–å‚™ SFTP é€£ç·š: {config_manager.sftp_config['host']}")
            print("â„¹ï¸  æ¯å€‹ DB å°‡ä½¿ç”¨ç¨ç«‹çš„ SFTP é€£ç·š")

            try:
                # æ±ºå®šè¦è™•ç†çš„ DB åˆ—è¡¨
                db_list = []
                db_versions = {}
                
                if args.dbs:
                    db_specs = [db.strip() for db in args.dbs.split(',')]
                    
                    for db_spec in db_specs:
                        if '#' in db_spec:
                            db_name, version = db_spec.split('#', 1)
                            db_list.append(db_name)
                            db_versions[db_name] = version
                        else:
                            db_list.append(db_spec)
                    
                    print(f"\nğŸ“Œ ä½¿ç”¨æŒ‡å®šçš„ DB åˆ—è¡¨: {', '.join(db_list)}")
                else:
                    all_db_infos = tool.get_all_dbs(args.type)
                    db_list = list(set([db.db_info for db in all_db_infos]))
                    
                    if args.type == 'all':
                        print(f"\nğŸ“Œ ä½¿ç”¨æ‰€æœ‰ DBï¼Œå…± {len(db_list)} å€‹")
                    else:
                        print(f"\nğŸ“Œ ä½¿ç”¨æ‰€æœ‰ {args.type} é¡å‹çš„ DBï¼Œå…± {len(db_list)} å€‹")
                
                # è™•ç†é¡å¤–çš„ç‰ˆæœ¬è¨­å®š
                if args.versions:
                    version_specs = [v.strip() for v in args.versions.split(',')]
                    for version_spec in version_specs:
                        if '#' in version_spec:
                            db_name, version = version_spec.split('#', 1)
                            db_versions[db_name] = version
                    
                    print(f"ğŸ“Œ è¨­å®šäº† {len(db_versions)} å€‹ DB çš„ç‰ˆæœ¬")
                
                # ç¢ºèªè™•ç†è³‡è¨Š
                print("\n" + "-"*40)
                print("ğŸ“‹ æº–å‚™è™•ç†ä»¥ä¸‹ DB:")
                for i, db in enumerate(db_list, 1):
                    version_info = f" (ç‰ˆæœ¬: {db_versions[db]})" if db in db_versions else " (æœ€æ–°ç‰ˆæœ¬)"
                    print(f"  {i:3d}. {db}{version_info}")
                print("-"*40)
                
                if not db_list:
                    print("âŒ æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„ DB")
                    sys.exit(1)
                
                # è©¢å•ç¢ºèªï¼ˆé™¤éæ˜¯æ¸¬è©¦æ¨¡å¼ï¼‰
                if not args.dry_run:
                    if sys.stdin.isatty():
                        confirm = input(f"\nç¢ºèªè¦è™•ç† {len(db_list)} å€‹ DB? (Y/n): ").strip().lower()
                        if confirm == 'n':
                            print("âŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                            sys.exit(0)
                
                # é–‹å§‹è™•ç†
                print("\n" + "="*60)
                if args.dry_run:
                    print("ğŸ§ª é–‹å§‹æ¸¬è©¦åŸ·è¡Œï¼ˆä¸æœƒå¯¦éš›åŸ·è¡Œæ“ä½œï¼‰")
                else:
                    print("ğŸš€ é–‹å§‹åŸ·è¡Œå®šç‰ˆè™•ç†")
                print("="*60)
                
                start_time = datetime.now()
                
                # åŸ·è¡Œè™•ç†
                tool.process_selected_dbs(db_list, db_versions)
                
                end_time = datetime.now()
                elapsed_time = end_time - start_time
                
                # ç”¢ç”Ÿå ±å‘Š
                if not args.dry_run:
                    print("\nğŸ“Š ç”¢ç”Ÿè™•ç†å ±å‘Š...")
                    report_path = os.path.join(
                        tool.output_dir, 
                        config_manager.path_config['report_filename']
                    )
                    tool.generate_report(report_path)
                
                # é¡¯ç¤ºçµæœæ‘˜è¦
                print("\n" + "="*60)
                print("âœ¨ è™•ç†å®Œæˆï¼")
                print("="*60)
                print(f"ğŸ“Š ç¸½ DB æ•¸: {tool.report.total_dbs}")
                print(f"âœ… æˆåŠŸ: {tool.report.successful_dbs}")
                print(f"âŒ å¤±æ•—: {tool.report.failed_dbs}")
                print(f"â­ï¸ è·³é: {tool.report.skipped_dbs}")
                print(f"â±ï¸ ç¸½è€—æ™‚: {elapsed_time}")
                print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {tool.output_dir}")
                if not args.dry_run:
                    print(f"ğŸ“Š å ±å‘Šæª”æ¡ˆ: {report_path}")
                print("="*60)
                
                # å¦‚æœæœ‰å¤±æ•—çš„é …ç›®ï¼Œé¡¯ç¤ºè©³ç´°è³‡è¨Š
                if tool.report.failed_dbs > 0:
                    print("\nâŒ å¤±æ•—çš„ DB:")
                    for db in tool.report.db_details:
                        if db.status == DBStatus.FAILED:
                            print(f"  - {db.module}/{db.db_info}: {db.error_message}")
                
            finally:
                print("\nğŸ“Œ æ¸…ç†è³‡æº...")
                resource_manager.cleanup_all()
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ æ”¶åˆ° Ctrl+Cï¼Œæ¸…ç†æ‰€æœ‰é€²ç¨‹...")
            resource_manager.cleanup_all()
            sys.exit(0)
            
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            if args.debug:
                import traceback
                traceback.print_exc()
            sys.exit(1)
        
        finally:
            resource_manager.cleanup_all()
    
    else:
        # äº’å‹•å¼æ¨¡å¼
        print("\n" + "="*60)
        print(f"ğŸ® Manifest å®šç‰ˆå·¥å…· v{__version__} - äº’å‹•å¼ä»‹é¢ (æ”¹é€²ç‰ˆ)")
        print("="*60)
        print("æ”¹é€²å…§å®¹: ä¿®å¾© SFTP Garbage packet å•é¡Œã€æ”¹å–„æ—¥èªŒè¼¸å‡º")
        print("æç¤º: ä½¿ç”¨ -h åƒæ•¸æŸ¥çœ‹å‘½ä»¤åˆ—é¸é …")
        print("="*60)
        
        try:
            ui = InteractiveUI()
            
            if args.output:
                ui.tool.output_dir = args.output
            
            if args.dry_run:
                ui.tool.dry_run = True
                print("ğŸ§ª æ¸¬è©¦æ¨¡å¼å·²å•Ÿç”¨")
            
            ui.run_interactive()
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è¦‹ï¼")
            sys.exit(0)
            
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            if args.debug:
                import traceback
                traceback.print_exc()
            sys.exit(1)
        
        finally:
            resource_manager.cleanup_all()

# åœ¨ä½ çš„ç¨‹å¼ä¸­æ·»åŠ é€™å€‹æ¸¬è©¦å‡½æ•¸
def test_sftp_connection():
    """æ¸¬è©¦ SFTP é€£ç·šå’Œä¼ºæœå™¨èƒ½åŠ›"""
    print("="*50)
    print("SFTP é€£ç·šå’Œèƒ½åŠ›æ¸¬è©¦")
    print("="*50)
    
    sftp_mgr = SFTPManager()
    
    try:
        # æ¸¬è©¦é€£ç·š
        print("1. æ¸¬è©¦ SFTP é€£ç·š...")
        if sftp_mgr.connect():
            print("âœ… SFTP é€£ç·šæˆåŠŸ")
            
            # é¡¯ç¤ºä¼ºæœå™¨èƒ½åŠ›
            print("\n2. ä¼ºæœå™¨èƒ½åŠ›æª¢æ¸¬çµæœ:")
            capabilities = sftp_mgr._server_capabilities
            print(f"   listdir_attr æ”¯æ´: {'âœ…' if capabilities['supports_listdir_attr'] else 'âŒ'}")
            print(f"   stat æ”¯æ´: {'âœ…' if capabilities['supports_stat'] else 'âŒ'}")
            
            # æ¸¬è©¦åŸºæœ¬æ“ä½œ
            print("\n3. æ¸¬è©¦åŸºæœ¬æ“ä½œ...")
            try:
                items = sftp_mgr._safe_listdir('.')
                print(f"âœ… åŸºæœ¬ listdir æˆåŠŸï¼Œæ‰¾åˆ° {len(items)} å€‹é …ç›®")
            except Exception as e:
                print(f"âŒ åŸºæœ¬ listdir å¤±æ•—: {e}")
            
            # æ¸¬è©¦è©³ç´°åˆ—è¡¨
            print("\n4. æ¸¬è©¦è©³ç´°åˆ—è¡¨...")
            try:
                items = sftp_mgr._safe_listdir_with_details('.')
                print(f"âœ… è©³ç´°åˆ—è¡¨æˆåŠŸï¼Œæ‰¾åˆ° {len(items)} å€‹é …ç›®")
                for item in items[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    print(f"   - {item['name']} ({'ç›®éŒ„' if item['is_dir'] else 'æª”æ¡ˆ'})")
            except Exception as e:
                print(f"âŒ è©³ç´°åˆ—è¡¨å¤±æ•—: {e}")
            
            sftp_mgr.disconnect()
            print("\nâœ… æ¸¬è©¦å®Œæˆ")
            
        else:
            print("âŒ SFTP é€£ç·šå¤±æ•—")
    
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

def test_manifest_search():
    """æ¸¬è©¦å¯¦éš›çš„ manifest æœå°‹åŠŸèƒ½"""
    print("="*60)
    print("é€²éš Manifest æœå°‹æ¸¬è©¦")
    print("="*60)
    
    sftp_mgr = SFTPManager()
    
    try:
        # é€£ç·š
        print("1. å»ºç«‹ SFTP é€£ç·š...")
        if not sftp_mgr.connect():
            print("âŒ SFTP é€£ç·šå¤±æ•—")
            return
        print("âœ… SFTP é€£ç·šæˆåŠŸ")
        
        # æ¸¬è©¦è·¯å¾‘ï¼ˆä½¿ç”¨ DB2145 çš„å¯¦éš›è·¯å¾‘ï¼‰
        test_paths = [
            "/DailyBuild/Merlin8/DB2145_Merlin8_FW_Android14_Ref_Plus_GoogleGMS",
            "/DailyBuild/Merlin8",  # ä¸Šå±¤ç›®éŒ„æ¸¬è©¦
        ]
        
        for test_path in test_paths:
            print(f"\n2. æ¸¬è©¦è·¯å¾‘: {test_path}")
            
            # æ¸¬è©¦åŸºæœ¬åˆ—ç›®éŒ„
            try:
                print(f"   2.1 æ¸¬è©¦åŸºæœ¬ listdir...")
                items = sftp_mgr._safe_listdir(test_path)
                print(f"   âœ… æ‰¾åˆ° {len(items)} å€‹é …ç›®")
                
                # é¡¯ç¤ºå‰å¹¾å€‹é …ç›®
                for i, item in enumerate(items[:5]):
                    print(f"      - {item}")
                if len(items) > 5:
                    print(f"      ... é‚„æœ‰ {len(items) - 5} å€‹é …ç›®")
                    
            except Exception as e:
                print(f"   âŒ åŸºæœ¬ listdir å¤±æ•—: {e}")
                continue
            
            # æ¸¬è©¦è©³ç´°åˆ—ç›®éŒ„
            try:
                print(f"   2.2 æ¸¬è©¦è©³ç´° listdir...")
                items = sftp_mgr._safe_listdir_with_details(test_path)
                print(f"   âœ… æ‰¾åˆ° {len(items)} å€‹è©³ç´°é …ç›®")
                
                # åˆ†æç›®éŒ„çµæ§‹
                dirs = [item for item in items if item['is_dir']]
                files = [item for item in items if not item['is_dir']]
                
                print(f"      ç›®éŒ„: {len(dirs)} å€‹")
                print(f"      æª”æ¡ˆ: {len(files)} å€‹")
                
                # é¡¯ç¤ºç‰ˆæœ¬ç›®éŒ„
                version_dirs = []
                for item in dirs:
                    version_num = sftp_mgr._extract_version_number(item['name'])
                    if version_num:
                        version_dirs.append((int(version_num), item['name']))
                
                if version_dirs:
                    version_dirs.sort(reverse=True)
                    print(f"      ç‰ˆæœ¬ç›®éŒ„: {len(version_dirs)} å€‹")
                    for ver_num, ver_name in version_dirs[:3]:
                        print(f"         ç‰ˆæœ¬ {ver_num}: {ver_name}")
                    
            except Exception as e:
                print(f"   âŒ è©³ç´° listdir å¤±æ•—: {e}")
                continue
            
            # æ¸¬è©¦ manifest æœå°‹
            try:
                print(f"   2.3 æ¸¬è©¦ manifest æœå°‹...")
                result = sftp_mgr.find_latest_manifest(test_path)
                
                if result:
                    manifest_path, manifest_name = result
                    print(f"   âœ… æ‰¾åˆ° manifest: {manifest_name}")
                    print(f"      å®Œæ•´è·¯å¾‘: {manifest_path}")
                    
                    # æ¸¬è©¦æª”æ¡ˆå­˜åœ¨æ€§
                    print(f"   2.4 é©—è­‰ manifest æª”æ¡ˆ...")
                    if sftp_mgr._file_exists_and_valid(manifest_path):
                        print(f"   âœ… Manifest æª”æ¡ˆæœ‰æ•ˆ")
                    else:
                        print(f"   âŒ Manifest æª”æ¡ˆç„¡æ•ˆ")
                else:
                    print(f"   âŒ æœªæ‰¾åˆ° manifest")
                    
            except Exception as e:
                print(f"   âŒ Manifest æœå°‹å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
        
        # æ¸¬è©¦ç‰¹å®šç‰ˆæœ¬æœå°‹ï¼ˆå¦‚æœç”¨æˆ¶æœ‰ç‰¹å®šç‰ˆæœ¬éœ€æ±‚ï¼‰
        print(f"\n3. æ¸¬è©¦ç‰¹å®šç‰ˆæœ¬æœå°‹...")
        test_version = "709"  # DB2145 çš„ç‰ˆæœ¬
        test_path = "/DailyBuild/Merlin8/DB2145_Merlin8_FW_Android14_Ref_Plus_GoogleGMS"
        
        try:
            result = sftp_mgr.find_latest_manifest(test_path, target_version=test_version)
            if result:
                manifest_path, manifest_name = result
                print(f"âœ… æ‰¾åˆ°ç‰ˆæœ¬ {test_version} çš„ manifest: {manifest_name}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°ç‰ˆæœ¬ {test_version} çš„ manifest")
        except Exception as e:
            print(f"âŒ ç‰¹å®šç‰ˆæœ¬æœå°‹å¤±æ•—: {e}")
        
        sftp_mgr.disconnect()
        print("\nâœ… é€²éšæ¸¬è©¦å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        
        if sftp_mgr.connected:
            sftp_mgr.disconnect()

if __name__ == "__main__":
    # test_sftp_connection()
    # test_manifest_search()
    main()